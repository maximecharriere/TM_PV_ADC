{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup & Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pvlib\n",
        "import json\n",
        "import os\n",
        "from pvlib.pvsystem import PVSystem, Array, FixedMount\n",
        "from pvlib.location import Location\n",
        "from pvlib.modelchain import ModelChain\n",
        "from pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, root_mean_squared_error, r2_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "import forestci as fci\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import threading\n",
        "from sklearn.metrics import make_scorer\n",
        "import dash\n",
        "from dash import dcc, html\n",
        "import plotly.graph_objects as go\n",
        "from dash.dependencies import Input, Output\n",
        "import webbrowser\n",
        "from threading import Timer\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from sklearn.utils.parallel import Parallel, delayed\n",
        "from sklearn.utils.validation import (\n",
        "    check_is_fitted,\n",
        ")\n",
        "from sklearn.ensemble._base import _partition_estimators\n",
        "\n",
        "pio.renderers.default = \"browser\"  # render plotly figures in browser\n",
        "\n",
        "PARENT_DATA_DIR = os.getenv('PARENT_DATA_DIR')\n",
        "if PARENT_DATA_DIR is None:\n",
        "    raise ValueError(\"PARENT_DATA_DIR environment variable is not set\")\n",
        "\n",
        "\n",
        "dataDirpath = PARENT_DATA_DIR + r\"\\PRiOT\\dataExport_2\"  # \"/Applications/Documents/TM Maxime/dataExport_3400_daily\"#\n",
        "dataCacheDirpath = os.path.join(dataDirpath, \"cache\")\n",
        "logsDirpath = \"../logs\"\n",
        "useCached = False\n",
        "forceTrain = True\n",
        "tuneMaxProductionEstimators = True\n",
        "random_state = 42\n",
        "\n",
        "\n",
        "testingDays = 100\n",
        "minTestingDays = 30\n",
        "minTrainingDays = 7\n",
        "\n",
        "if not os.path.exists(logsDirpath):\n",
        "    os.makedirs(logsDirpath)\n",
        "\n",
        "if not os.path.exists(dataCacheDirpath):\n",
        "    os.makedirs(dataCacheDirpath)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# create 15 random value between 0.0 and 20.0\n",
        "expected = np.random.uniform(0.0, 20.0, 15)\n",
        "expectedRelative = expected/20.0\n",
        "# create 10 value linearly spaced between 100 and 95\n",
        "linear_values = np.linspace(1.0, 0.90, num=10)\n",
        "#create 5 value at 50\n",
        "constant_values = np.repeat(0.60, 5)\n",
        "#concatenate the three arrays\n",
        "factors = np.concatenate([linear_values, constant_values])\n",
        "measure =  factors*expected\n",
        "measureRelative = measure/20.0\n",
        "\n",
        "mape = (expected-measure)/expected*100\n",
        "mapeRelative = (expectedRelative-measureRelative)/expectedRelative*100\n",
        "\n",
        "# plot expected values and measure in two trace. y axis is kWh, x axis is the date from June 1st 2024 to June 15th 2024\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=pd.date_range(start='2024-06-01', periods=15, freq='D'), y=expected, mode='lines+markers', name='Expected'))\n",
        "fig.add_trace(go.Scatter(x=pd.date_range(start='2024-06-01', periods=15, freq='D'), y=measure, mode='lines+markers', name='Measure'))\n",
        "fig.update_layout(xaxis_title='Date', yaxis_title='Production [kWh]')\n",
        "\n",
        "# set the fig to 1000x666\n",
        "fig.update_layout(width=1000, height=666)\n",
        "fig.show()\n",
        "# plot factors\n",
        "# set the y axis to % between 0 and 100\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=pd.date_range(start='2024-06-01', periods=15, freq='D'), y=mape, mode='markers', name='Relative Error'))\n",
        "fig.add_trace(go.Scatter(x=pd.date_range(start='2024-06-01', periods=15, freq='D'), y=mapeRelative, mode='markers', name='Relative Error RELATIVE'))\n",
        "fig.update_layout(xaxis_title='Date', yaxis_title='Relative Difference [%]')\n",
        "fig.update_yaxes(range=[0, 100])\n",
        "fig.update_layout(width=1000, height=666)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Serializer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://scikit-learn.org/stable/model_persistence.html\n",
        "\n",
        "\n",
        "class ModelSerializer:\n",
        "    def _save_model(self, model, serial_type, save_params):\n",
        "        serial_type.dump(model, save_params)\n",
        "\n",
        "    def _retrieve_model(self, serial_type, retrieve_params):\n",
        "        return serial_type.load(retrieve_params)\n",
        "\n",
        "\n",
        "# save_model_path = \"Serialized_models\\\\\"\n",
        "\n",
        "\n",
        "class JoblibSerializer(ModelSerializer):\n",
        "    def save_model(self, model, save_model_path, filename):\n",
        "        super()._save_model(model, joblib, os.path.join(save_model_path, filename + \".joblib\"))\n",
        "\n",
        "    def retrieve_model(self, save_model_path, filename):\n",
        "        return super()._retrieve_model(joblib, os.path.join(save_model_path, filename + '.joblib'))\n",
        "\n",
        "\n",
        "class PickleSerializer(ModelSerializer):\n",
        "    def save_model(self, model, save_model_path, filename):\n",
        "        # create folder if not exists\n",
        "        if not os.path.exists(save_model_path):\n",
        "            os.makedirs(save_model_path)\n",
        "        with open(os.path.join(save_model_path, filename + \".pkl\"), 'wb') as f:\n",
        "            super()._save_model(model, pickle, f)\n",
        "\n",
        "    def retrieve_model(self, save_model_path, filename):\n",
        "        with open(os.path.join(save_model_path, filename + \".pkl\"), 'rb') as f:\n",
        "            return super()._retrieve_model(pickle, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_altitude_from_wgs84(longitude, latitude):\n",
        "    # Convert WGS84 to LV95\n",
        "    lv95_url = \"https://geodesy.geo.admin.ch/reframe/wgs84tolv95\"\n",
        "    params_lv95 = {\n",
        "        \"easting\": longitude,\n",
        "        \"northing\": latitude,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    response_lv95 = requests.get(lv95_url, params=params_lv95)\n",
        "    if response_lv95.status_code != 200:\n",
        "        raise Exception(\"Error converting WGS84 to LV95: \" + response_lv95.text)\n",
        "\n",
        "    lv95_data = response_lv95.json()\n",
        "    lv95_easting = lv95_data[\"easting\"]\n",
        "    lv95_northing = lv95_data[\"northing\"]\n",
        "\n",
        "    # Get altitude from LV95 coordinates\n",
        "    altitude_url = \"https://api3.geo.admin.ch/rest/services/height\"\n",
        "    params_altitude = {\n",
        "        \"easting\": lv95_easting,\n",
        "        \"northing\": lv95_northing\n",
        "    }\n",
        "\n",
        "    response_altitude = requests.get(altitude_url, params=params_altitude)\n",
        "    if response_altitude.status_code != 200:\n",
        "        raise Exception(\"Error retrieving altitude: \" + response_altitude.text)\n",
        "\n",
        "    altitude_data = response_altitude.json()\n",
        "    altitude = altitude_data[\"height\"]\n",
        "\n",
        "    return float(altitude)\n",
        "\n",
        "\n",
        "def remove_system(systemName, message):\n",
        "    if 'systemsName_Valid' in globals() and systemName in systemsName_Valid:\n",
        "        systemsName_Valid.remove(systemName)\n",
        "    if 'systemsName_Valid' in globals() and systemName in systemsName_Valid:\n",
        "        systemsName_Valid.remove(systemName)\n",
        "    if 'systemsData_EstimatedMaxDailyEnergy' in globals() and systemName in systemsData_EstimatedMaxDailyEnergy.columns:\n",
        "        systemsData_EstimatedMaxDailyEnergy.drop(columns=systemName, inplace=True)\n",
        "    if 'systemsData_MeasuredDailyEnergy_train' in globals() and systemName in systemsData_MeasuredDailyEnergy_train.columns:\n",
        "        systemsData_MeasuredDailyEnergy_train.drop(columns=systemName, inplace=True)\n",
        "    if 'systemsData_MeasuredDailyEnergy_test' in globals() and systemName in systemsData_MeasuredDailyEnergy_test.columns:\n",
        "        systemsData_MeasuredDailyEnergy_test.drop(columns=systemName, inplace=True)\n",
        "    print(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 481/481 [00:00<00:00, 477392.39it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "metadataFilepath = os.path.join(dataDirpath, \"metadata.json\")\n",
        "\n",
        "with open(metadataFilepath, 'r') as f:\n",
        "    systemsMetadata = json.load(f)\n",
        "\n",
        "# Add altitude to metadata, if not already present (TODO : imporove with multi threading)\n",
        "\n",
        "for systemId, systemMetadata in tqdm(systemsMetadata.items()):\n",
        "    if \"loc_altitude\" not in systemMetadata['metadata']:\n",
        "        if \"loc_longitude\" in systemMetadata['metadata'] and \"loc_latitude\" in systemMetadata['metadata']:\n",
        "            systemMetadata['metadata'][\"loc_altitude\"] = get_altitude_from_wgs84(systemMetadata['metadata'][\"loc_longitude\"], systemMetadata['metadata'][\"loc_latitude\"])\n",
        "\n",
        "# Split arrays in dictionaries by module number\n",
        "for systemId, systemMetadata in systemsMetadata.items():\n",
        "    arrays = {}\n",
        "    keys_to_delete = []\n",
        "    for key, value in systemMetadata['metadata'].items():\n",
        "        if 'mod' in key:\n",
        "            # Extract the module number\n",
        "            array_num = key.split('_')[1][-1]\n",
        "            # Remove the module number from the key\n",
        "            new_key = '_'.join(key.split('_')[:1] + key.split('_')[2:])\n",
        "            # Add the key-value pair to the appropriate module dictionary\n",
        "            if 'arrays' not in systemMetadata:\n",
        "                systemMetadata['arrays'] = {}\n",
        "            if array_num not in systemMetadata['arrays']:\n",
        "                systemMetadata['arrays'][array_num] = {}\n",
        "            systemMetadata['arrays'][array_num][new_key] = value\n",
        "            keys_to_delete.append(key)\n",
        "    for key in keys_to_delete:\n",
        "        del systemMetadata['metadata'][key]\n",
        "\n",
        "# Save metadata with altitude\n",
        "with open(metadataFilepath, 'w') as f:\n",
        "    json.dump(systemsMetadata, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import measures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate dates found: 1004 (see log file for more details)\n",
            "System 2026239 : No measures found\n",
            "System 2026239 : No 'pv_kwp' found\n",
            "-> Removing system 2026239 from the list of systems\n",
            "System a001001 : No 'pv_wp' found for array 1\n",
            "System a001001 : No 'pv_number' found for array 1\n",
            "-> Removing system a001001 from the list of systems\n",
            "System a001028 : The key-value 'pv_azimut:39-129-219' is not a number for array 1\n",
            "-> Removing system a001028 from the list of systems\n",
            "System a001038 : The key-value 'pv_azimut:58-138-238' is not a number for array 1\n",
            "-> Removing system a001038 from the list of systems\n",
            "System a001103 : No 'pv_tilt' found for array 1\n",
            "-> Removing system a001103 from the list of systems\n",
            "System a001116 : No 'pv_wp' found for array 2\n",
            "-> Removing system a001116 from the list of systems\n",
            "System a001118 : The key-value 'pv_azimut:55-235' is not a number for array 1\n",
            "-> Removing system a001118 from the list of systems\n",
            "System a001122 : No 'pv_tilt' found for array 2\n",
            "System a001122 : No 'pv_tilt' found for array 1\n",
            "-> Removing system a001122 from the list of systems\n",
            "System a001164 : No measures found\n",
            "-> Removing system a001164 from the list of systems\n",
            "System a001165 : The key-value 'pv_azimut:90° / 270°' is not a number for array 1\n",
            "-> Removing system a001165 from the list of systems\n",
            "System a001199 : No 'pv_azimut' found for array 1\n",
            "-> Removing system a001199 from the list of systems\n",
            "System a001222 : The key-value 'pv_azimut:56-146-236' is not a number for array 1\n",
            "-> Removing system a001222 from the list of systems\n",
            "System a001226 : No 'pv_tilt' found for array 1\n",
            "System a001226 : No 'pv_azimut' found for array 1\n",
            "System a001226 : No 'pv_number' found for array 1\n",
            "-> Removing system a001226 from the list of systems\n",
            "System a001258 : No 'pv_tilt' found for array 1\n",
            "System a001258 : No 'pv_azimut' found for array 1\n",
            "System a001258 : No 'pv_number' found for array 1\n",
            "-> Removing system a001258 from the list of systems\n",
            "System a001280 : No 'pv_tilt' found for array 2\n",
            "System a001280 : No 'pv_tilt' found for array 1\n",
            "-> Removing system a001280 from the list of systems\n",
            "System a001324 : No 'pv_azimut' found for array 1\n",
            "-> Removing system a001324 from the list of systems\n",
            "System a001380 : No 'pv_tilt' found for array 1\n",
            "-> Removing system a001380 from the list of systems\n",
            "System a001382 : No 'pv_tilt' found for array 3\n",
            "System a001382 : No 'pv_azimut' found for array 3\n",
            "System a001382 : No 'pv_wp' found for array 3\n",
            "-> Removing system a001382 from the list of systems\n",
            "System a001404 : No 'pv_tilt' found for array 2\n",
            "System a001404 : No 'pv_tilt' found for array 1\n",
            "-> Removing system a001404 from the list of systems\n",
            "System a001411 : No 'pv_tilt' found for array 2\n",
            "-> Removing system a001411 from the list of systems\n",
            "System a001412 : No 'pv_tilt' found for array 3\n",
            "System a001412 : No 'pv_tilt' found for array 4\n",
            "-> Removing system a001412 from the list of systems\n",
            "System a001415 : The key-value 'pv_azimut:152/332' is not a number for array 1\n",
            "System a001415 : No 'pv_azimut' found for array 2\n",
            "System a001415 : No 'pv_wp' found for array 2\n",
            "System a001415 : No 'pv_number' found for array 2\n",
            "-> Removing system a001415 from the list of systems\n",
            "System a001422 : No 'pv_number' found for array 1\n",
            "-> Removing system a001422 from the list of systems\n",
            "System a001423 : No measures found\n",
            "System a001423 : No 'loc_latitude' found\n",
            "System a001423 : No 'loc_longitude' found\n",
            "System a001423 : No 'loc_altitude' found\n",
            "-> Removing system a001423 from the list of systems\n",
            "System a001426 : No 'pv_tilt' found for array 1\n",
            "-> Removing system a001426 from the list of systems\n",
            "System a001434 : No 'loc_latitude' found\n",
            "System a001434 : No 'loc_longitude' found\n",
            "System a001434 : No 'loc_altitude' found\n",
            "-> Removing system a001434 from the list of systems\n",
            "System a001446 : The key-value 'pv_azimut:150/330' is not a number for array 1\n",
            "-> Removing system a001446 from the list of systems\n",
            "System a001460 : No 'pv_azimut' found for array 2\n",
            "System a001460 : The key-value 'pv_azimut:47-137-227-317' is not a number for array 1\n",
            "-> Removing system a001460 from the list of systems\n",
            "System a001466 : No 'pv_kwp' found\n",
            "System a001466 : No PV arrays found\n",
            "-> Removing system a001466 from the list of systems\n",
            "System a001471 : The key-value 'pv_azimut:51 / 231' is not a number for array 1\n",
            "-> Removing system a001471 from the list of systems\n",
            "System a001477 : The key-value 'pv_azimut:141 / 321' is not a number for array 1\n",
            "-> Removing system a001477 from the list of systems\n",
            "System a001488 : No 'pv_tilt' found for array 3\n",
            "System a001488 : No 'pv_azimut' found for array 3\n",
            "System a001488 : No 'pv_tilt' found for array 2\n",
            "System a001488 : No 'pv_azimut' found for array 2\n",
            "System a001488 : No 'pv_tilt' found for array 4\n",
            "System a001488 : No 'pv_azimut' found for array 4\n",
            "System a001488 : No 'pv_tilt' found for array 1\n",
            "System a001488 : The key-value 'pv_azimut:Nordost 46 Südost 136 Südwest 226' is not a number for array 1\n",
            "-> Removing system a001488 from the list of systems\n",
            "System a001495 : The key-value 'pv_azimut:Südost: 147°' is not a number for array 1\n",
            "-> Removing system a001495 from the list of systems\n",
            "System a001499 : The key-value 'pv_azimut:West 270° / Ost 90°' is not a number for array 1\n",
            "-> Removing system a001499 from the list of systems\n",
            "System a001501 : The key-value 'pv_azimut:95/275' is not a number for array 1\n",
            "-> Removing system a001501 from the list of systems\n",
            "System a001502 : The key-value 'pv_azimut:238 / 148 / 58' is not a number for array 1\n",
            "-> Removing system a001502 from the list of systems\n",
            "System a001507 : The key-value 'pv_azimut:239 /59' is not a number for array 1\n",
            "-> Removing system a001507 from the list of systems\n",
            "System a001509 : The key-value 'pv_azimut: 141 / 321' is not a number for array 1\n",
            "-> Removing system a001509 from the list of systems\n",
            "System a001514 : No 'pv_tilt' found for array 1\n",
            "System a001514 : No 'pv_azimut' found for array 1\n",
            "System a001514 : No 'pv_number' found for array 1\n",
            "-> Removing system a001514 from the list of systems\n",
            "System a001520 : The key-value 'pv_azimut:322/142' is not a number for array 1\n",
            "-> Removing system a001520 from the list of systems\n",
            "System a001528 : The key-value 'pv_azimut:146 / 326' is not a number for array 1\n",
            "-> Removing system a001528 from the list of systems\n",
            "System a001539 : The key-value 'pv_azimut:West: 293° / Ost: 113°' is not a number for array 1\n",
            "-> Removing system a001539 from the list of systems\n",
            "System a001541 : No 'pv_tilt' found for array 1\n",
            "System a001541 : The key-value 'pv_azimut:75/255' is not a number for array 1\n",
            "-> Removing system a001541 from the list of systems\n",
            "System a001544 : No 'pv_azimut' found for array 2\n",
            "System a001544 : The key-value 'pv_azimut:299/119' is not a number for array 1\n",
            "-> Removing system a001544 from the list of systems\n",
            "System a001545 : The key-value 'pv_azimut:141 / 51' is not a number for array 1\n",
            "-> Removing system a001545 from the list of systems\n",
            "System a001550 : No measures found\n",
            "-> Removing system a001550 from the list of systems\n",
            "System a001556 : The key-value 'pv_azimut:90 / 270' is not a number for array 1\n",
            "-> Removing system a001556 from the list of systems\n",
            "System a001558 : No measures found\n",
            "System a001558 : No 'loc_latitude' found\n",
            "System a001558 : No 'loc_longitude' found\n",
            "System a001558 : No 'loc_altitude' found\n",
            "System a001558 : No 'pv_kwp' found\n",
            "System a001558 : No PV arrays found\n",
            "-> Removing system a001558 from the list of systems\n",
            "System a001562 : The key-value 'pv_azimut:174 /354' is not a number for array 1\n",
            "-> Removing system a001562 from the list of systems\n",
            "System a001564 : The key-value 'pv_azimut:116 / 296' is not a number for array 1\n",
            "-> Removing system a001564 from the list of systems\n",
            "System a001565 : No 'pv_wp' found for array 1\n",
            "System a001565 : No 'pv_wp' found for array 2\n",
            "-> Removing system a001565 from the list of systems\n",
            "System a001567 : The key-value 'pv_azimut:134 /224' is not a number for array 1\n",
            "-> Removing system a001567 from the list of systems\n",
            "System a001568 : No 'pv_tilt' found for array 1\n",
            "System a001568 : No 'pv_azimut' found for array 1\n",
            "System a001568 : No 'pv_number' found for array 1\n",
            "-> Removing system a001568 from the list of systems\n",
            "System a001573 : The key-value 'pv_azimut:67 / 157 /247' is not a number for array 1\n",
            "-> Removing system a001573 from the list of systems\n",
            "System a001574 : No 'pv_azimut' found for array 2\n",
            "System a001574 : The key-value 'pv_azimut:156 / 336 ' is not a number for array 1\n",
            "-> Removing system a001574 from the list of systems\n",
            "System a001576 : The key-value 'pv_azimut:100 / 190 /280' is not a number for array 1\n",
            "-> Removing system a001576 from the list of systems\n",
            "System a001578 : No 'loc_latitude' found\n",
            "System a001578 : No 'loc_longitude' found\n",
            "System a001578 : No 'loc_altitude' found\n",
            "System a001578 : No 'pv_kwp' found\n",
            "System a001578 : No PV arrays found\n",
            "-> Removing system a001578 from the list of systems\n",
            "System a001579 : No 'pv_kwp' found\n",
            "System a001579 : No PV arrays found\n",
            "-> Removing system a001579 from the list of systems\n",
            "System a001585 : The key-value 'pv_azimut:315 / 135' is not a number for array 1\n",
            "-> Removing system a001585 from the list of systems\n",
            "System a001586 : No measures found\n",
            "System a001586 : The key-value 'pv_azimut:59 / 239' is not a number for array 1\n",
            "-> Removing system a001586 from the list of systems\n",
            "System a001588 : The key-value 'pv_azimut:153/333' is not a number for array 1\n",
            "-> Removing system a001588 from the list of systems\n",
            "System a001594 : The key-value 'pv_azimut:243 / 63' is not a number for array 1\n",
            "-> Removing system a001594 from the list of systems\n",
            "System a001595 : The key-value 'pv_azimut:256+56' is not a number for array 1\n",
            "-> Removing system a001595 from the list of systems\n",
            "System a001597 : No measures found\n",
            "System a001597 : No 'loc_latitude' found\n",
            "System a001597 : No 'loc_longitude' found\n",
            "System a001597 : No 'loc_altitude' found\n",
            "System a001597 : No 'pv_kwp' found\n",
            "System a001597 : No PV arrays found\n",
            "-> Removing system a001597 from the list of systems\n",
            "System a001601 : The key-value 'pv_azimut:106 / 286' is not a number for array 1\n",
            "-> Removing system a001601 from the list of systems\n",
            "System a001602 : The key-value 'pv_azimut:243 / 63' is not a number for array 1\n",
            "-> Removing system a001602 from the list of systems\n",
            "System a001603 : The key-value 'pv_azimut:116 / 296' is not a number for array 1\n",
            "-> Removing system a001603 from the list of systems\n",
            "System a001604 : The key-value 'pv_azimut:111 / 291' is not a number for array 1\n",
            "-> Removing system a001604 from the list of systems\n",
            "System a001611 : The key-value 'pv_azimut:180 /360' is not a number for array 1\n",
            "-> Removing system a001611 from the list of systems\n",
            "System a001612 : The key-value 'pv_azimut:65 /245' is not a number for array 1\n",
            "-> Removing system a001612 from the list of systems\n",
            "System a001616 : No 'pv_azimut' found for array 2\n",
            "System a001616 : The key-value 'pv_azimut:180/90/270' is not a number for array 1\n",
            "-> Removing system a001616 from the list of systems\n",
            "System a001617 : The key-value 'pv_azimut:28 /208' is not a number for array 1\n",
            "-> Removing system a001617 from the list of systems\n",
            "System a001622 : No measures found\n",
            "-> Removing system a001622 from the list of systems\n",
            "System a001626 : The key-value 'pv_azimut:132 /45' is not a number for array 1\n",
            "-> Removing system a001626 from the list of systems\n",
            "System a001627 : No 'loc_latitude' found\n",
            "System a001627 : No 'loc_longitude' found\n",
            "System a001627 : No 'loc_altitude' found\n",
            "-> Removing system a001627 from the list of systems\n",
            "System a001629 : The key-value 'pv_azimut:0-180-62-242' is not a number for array 1\n",
            "-> Removing system a001629 from the list of systems\n",
            "System a001631 : The key-value 'pv_azimut:270 / 90' is not a number for array 1\n",
            "-> Removing system a001631 from the list of systems\n",
            "System a001632 : The key-value 'pv_azimut:267 / 87' is not a number for array 1\n",
            "-> Removing system a001632 from the list of systems\n",
            "System a001635 : The key-value 'pv_azimut:100 / 280' is not a number for array 1\n",
            "-> Removing system a001635 from the list of systems\n",
            "System a001636 : The key-value 'pv_azimut:260/180' is not a number for array 1\n",
            "-> Removing system a001636 from the list of systems\n",
            "System a001640 : The key-value 'pv_azimut:82 / 262' is not a number for array 1\n",
            "-> Removing system a001640 from the list of systems\n",
            "System a001643 : The key-value 'pv_azimut:236 /146' is not a number for array 1\n",
            "-> Removing system a001643 from the list of systems\n",
            "System a001644 : The key-value 'pv_azimut:130/310' is not a number for array 1\n",
            "-> Removing system a001644 from the list of systems\n",
            "System a001653 : The key-value 'pv_azimut:106 / 286' is not a number for array 1\n",
            "-> Removing system a001653 from the list of systems\n",
            "System a001654 : No measures found\n",
            "-> Removing system a001654 from the list of systems\n",
            "System a001666 : No 'pv_azimut' found for array 1\n",
            "-> Removing system a001666 from the list of systems\n",
            "System a001694 : The key-value 'pv_azimut:41 / 221' is not a number for array 1\n",
            "-> Removing system a001694 from the list of systems\n",
            "System a001711 : The key-value 'pv_azimut:113 / 293' is not a number for array 1\n",
            "-> Removing system a001711 from the list of systems\n",
            "System a001719 : The key-value 'pv_azimut:22 /202' is not a number for array 1\n",
            "-> Removing system a001719 from the list of systems\n",
            "System a001720 : The key-value 'pv_azimut:106 / 286' is not a number for array 1\n",
            "-> Removing system a001720 from the list of systems\n",
            "System a001721 : The key-value 'pv_azimut:22 / 202' is not a number for array 1\n",
            "-> Removing system a001721 from the list of systems\n",
            "System a001779 : No measures found\n",
            "System a001779 : No 'pv_kwp' found\n",
            "System a001779 : No PV arrays found\n",
            "-> Removing system a001779 from the list of systems\n",
            "System a001794 : No measures found\n",
            "System a001794 : The key-value 'pv_azimut:69 /249' is not a number for array 1\n",
            "-> Removing system a001794 from the list of systems\n",
            "System a001811 : No measures found\n",
            "System a001811 : No 'loc_latitude' found\n",
            "System a001811 : No 'loc_longitude' found\n",
            "System a001811 : No 'loc_altitude' found\n",
            "System a001811 : No 'pv_kwp' found\n",
            "System a001811 : No PV arrays found\n",
            "-> Removing system a001811 from the list of systems\n",
            "System g001001 : The key-value 'pv_azimut:269-89' is not a number for array 1\n",
            "-> Removing system g001001 from the list of systems\n",
            "Number of systems with all the necessary data: 356/451\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>2026250</th>\n",
              "      <th>2026251</th>\n",
              "      <th>2026258</th>\n",
              "      <th>2026269</th>\n",
              "      <th>2026271</th>\n",
              "      <th>a001017</th>\n",
              "      <th>a001018</th>\n",
              "      <th>a001020</th>\n",
              "      <th>a001021</th>\n",
              "      <th>a001022</th>\n",
              "      <th>...</th>\n",
              "      <th>a001633</th>\n",
              "      <th>a001634</th>\n",
              "      <th>a001637</th>\n",
              "      <th>a001638</th>\n",
              "      <th>a001661</th>\n",
              "      <th>g001002</th>\n",
              "      <th>g001003</th>\n",
              "      <th>g001004</th>\n",
              "      <th>g001005</th>\n",
              "      <th>g001006</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2023-03-20</th>\n",
              "      <td>47.1</td>\n",
              "      <td>76.5</td>\n",
              "      <td>123.3</td>\n",
              "      <td>51.70</td>\n",
              "      <td>20.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>43.7</td>\n",
              "      <td>46.4</td>\n",
              "      <td>25.1</td>\n",
              "      <td>32.40</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>29.7</td>\n",
              "      <td>65.2</td>\n",
              "      <td>81.7</td>\n",
              "      <td>84.7</td>\n",
              "      <td>58.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-21</th>\n",
              "      <td>53.4</td>\n",
              "      <td>77.7</td>\n",
              "      <td>121.2</td>\n",
              "      <td>60.20</td>\n",
              "      <td>24.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>40.5</td>\n",
              "      <td>54.4</td>\n",
              "      <td>25.4</td>\n",
              "      <td>40.50</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.3</td>\n",
              "      <td>78.9</td>\n",
              "      <td>71.1</td>\n",
              "      <td>103.3</td>\n",
              "      <td>68.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-22</th>\n",
              "      <td>70.5</td>\n",
              "      <td>97.0</td>\n",
              "      <td>158.9</td>\n",
              "      <td>75.10</td>\n",
              "      <td>29.8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>57.4</td>\n",
              "      <td>68.5</td>\n",
              "      <td>32.6</td>\n",
              "      <td>52.30</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>39.4</td>\n",
              "      <td>97.5</td>\n",
              "      <td>83.7</td>\n",
              "      <td>127.5</td>\n",
              "      <td>84.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-23</th>\n",
              "      <td>25.8</td>\n",
              "      <td>47.2</td>\n",
              "      <td>55.5</td>\n",
              "      <td>21.80</td>\n",
              "      <td>9.2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19.6</td>\n",
              "      <td>24.2</td>\n",
              "      <td>12.1</td>\n",
              "      <td>18.30</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.9</td>\n",
              "      <td>30.6</td>\n",
              "      <td>28.0</td>\n",
              "      <td>50.6</td>\n",
              "      <td>39.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-24</th>\n",
              "      <td>9.7</td>\n",
              "      <td>19.1</td>\n",
              "      <td>24.5</td>\n",
              "      <td>9.60</td>\n",
              "      <td>4.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.8</td>\n",
              "      <td>13.3</td>\n",
              "      <td>5.4</td>\n",
              "      <td>8.60</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.8</td>\n",
              "      <td>14.6</td>\n",
              "      <td>21.9</td>\n",
              "      <td>16.1</td>\n",
              "      <td>15.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-07-06</th>\n",
              "      <td>26.7</td>\n",
              "      <td>55.5</td>\n",
              "      <td>69.7</td>\n",
              "      <td>32.75</td>\n",
              "      <td>13.0</td>\n",
              "      <td>84.8</td>\n",
              "      <td>19.8</td>\n",
              "      <td>28.1</td>\n",
              "      <td>12.7</td>\n",
              "      <td>18.32</td>\n",
              "      <td>...</td>\n",
              "      <td>35.9</td>\n",
              "      <td>50.95</td>\n",
              "      <td>38.6</td>\n",
              "      <td>29.6</td>\n",
              "      <td>43.74</td>\n",
              "      <td>NaN</td>\n",
              "      <td>38.1</td>\n",
              "      <td>42.9</td>\n",
              "      <td>49.2</td>\n",
              "      <td>44.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-07-07</th>\n",
              "      <td>38.5</td>\n",
              "      <td>76.7</td>\n",
              "      <td>89.1</td>\n",
              "      <td>33.24</td>\n",
              "      <td>24.7</td>\n",
              "      <td>109.9</td>\n",
              "      <td>36.4</td>\n",
              "      <td>38.6</td>\n",
              "      <td>20.5</td>\n",
              "      <td>30.53</td>\n",
              "      <td>...</td>\n",
              "      <td>60.9</td>\n",
              "      <td>60.53</td>\n",
              "      <td>52.6</td>\n",
              "      <td>41.3</td>\n",
              "      <td>47.46</td>\n",
              "      <td>NaN</td>\n",
              "      <td>47.4</td>\n",
              "      <td>61.8</td>\n",
              "      <td>101.0</td>\n",
              "      <td>62.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-07-08</th>\n",
              "      <td>73.6</td>\n",
              "      <td>133.0</td>\n",
              "      <td>190.5</td>\n",
              "      <td>85.23</td>\n",
              "      <td>40.1</td>\n",
              "      <td>200.9</td>\n",
              "      <td>61.8</td>\n",
              "      <td>78.3</td>\n",
              "      <td>35.4</td>\n",
              "      <td>54.48</td>\n",
              "      <td>...</td>\n",
              "      <td>102.0</td>\n",
              "      <td>124.97</td>\n",
              "      <td>105.7</td>\n",
              "      <td>73.5</td>\n",
              "      <td>113.71</td>\n",
              "      <td>23.0</td>\n",
              "      <td>102.4</td>\n",
              "      <td>105.9</td>\n",
              "      <td>188.6</td>\n",
              "      <td>116.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-07-09</th>\n",
              "      <td>72.9</td>\n",
              "      <td>129.1</td>\n",
              "      <td>178.4</td>\n",
              "      <td>82.22</td>\n",
              "      <td>40.3</td>\n",
              "      <td>216.5</td>\n",
              "      <td>58.8</td>\n",
              "      <td>77.8</td>\n",
              "      <td>33.5</td>\n",
              "      <td>51.86</td>\n",
              "      <td>...</td>\n",
              "      <td>100.2</td>\n",
              "      <td>121.30</td>\n",
              "      <td>101.8</td>\n",
              "      <td>71.5</td>\n",
              "      <td>103.27</td>\n",
              "      <td>42.3</td>\n",
              "      <td>100.2</td>\n",
              "      <td>102.5</td>\n",
              "      <td>186.1</td>\n",
              "      <td>110.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-07-10</th>\n",
              "      <td>40.2</td>\n",
              "      <td>68.6</td>\n",
              "      <td>100.6</td>\n",
              "      <td>35.39</td>\n",
              "      <td>20.6</td>\n",
              "      <td>112.1</td>\n",
              "      <td>35.3</td>\n",
              "      <td>45.1</td>\n",
              "      <td>22.4</td>\n",
              "      <td>29.05</td>\n",
              "      <td>...</td>\n",
              "      <td>62.1</td>\n",
              "      <td>59.38</td>\n",
              "      <td>57.5</td>\n",
              "      <td>39.7</td>\n",
              "      <td>56.43</td>\n",
              "      <td>21.7</td>\n",
              "      <td>50.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>106.3</td>\n",
              "      <td>61.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>479 rows × 356 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            2026250  2026251  2026258  2026269  2026271  a001017  a001018  \\\n",
              "Date                                                                        \n",
              "2023-03-20     47.1     76.5    123.3    51.70     20.5      NaN     43.7   \n",
              "2023-03-21     53.4     77.7    121.2    60.20     24.1      NaN     40.5   \n",
              "2023-03-22     70.5     97.0    158.9    75.10     29.8      NaN     57.4   \n",
              "2023-03-23     25.8     47.2     55.5    21.80      9.2      NaN     19.6   \n",
              "2023-03-24      9.7     19.1     24.5     9.60      4.6      NaN      6.8   \n",
              "...             ...      ...      ...      ...      ...      ...      ...   \n",
              "2024-07-06     26.7     55.5     69.7    32.75     13.0     84.8     19.8   \n",
              "2024-07-07     38.5     76.7     89.1    33.24     24.7    109.9     36.4   \n",
              "2024-07-08     73.6    133.0    190.5    85.23     40.1    200.9     61.8   \n",
              "2024-07-09     72.9    129.1    178.4    82.22     40.3    216.5     58.8   \n",
              "2024-07-10     40.2     68.6    100.6    35.39     20.6    112.1     35.3   \n",
              "\n",
              "            a001020  a001021  a001022  ...  a001633  a001634  a001637  \\\n",
              "Date                                   ...                              \n",
              "2023-03-20     46.4     25.1    32.40  ...      NaN      NaN      NaN   \n",
              "2023-03-21     54.4     25.4    40.50  ...      NaN      NaN      NaN   \n",
              "2023-03-22     68.5     32.6    52.30  ...      NaN      NaN      NaN   \n",
              "2023-03-23     24.2     12.1    18.30  ...      NaN      NaN      NaN   \n",
              "2023-03-24     13.3      5.4     8.60  ...      NaN      NaN      NaN   \n",
              "...             ...      ...      ...  ...      ...      ...      ...   \n",
              "2024-07-06     28.1     12.7    18.32  ...     35.9    50.95     38.6   \n",
              "2024-07-07     38.6     20.5    30.53  ...     60.9    60.53     52.6   \n",
              "2024-07-08     78.3     35.4    54.48  ...    102.0   124.97    105.7   \n",
              "2024-07-09     77.8     33.5    51.86  ...    100.2   121.30    101.8   \n",
              "2024-07-10     45.1     22.4    29.05  ...     62.1    59.38     57.5   \n",
              "\n",
              "            a001638  a001661  g001002  g001003  g001004  g001005  g001006  \n",
              "Date                                                                       \n",
              "2023-03-20      NaN      NaN     29.7     65.2     81.7     84.7     58.6  \n",
              "2023-03-21      NaN      NaN     30.3     78.9     71.1    103.3     68.4  \n",
              "2023-03-22      NaN      NaN     39.4     97.5     83.7    127.5     84.4  \n",
              "2023-03-23      NaN      NaN     13.9     30.6     28.0     50.6     39.8  \n",
              "2023-03-24      NaN      NaN      6.8     14.6     21.9     16.1     15.9  \n",
              "...             ...      ...      ...      ...      ...      ...      ...  \n",
              "2024-07-06     29.6    43.74      NaN     38.1     42.9     49.2     44.0  \n",
              "2024-07-07     41.3    47.46      NaN     47.4     61.8    101.0     62.2  \n",
              "2024-07-08     73.5   113.71     23.0    102.4    105.9    188.6    116.3  \n",
              "2024-07-09     71.5   103.27     42.3    100.2    102.5    186.1    110.5  \n",
              "2024-07-10     39.7    56.43     21.7     50.5      NaN    106.3     61.8  \n",
              "\n",
              "[479 rows x 356 columns]"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cacheFilename_systemsData_MeasuredDailyEnergy = os.path.join(dataCacheDirpath, 'systemsData_MeasuredDailyEnergy.pkl')\n",
        "if useCached and os.path.exists(cacheFilename_systemsData_MeasuredDailyEnergy):\n",
        "    print(f\"Loading cached data in {cacheFilename_systemsData_MeasuredDailyEnergy}\")\n",
        "    systemsData_MeasuredDailyEnergy = pd.read_pickle(cacheFilename_systemsData_MeasuredDailyEnergy)\n",
        "    systemsName_Valid = systemsData_MeasuredDailyEnergy.columns\n",
        "else:\n",
        "    # Load all csv files from the data directory\n",
        "    systemsData = {}\n",
        "    for file in os.listdir(dataDirpath):\n",
        "        if file.endswith(\".csv\"):\n",
        "            systemName = file.split(\"_\")[0]\n",
        "            systemsData[systemName] = pd.read_csv(os.path.join(dataDirpath, file))\n",
        "            systemsData[systemName]['Datetime'] = pd.to_datetime(systemsData[systemName]['Timestamp'], unit='ms', utc=True).dt.tz_convert('Europe/Zurich')\n",
        "            systemsData[systemName]['Date'] = (systemsData[systemName]['Datetime'] + pd.Timedelta(hours=1)).dt.date  # Convert the datetime to only the date, as the production is the daily production. The +1h is to manage the saving time. Normally PRiOT exports the data at midnight (local time) for the day after (e.g. the energy for the July 1st is saved at July 1st 00:00 Europe/Zurich). However it seams that the saving time is not always correctly handled, and sometime the export is done at 23:00 the day before (e.g. the energy for the July 1st is saved at June 30th 23:00 Europe/Zurich). This is why we add 1h to the datetime to be sure to have the correct date.\n",
        "\n",
        "    systemsName = list(systemsData.keys())\n",
        "\n",
        "    df_duplicate_list = list()\n",
        "    for systemName, systemData in systemsData.items():\n",
        "        # Save duplicate dates to log list, and the in a log file\n",
        "        duplicates = systemData[systemData['Date'].duplicated(keep=False)]\n",
        "        if len(duplicates) > 0:\n",
        "            df_duplicate_list.append(duplicates)\n",
        "\n",
        "            # Remove duplicate date where tt_forward_active_energy_total_toDay is the smallest\n",
        "            # TODO maybe we should sum the energy of the duplicates instead of removing the smallest one. However, when looking in PRiOT Portal, it seams that in the daily energy, only the biggest value is represented. We do the same here.\n",
        "            systemData.sort_values('tt_forward_active_energy_total_toDay', ascending=True, inplace=True)\n",
        "            systemsData[systemName].drop_duplicates(subset='Date', keep='last', inplace=True)\n",
        "\n",
        "        # Set date as the index and sort the data by date\n",
        "        systemsData[systemName].set_index('Date', inplace=True)\n",
        "        systemData.sort_index(ascending=True, inplace=True)\n",
        "\n",
        "    # Save duplicate dates to log file\n",
        "    df_duplicate = pd.concat(df_duplicate_list)\n",
        "    print(f\"Number of duplicate dates found: {len(df_duplicate)} (see log file for more details)\")\n",
        "    df_duplicate.to_csv(os.path.join(logsDirpath, 'duplicateDates.csv'), index=True)\n",
        "\n",
        "    ## ----------------------------------------------- ##\n",
        "    ## Convert data & Filter out invalid PRiOT systems ##\n",
        "    ## ----------------------------------------------- ##\n",
        "\n",
        "    systemsName_Valid = systemsName.copy()\n",
        "    for systemName in systemsName:\n",
        "        missingData = False\n",
        "        # Check if the system has measures\n",
        "        if len(systemsData[systemName]) == 0:\n",
        "            missingData = True\n",
        "            print(f\"System {systemName} : No measures found\")\n",
        "        # Check if the system has metadata\n",
        "        if systemName not in systemsMetadata:\n",
        "            missingData = True\n",
        "            print(f\"System {systemName} : No metadata found\")\n",
        "\n",
        "        else:\n",
        "            # Check metadata for the system\n",
        "            for key in ['loc_latitude', 'loc_longitude', 'loc_altitude', 'pv_kwp']:\n",
        "                # test that the key is present\n",
        "                if key not in systemsMetadata[systemName]['metadata']:\n",
        "                    missingData = True\n",
        "                    print(f\"System {systemName} : No '{key}' found\")\n",
        "                # if present, convert the value to a number, if possible\n",
        "                elif not isinstance(systemsMetadata[systemName]['metadata'][key], (int, float)):\n",
        "                    try:\n",
        "                        systemsMetadata[systemName]['metadata'][key] = int(systemsMetadata[systemName]['metadata'][key])\n",
        "                    except ValueError:\n",
        "                        try:\n",
        "                            systemsMetadata[systemName]['metadata'][key] = float(systemsMetadata[systemName]['metadata'][key])\n",
        "                        except ValueError:\n",
        "                            missingData = True\n",
        "                            print(f\"System {systemName} : The key-value '{key}:{systemsMetadata[systemName]['metadata'][key]}' is not a number\")\n",
        "\n",
        "            # Check metadata for the arrays\n",
        "            if 'arrays' not in systemsMetadata[systemName] or len(systemsMetadata[systemName]['arrays']) == 0:\n",
        "                print(f\"System {systemName} : No PV arrays found\")\n",
        "                missingData = True\n",
        "            else:\n",
        "                for array_num, arrayData in systemsMetadata[systemName]['arrays'].items():\n",
        "                    for key in ['pv_tilt', 'pv_azimut', 'pv_wp', 'pv_number']:\n",
        "                        if key not in arrayData:\n",
        "                            missingData = True\n",
        "                            print(f\"System {systemName} : No '{key}' found for array {array_num}\")\n",
        "                        # test that the value is a number\n",
        "                        elif not isinstance(arrayData[key], (int, float)):\n",
        "                            try:\n",
        "                                arrayData[key] = int(arrayData[key])\n",
        "                            except ValueError:\n",
        "                                try:\n",
        "                                    arrayData[key] = float(arrayData[key])\n",
        "                                except ValueError:\n",
        "                                    missingData = True\n",
        "                                    print(f\"System {systemName} : The key-value '{key}:{arrayData[key]}' is not a number for array {array_num}\")\n",
        "\n",
        "            # add the loss metadata if not present\n",
        "            if 'loss' not in systemsMetadata[systemName]['metadata']:\n",
        "                systemsMetadata[systemName]['metadata']['loss'] = 0\n",
        "\n",
        "        if missingData:\n",
        "            systemsName_Valid.remove(systemName)\n",
        "            print(f\"-> Removing system {systemName} from the list of systems\")\n",
        "\n",
        "    print(f\"Number of systems with all the necessary data: {len(systemsName_Valid)}/{len(systemsName)}\")\n",
        "\n",
        "    # # Filter out systems with less than X days of data\n",
        "    # for systemName in systemsName_Valid[:]:  # Create a copy of the list using slicing [:] to avoid removing elements while iterating over the list itself\n",
        "    #     if len(systemsData[systemName]) < minMeasurements:\n",
        "    #         systemsName_Valid.remove(systemName)\n",
        "    #         print(f\"-> Removing system {systemName} from the list of systems because it has less than {minMeasurements} days of data\")\n",
        "\n",
        "    # print(f\"Number of systems with at least {minMeasurements} days of data: {len(systemsName_Valid)}/{len(systemsName)}\")\n",
        "\n",
        "    ## ---------------------------------------------------------------------------- ##\n",
        "    ## Create one 2D DataFrame with the daily production of every remaining systems ##\n",
        "    ## ---------------------------------------------------------------------------- ##\n",
        "\n",
        "    # Create an empty list to store all measured data for each systems\n",
        "    systemsData_MeasuredDailyEnergy_List = []\n",
        "\n",
        "    # Iterate over each key-value pair in the systemsData dictionary\n",
        "    for systemName in systemsName_Valid:\n",
        "        # Extract the 'tt_forward_active_energy_total_toDay' column from the current dataframe\n",
        "        measuredDailyEnergy = systemsData[systemName]['tt_forward_active_energy_total_toDay']\n",
        "\n",
        "        # Rename the column with the system name\n",
        "        measuredDailyEnergy.rename(systemName, inplace=True)\n",
        "\n",
        "        systemsData_MeasuredDailyEnergy_List.append(measuredDailyEnergy)\n",
        "        # Concatenate the column to the new_dataframe\n",
        "\n",
        "    # Concatenate all the columns in the list to create one dataframe\n",
        "    systemsData_MeasuredDailyEnergy = pd.concat(systemsData_MeasuredDailyEnergy_List, axis=1)\n",
        "    systemsData_MeasuredDailyEnergy.index = pd.to_datetime(systemsData_MeasuredDailyEnergy.index)\n",
        "    systemsData_MeasuredDailyEnergy.sort_index(inplace=True)\n",
        "\n",
        "    ## ------------------ ##\n",
        "    ## Save the dataframe ##\n",
        "    ## ------------------ ##\n",
        "    # Save the dataframe for later use\n",
        "    # create cache directory if it does not exist\n",
        "\n",
        "    systemsData_MeasuredDailyEnergy.to_pickle(cacheFilename_systemsData_MeasuredDailyEnergy)\n",
        "\n",
        "# Print the dataframe\n",
        "systemsData_MeasuredDailyEnergy"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# Assuming systemsData_MeasuredDailyEnergy is already defined\n",
        "\n",
        "# To store the regressors and their evaluation metrics\n",
        "lin_regressors = {}\n",
        "r2_scores = {}\n",
        "\n",
        "for systemName in systemsData_MeasuredDailyEnergy.columns:\n",
        "    val = systemsData_MeasuredDailyEnergy[['a001035', systemName]].dropna()\n",
        "    if not len(val):\n",
        "        continue\n",
        "    X = val[[systemName]]\n",
        "    y = val['a001035']\n",
        "    \n",
        "    # Fit the linear regression model\n",
        "    linreg = LinearRegression().fit(X, y)\n",
        "    lin_regressors[systemName] = linreg\n",
        "    \n",
        "    # Make predictions   \n",
        "    r2_scores[systemName] = linreg.score(X, y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Create a list of index in r2_scores where the value is more than 0.9\n",
        "high_r2_scores = r2_scores[r2_scores > 0.9].index\n",
        "print(\"number of systems with R^2 > 0.95:\", len(high_r2_scores))\n",
        "# Prepare the feature matrix X and the target vector y\n",
        "X = filtered_data.drop(columns='a001035')\n",
        "y = filtered_data['a001035']\n",
        "\n",
        "# Remove observations where y is NA\n",
        "X = X[~y.isna()]\n",
        "y = y[~y.isna()]\n",
        "\n",
        "# Create an empty DataFrame to store the predictions\n",
        "inter_pred = pd.DataFrame(index=y.index, columns=high_r2_scores)\n",
        "\n",
        "for systemName in high_r2_scores:\n",
        "    if systemName not in lin_regressors:\n",
        "        continue\n",
        "    \n",
        "    # Drop observations with NA values in the current feature\n",
        "    valid_X = X[[systemName]].dropna()\n",
        "    \n",
        "    # Get the indices of valid observations\n",
        "    valid_indices = valid_X.index\n",
        "    \n",
        "    # Make predictions only for valid observations\n",
        "    inter_pred.loc[valid_indices, systemName] = lin_regressors[systemName].predict(valid_X)\n",
        "\n",
        "# Display the inter_pred DataFrame\n",
        "inter_pred\n",
        "\n",
        "# For each day (index) in the inter_pred DataFrame, calculate the mean of the predictions\n",
        "pred = inter_pred.mean(axis=1)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "from sklearn.linear_model import LinearRegression, RANSACRegressor\n",
        "\n",
        "class SplittedLinearRegression:\n",
        "    def __init__(self, min_score=0.9):\n",
        "        self.regressors = None\n",
        "        self.scores = None\n",
        "        self.min_score = min_score\n",
        "    def fit(self, X, y):\n",
        "        self.regressors = {}\n",
        "        self.scores = pd.Series(index=X.columns)\n",
        "        # fit a linear regression for each feature in X\n",
        "        for featureName in X.columns:\n",
        "            # Manage NA value: Keep only observations where both systems (X and y) have values\n",
        "            val = pd.concat([X[featureName], y], axis=1).dropna()\n",
        "            if not len(val):\n",
        "                continue\n",
        "            X_test = val[[featureName]]\n",
        "            y_test = val[y]\n",
        "\n",
        "            # Fit the linear regression model\n",
        "            linreg = LinearRegression().fit(X_test, y_test)\n",
        "            self.regressors[featureName] = linreg\n",
        "\n",
        "            # Make predictions   \n",
        "            self.scores[featureName] = linreg.score(X_test, y_test)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Make predictions for each features in X where the score is above the threshold min_score\n",
        "        high_scores_features = self.scores[self.scores > self.min_score].index\n",
        "\n",
        "        # Create an empty DataFrame to store the intermediate predictions\n",
        "        inter_y_pred = pd.DataFrame(index=X.index, columns=high_sco res_features)\n",
        "        for featureName in high_scores_features:\n",
        "            if featureName not in self.regressors:\n",
        "                continue\n",
        "            # Drop observations with NA values in the current feature\n",
        "            valid_X = X[[featureName]].dropna()\n",
        "            # Get the indices of valid observations\n",
        "            valid_indices = valid_X.index\n",
        "            # Make predictions only for valid observations\n",
        "            inter_y_pred.loc[valid_indices, featureName] = self.regressors[featureName].predict(valid_X)\n",
        "        # Return the mean of the predictions\n",
        "        return inter_y_pred.mean(axis=1)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression, RANSACRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# create a mask to remove outliers of the shape of systemsData_MeasuredDailyEnergy\n",
        "inlier_masks = pd.DataFrame(True, index=systemsData_MeasuredDailyEnergy.index, columns=systemsData_MeasuredDailyEnergy.columns)\n",
        "# set the mask to false for value in systemsData_MeasuredDailyEnergy under 1kWh\n",
        "inlier_masks[systemsData_MeasuredDailyEnergy < 1] = False\n",
        "\n",
        "# set the mask to false, for each column, for each value in the upper quantile (Q3 + 1.5 * IQR) of each column\n",
        "for systemName in systemsData_MeasuredDailyEnergy.columns:\n",
        "    Q1 = systemsData_MeasuredDailyEnergy[systemName].quantile(0.25)\n",
        "    Q3 = systemsData_MeasuredDailyEnergy[systemName].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    inlier_masks[systemsData_MeasuredDailyEnergy[systemName] > Q3 + 1.5 * IQR] = False\n",
        "\n",
        "filtered_data = systemsData_MeasuredDailyEnergy.copy()\n",
        "filtered_data[~inlier_masks] = np.nan\n",
        "\n",
        "X = filtered_data.drop(columns='a001035')\n",
        "y = filtered_data['a001035']\n",
        "\n",
        "# Remove observations where y is NA\n",
        "X = X[~y.isna()]\n",
        "y = y[~y.isna()]\n",
        "\n",
        "regressor = SplittedLinearRegression().fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = regressor.predict(X)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Create a figure with 52 subplots\n",
        "fig, axs = plt.subplots(13, 4, figsize=(20, 40))\n",
        "\n",
        "# Flatten the axs array to iterate over it\n",
        "axs = axs.flatten()\n",
        "\n",
        "# Iterate over each system\n",
        "for i, systemName in enumerate([name for name in systemsData_MeasuredDailyEnergy.columns if name != 'a001035']):\n",
        "    if systemName not in lin_regressors:\n",
        "        continue\n",
        "    # Plot the daily production of the current system against the daily production of all other systems\n",
        "    val = systemsData_MeasuredDailyEnergy[['a001035', systemName]].dropna()\n",
        "    X = val[[systemName]]\n",
        "    y = val['a001035']\n",
        "    \n",
        "    # Plot all\n",
        "    sns.scatterplot(x=X.values.flatten(), y=y, ax=axs[i])\n",
        "    # Plot inliers\n",
        "    # sns.scatterplot(x=X[inlier_mask].values.flatten(), y=y[inlier_mask], ax=axs[i], color='blue', label='Inliers')\n",
        "    # # Plot outliers\n",
        "    # sns.scatterplot(x=X[outlier_mask].values.flatten(), y=y[outlier_mask], ax=axs[i], color='red', label='Outliers')\n",
        "    \n",
        "    # # Plot the regression line\n",
        "    line_x = pd.DataFrame(np.linspace(X.min().iloc[0], X.max().iloc[0], 10), columns=[systemName])\n",
        "    line_y = lin_regressors[systemName].predict(line_x)\n",
        "    axs[i].plot(line_x, line_y, color='green', linewidth=2)\n",
        "    \n",
        "    axs[i].set_xlabel(systemName)\n",
        "    axs[i].set_ylabel('a001035')\n",
        "    axs[i].set_title(f'R^2: {r2_scores[systemName]:.2f}')\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Create a list of index in r2_scores where the value is more than 0.9\n",
        "high_r2_scores = r2_scores[r2_scores > 0.9].index\n",
        "print(\"number of systems with R^2 > 0.95:\", len(high_r2_scores))\n",
        "# Prepare the feature matrix X and the target vector y\n",
        "X = filtered_data.drop(columns='a001035')\n",
        "y = filtered_data['a001035']\n",
        "\n",
        "# Remove observations where y is NA\n",
        "X = X[~y.isna()]\n",
        "y = y[~y.isna()]\n",
        "\n",
        "# Create an empty DataFrame to store the predictions\n",
        "inter_pred = pd.DataFrame(index=y.index, columns=high_r2_scores)\n",
        "\n",
        "for systemName in high_r2_scores:\n",
        "    if systemName not in lin_regressors:\n",
        "        continue\n",
        "    \n",
        "    # Drop observations with NA values in the current feature\n",
        "    valid_X = X[[systemName]].dropna()\n",
        "    \n",
        "    # Get the indices of valid observations\n",
        "    valid_indices = valid_X.index\n",
        "    \n",
        "    # Make predictions only for valid observations\n",
        "    inter_pred.loc[valid_indices, systemName] = lin_regressors[systemName].predict(valid_X)\n",
        "\n",
        "# Display the inter_pred DataFrame\n",
        "inter_pred\n",
        "\n",
        "# For each day (index) in the inter_pred DataFrame, calculate the mean of the predictions\n",
        "pred = inter_pred.mean(axis=1)\n",
        "\n",
        "# plot the predictions against the target, with markers, using plotly\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=y.index, y=y, mode='markers', name='True'))\n",
        "fig.add_trace(go.Scatter(x=pred.index, y=pred, mode='markers', name='Predicted'))\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simulate Anomalies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the percentage of True values\n",
        "percentage_true = 0.05\n",
        "\n",
        "# Calculate the number of True values per column\n",
        "num_rows = testingDays\n",
        "num_true_per_column = int(num_rows * percentage_true)\n",
        "\n",
        "# Initialize the mask DataFrame with all False values\n",
        "anomalies_mask = pd.DataFrame(False, index=systemsData_MeasuredDailyEnergy[-testingDays:].index, columns=systemsData_MeasuredDailyEnergy.columns)\n",
        "\n",
        "# Randomly assign True values in each column\n",
        "for col in anomalies_mask.columns:\n",
        "    true_indices = np.random.choice(anomalies_mask.index, num_true_per_column, replace=False, )\n",
        "    anomalies_mask.loc[true_indices, col] = True\n",
        "\n",
        "\n",
        "anomalies_losses = anomalies_mask * 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "# apply anomalies to the test set\n",
        "systemsData_MeasuredDailyEnergy[-testingDays:] = systemsData_MeasuredDailyEnergy[-testingDays:] * (1 - anomalies_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create train & test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a validation set with the last 100 days\n",
        "# if testingDays == 0:\n",
        "#     systemsData_MeasuredDailyEnergy_train = systemsData_MeasuredDailyEnergy\n",
        "#     systemsData_MeasuredDailyEnergy_test = pd.DataFrame()\n",
        "# else:\n",
        "if testingDays > len(systemsData_MeasuredDailyEnergy):\n",
        "    raise ValueError(f\"testingDays ({testingDays}) is greater than the number of days in the dataset ({len(systemsData_MeasuredDailyEnergy)})\")\n",
        "systemsData_MeasuredDailyEnergy_train, systemsData_MeasuredDailyEnergy_test = train_test_split(systemsData_MeasuredDailyEnergy, test_size=testingDays, random_state=42, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System a001037 : Not enough days for training (min 7 days required)\n",
            "System a001472 : Not enough days for training (min 7 days required)\n",
            "System a001508 : Not enough days for training (min 7 days required)\n",
            "System a001543 : Not enough days for training (min 7 days required)\n",
            "System a001546 : Not enough days for training (min 7 days required)\n",
            "System a001548 : Not enough days for training (min 7 days required)\n",
            "System a001553 : Not enough days for training (min 7 days required)\n",
            "System a001557 : Not enough days for training (min 7 days required)\n",
            "System a001560 : Not enough days for training (min 7 days required)\n",
            "System a001569 : Not enough days for training (min 7 days required)\n",
            "System a001570 : Not enough days for training (min 7 days required)\n",
            "System a001581 : Not enough days for training (min 7 days required)\n",
            "System a001589 : Not enough days for training (min 7 days required)\n",
            "System a001593 : Not enough days for training (min 7 days required)\n",
            "System a001596 : Not enough days for training (min 7 days required)\n",
            "System a001599 : Not enough days for training (min 7 days required)\n",
            "System a001605 : Not enough days for training (min 7 days required)\n",
            "System a001614 : Not enough days for training (min 7 days required)\n",
            "System a001634 : Not enough days for training (min 7 days required)\n",
            "System a001637 : Not enough days for training (min 7 days required)\n",
            "System a001661 : Not enough days for training (min 7 days required)\n",
            "System a001041 : Not enough days for testing (min 30 days required)\n",
            "System a001099 : Not enough days for testing (min 30 days required)\n",
            "System a001180 : Not enough days for testing (min 30 days required)\n"
          ]
        }
      ],
      "source": [
        "# remove systems with not enough days that are not null for training or testing\n",
        "for systemName in systemsData_MeasuredDailyEnergy_train.loc[:, systemsData_MeasuredDailyEnergy_train.notnull().sum() < minTrainingDays].columns:\n",
        "\n",
        "    remove_system(systemName, f\"System {systemName} : Not enough days for training (min {minTrainingDays} days required)\")\n",
        "\n",
        "for systemName in systemsData_MeasuredDailyEnergy_test.loc[:, systemsData_MeasuredDailyEnergy_test.notnull().sum() < minTestingDays].columns:\n",
        "    remove_system(systemName, f\"System {systemName} : Not enough days for testing (min {minTestingDays} days required)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Max production estimator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the power production with a given frequency to the total daily energy\n",
        "def daily_energy(df_power):\n",
        "    # Get the frequency in minutes\n",
        "    freq_in_minutes = pd.Timedelta(df_power.index.freq).seconds / 60\n",
        "    # Convert power from kW to kWh\n",
        "    df_energy = df_power * (freq_in_minutes / 60)\n",
        "    # Resample to daily frequency and sum the values\n",
        "    daily_energy = df_energy.resample('D').sum()\n",
        "    # daily_energy.index = daily_energy.index.date\n",
        "\n",
        "    return daily_energy\n",
        "\n",
        "# Simulate the daily production of a system from a start date to an end date using the given PVLib ModelChain\n",
        "\n",
        "\n",
        "def generate_max_production_estimate(startDate, endDate, estimator: ModelChain, samplingFreq='1h'):\n",
        "    # The end date needs to be estimated completly(end date at 23:59). But \"endDate\" is considered as 00:00 by pd.date_range().\n",
        "    # So we add 1 day to the end date to include the entire end date in the date_range(), and then we exclude the last value with the inclusive='left' proprety, to remove \"endDate+1\" at 00:00) in the date_range().\n",
        "    endDate = endDate + pd.Timedelta(days=1)\n",
        "\n",
        "    times = pd.date_range(start=startDate, end=endDate, freq=samplingFreq, tz=estimator.location.tz, inclusive='left')\n",
        "    weatherClearSky = estimator.location.get_clearsky(times)  # In W/m2\n",
        "    # TODO adjust the clear sky model to take into account the horizon https://pvlib-python.readthedocs.io/en/stable/gallery/shading/plot_simple_irradiance_adjustment_for_horizon_shading.html\n",
        "    estimator.run_model(weatherClearSky)\n",
        "    production = estimator.results.ac / 1000  # Convert W to kW\n",
        "    dailyProduction = daily_energy(production)\n",
        "    dailyProduction.index = pd.to_datetime(dailyProduction.index.date)\n",
        "    return dailyProduction\n",
        "\n",
        "\n",
        "def generate_max_production_estimator(systemMetadata):\n",
        "    latitude = systemMetadata['metadata']['loc_latitude']\n",
        "    longitude = systemMetadata['metadata']['loc_longitude']\n",
        "    altitude = systemMetadata['metadata']['loc_altitude']\n",
        "    Wp_Tot = systemMetadata['metadata']['pv_kwp'] * 1000\n",
        "    loss = systemMetadata['metadata']['loss'] * 100\n",
        "\n",
        "    arrays = []\n",
        "    for array_num, arrayData in systemMetadata['arrays'].items():\n",
        "        array = Array(\n",
        "            mount=FixedMount(surface_tilt=arrayData['pv_tilt'], surface_azimuth=arrayData['pv_azimut'], racking_model='open_rack'),\n",
        "            module_parameters={'pdc0': arrayData['pv_wp'], 'gamma_pdc': -0.004},\n",
        "            module_type='glass_polymer',\n",
        "            modules_per_string=arrayData['pv_number'],\n",
        "            strings=1,\n",
        "            temperature_model_parameters=TEMPERATURE_MODEL_PARAMETERS['sapm']['open_rack_glass_polymer'],\n",
        "        )\n",
        "        arrays.append(array)\n",
        "\n",
        "    location = Location(latitude=latitude, longitude=longitude, altitude=altitude, tz='Europe/Zurich')\n",
        "    system = PVSystem(arrays=arrays,\n",
        "                      inverter_parameters={'pdc0': Wp_Tot, 'eta_inv_nom': 0.96},\n",
        "                      losses_parameters={'nameplate_rating': loss, 'soiling': 0, 'shading': 0, 'snow': 0, 'mismatch': 0, 'wiring': 0, 'connections': 0, 'lid': 0, 'age': 0, 'availability': 0})\n",
        "    modelChain = ModelChain(system, location, clearsky_model='ineichen', aoi_model='no_loss', spectral_model=\"no_loss\", losses_model='pvwatts')\n",
        "\n",
        "    return modelChain\n",
        "\n",
        "\n",
        "def tune_max_production_estimator(measured_series, max_estimated_series, window=7):\n",
        "    # Remove the obvious outliers. It's important before calculating the std, which can be strongly impacted by the strong outliers.\n",
        "    outliers_mask = measured_series > 2 * max_estimated_series\n",
        "    measured_no_outliers_series = measured_series[~outliers_mask]\n",
        "    # if 10% of the data is removed as outliers, we consider that the system is not valid\n",
        "    if outliers_mask.sum().sum() / outliers_mask.size > 0.1:\n",
        "        return None, None, None, None\n",
        "    # Keep only the max measured value\n",
        "    max_measured_series = pd.Series(index=measured_series.index, dtype=float)\n",
        "    # Iterate over windows of a given size, and keep only the maximum value in each window\n",
        "    for i in range(0, len(measured_series), window):\n",
        "        window_data = measured_no_outliers_series.iloc[i:i + window]\n",
        "        if not window_data.empty and not window_data.isna().all():\n",
        "            max_value = window_data.max()\n",
        "            max_index = window_data.idxmax(skipna=True)\n",
        "            max_measured_series[max_index] = max_value\n",
        "\n",
        "    # Calculate the relative difference between the maximum measured and maximum estimated value\n",
        "    realtive_difference = max_measured_series / max_estimated_series\n",
        "\n",
        "    # Compute statistics\n",
        "    std = realtive_difference.std()\n",
        "    mean = realtive_difference.mean()\n",
        "\n",
        "    # Remove the outilers that have a z-score greater than 1\n",
        "    z_scores = np.abs(realtive_difference - mean) / std\n",
        "\n",
        "    # Add the measure with a z-score greater than 1 to the previous outliers (AND operation)\n",
        "    outliers_mask = outliers_mask | (z_scores > 1)\n",
        "\n",
        "    # Get the loss that overestimate the estimate maximum daily energy\n",
        "    loss = 1 - realtive_difference[~outliers_mask].max()\n",
        "\n",
        "    return loss, std, max_measured_series, outliers_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create estimator\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "\n",
        "systemsData_EstimatedMaxDailyEnergy_dic = {}\n",
        "unfitted_systems = []\n",
        "for systemName in tqdm(systemsName_Valid):\n",
        "\n",
        "    systemsMetadata[systemName]['metadata']['loss'] = 0\n",
        "\n",
        "    estimator = generate_max_production_estimator(systemsMetadata[systemName])\n",
        "\n",
        "    ## ------------------- ##\n",
        "    ## Simulate production ##\n",
        "    ## ------------------- ##\n",
        "    measured_series = systemsData_MeasuredDailyEnergy[systemName]\n",
        "    startDate = measured_series[~measured_series.isna()].index.min()\n",
        "    endDate = measured_series[~measured_series.isna()].index.max()\n",
        "    estimatedMaxDailyEnergy = generate_max_production_estimate(startDate, endDate, estimator, samplingFreq='1h')\n",
        "\n",
        "        # fill remaining days with NaN\n",
        "    estimatedMaxDailyEnergy = estimatedMaxDailyEnergy.reindex(measured_series.index, fill_value=np.nan)\n",
        "\n",
        "        # add the series to the dictionary\n",
        "    systemsData_EstimatedMaxDailyEnergy_dic[systemName] = estimatedMaxDailyEnergy\n",
        "\n",
        "    # loss, std = tune_max_production_estimator(measured_series, estimatedMaxDailyEnergy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Concatenate all the columns in the list to create one dataframe\n",
        "systemsData_EstimatedMaxDailyEnergy = pd.concat(systemsData_EstimatedMaxDailyEnergy_dic, axis=1)\n",
        "systemsData_EstimatedMaxDailyEnergy.index = pd.to_datetime(systemsData_EstimatedMaxDailyEnergy.index)\n",
        "systemsData_EstimatedMaxDailyEnergy.sort_index(inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading cached data in C:\\Users\\conta\\Berner Fachhochschule\\TI IEM PVLab PA-BA-MA - 2024_FS_CIS-MA-PV-Meter_Data_Analysis - 2024_FS_CIS-MA-PV-Meter_Data_Analysis\\07_Data\\databases\\PRiOT\\dataExport_2\\cache\\systemsData_EstimatedMaxDailyEnergy.pkl\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>2026250</th>\n",
              "      <th>2026251</th>\n",
              "      <th>2026258</th>\n",
              "      <th>2026269</th>\n",
              "      <th>2026271</th>\n",
              "      <th>a001017</th>\n",
              "      <th>a001018</th>\n",
              "      <th>a001020</th>\n",
              "      <th>a001021</th>\n",
              "      <th>a001022</th>\n",
              "      <th>...</th>\n",
              "      <th>a001623</th>\n",
              "      <th>a001624</th>\n",
              "      <th>a001625</th>\n",
              "      <th>a001633</th>\n",
              "      <th>a001638</th>\n",
              "      <th>g001002</th>\n",
              "      <th>g001003</th>\n",
              "      <th>g001004</th>\n",
              "      <th>g001005</th>\n",
              "      <th>g001006</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2023-03-20</th>\n",
              "      <td>70.888989</td>\n",
              "      <td>95.336762</td>\n",
              "      <td>164.184834</td>\n",
              "      <td>78.452140</td>\n",
              "      <td>30.376840</td>\n",
              "      <td>NaN</td>\n",
              "      <td>60.615294</td>\n",
              "      <td>73.873859</td>\n",
              "      <td>34.826828</td>\n",
              "      <td>58.214068</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>41.749462</td>\n",
              "      <td>100.159754</td>\n",
              "      <td>82.698311</td>\n",
              "      <td>124.438180</td>\n",
              "      <td>80.333151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-21</th>\n",
              "      <td>71.264898</td>\n",
              "      <td>96.521976</td>\n",
              "      <td>165.472712</td>\n",
              "      <td>78.911319</td>\n",
              "      <td>30.698535</td>\n",
              "      <td>NaN</td>\n",
              "      <td>60.969196</td>\n",
              "      <td>74.338863</td>\n",
              "      <td>34.992887</td>\n",
              "      <td>58.406191</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>41.966318</td>\n",
              "      <td>100.552129</td>\n",
              "      <td>83.536288</td>\n",
              "      <td>126.037822</td>\n",
              "      <td>81.302952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-22</th>\n",
              "      <td>71.635194</td>\n",
              "      <td>97.703614</td>\n",
              "      <td>166.742508</td>\n",
              "      <td>79.363039</td>\n",
              "      <td>31.019467</td>\n",
              "      <td>NaN</td>\n",
              "      <td>61.315983</td>\n",
              "      <td>74.799466</td>\n",
              "      <td>35.155068</td>\n",
              "      <td>58.592192</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>42.180589</td>\n",
              "      <td>100.936241</td>\n",
              "      <td>84.372736</td>\n",
              "      <td>127.634776</td>\n",
              "      <td>82.268455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-23</th>\n",
              "      <td>71.999616</td>\n",
              "      <td>98.885005</td>\n",
              "      <td>167.993447</td>\n",
              "      <td>79.806344</td>\n",
              "      <td>31.339167</td>\n",
              "      <td>NaN</td>\n",
              "      <td>61.655443</td>\n",
              "      <td>75.255347</td>\n",
              "      <td>35.313119</td>\n",
              "      <td>58.772028</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>42.391936</td>\n",
              "      <td>101.311083</td>\n",
              "      <td>85.206992</td>\n",
              "      <td>129.227605</td>\n",
              "      <td>83.228758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-24</th>\n",
              "      <td>72.357927</td>\n",
              "      <td>100.065710</td>\n",
              "      <td>169.224871</td>\n",
              "      <td>80.240469</td>\n",
              "      <td>31.657152</td>\n",
              "      <td>NaN</td>\n",
              "      <td>61.987403</td>\n",
              "      <td>75.706208</td>\n",
              "      <td>35.466839</td>\n",
              "      <td>58.945679</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>42.600060</td>\n",
              "      <td>101.675845</td>\n",
              "      <td>86.038310</td>\n",
              "      <td>130.814868</td>\n",
              "      <td>84.183033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-07-06</th>\n",
              "      <td>83.185355</td>\n",
              "      <td>156.286307</td>\n",
              "      <td>216.079947</td>\n",
              "      <td>93.654375</td>\n",
              "      <td>43.976330</td>\n",
              "      <td>247.806622</td>\n",
              "      <td>70.785363</td>\n",
              "      <td>91.611577</td>\n",
              "      <td>38.679248</td>\n",
              "      <td>60.498934</td>\n",
              "      <td>...</td>\n",
              "      <td>103.585458</td>\n",
              "      <td>114.677673</td>\n",
              "      <td>76.194899</td>\n",
              "      <td>105.923430</td>\n",
              "      <td>75.765813</td>\n",
              "      <td>49.036988</td>\n",
              "      <td>107.727989</td>\n",
              "      <td>127.335693</td>\n",
              "      <td>206.992937</td>\n",
              "      <td>129.228550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-07-07</th>\n",
              "      <td>83.127291</td>\n",
              "      <td>155.993437</td>\n",
              "      <td>215.772194</td>\n",
              "      <td>93.552374</td>\n",
              "      <td>43.939366</td>\n",
              "      <td>247.875290</td>\n",
              "      <td>70.759369</td>\n",
              "      <td>91.490535</td>\n",
              "      <td>38.647102</td>\n",
              "      <td>60.474055</td>\n",
              "      <td>...</td>\n",
              "      <td>103.498158</td>\n",
              "      <td>114.612248</td>\n",
              "      <td>76.043658</td>\n",
              "      <td>105.761748</td>\n",
              "      <td>75.661299</td>\n",
              "      <td>48.997777</td>\n",
              "      <td>107.648891</td>\n",
              "      <td>127.197147</td>\n",
              "      <td>206.633137</td>\n",
              "      <td>128.959225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-07-08</th>\n",
              "      <td>83.066360</td>\n",
              "      <td>155.682826</td>\n",
              "      <td>215.449770</td>\n",
              "      <td>93.445723</td>\n",
              "      <td>43.899008</td>\n",
              "      <td>247.946040</td>\n",
              "      <td>70.731730</td>\n",
              "      <td>91.364191</td>\n",
              "      <td>38.613683</td>\n",
              "      <td>60.448477</td>\n",
              "      <td>...</td>\n",
              "      <td>103.406466</td>\n",
              "      <td>114.543562</td>\n",
              "      <td>75.883166</td>\n",
              "      <td>105.591870</td>\n",
              "      <td>75.550270</td>\n",
              "      <td>48.956724</td>\n",
              "      <td>107.567283</td>\n",
              "      <td>127.047433</td>\n",
              "      <td>206.249675</td>\n",
              "      <td>128.675676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-07-09</th>\n",
              "      <td>83.002497</td>\n",
              "      <td>155.360955</td>\n",
              "      <td>215.117931</td>\n",
              "      <td>93.334334</td>\n",
              "      <td>43.855235</td>\n",
              "      <td>248.018628</td>\n",
              "      <td>70.702395</td>\n",
              "      <td>91.232514</td>\n",
              "      <td>38.578960</td>\n",
              "      <td>60.422127</td>\n",
              "      <td>...</td>\n",
              "      <td>103.310337</td>\n",
              "      <td>114.471552</td>\n",
              "      <td>75.713471</td>\n",
              "      <td>105.413825</td>\n",
              "      <td>75.432732</td>\n",
              "      <td>48.913794</td>\n",
              "      <td>107.483042</td>\n",
              "      <td>126.886526</td>\n",
              "      <td>205.842658</td>\n",
              "      <td>128.377928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-07-10</th>\n",
              "      <td>82.935634</td>\n",
              "      <td>155.024098</td>\n",
              "      <td>214.803521</td>\n",
              "      <td>93.218119</td>\n",
              "      <td>43.808021</td>\n",
              "      <td>248.092797</td>\n",
              "      <td>70.671310</td>\n",
              "      <td>91.095472</td>\n",
              "      <td>38.542906</td>\n",
              "      <td>60.394926</td>\n",
              "      <td>...</td>\n",
              "      <td>103.209727</td>\n",
              "      <td>114.396151</td>\n",
              "      <td>75.534625</td>\n",
              "      <td>105.227649</td>\n",
              "      <td>75.312210</td>\n",
              "      <td>48.868951</td>\n",
              "      <td>107.396039</td>\n",
              "      <td>NaN</td>\n",
              "      <td>205.412205</td>\n",
              "      <td>128.066011</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>479 rows × 326 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              2026250     2026251     2026258    2026269    2026271  \\\n",
              "Date                                                                  \n",
              "2023-03-20  70.888989   95.336762  164.184834  78.452140  30.376840   \n",
              "2023-03-21  71.264898   96.521976  165.472712  78.911319  30.698535   \n",
              "2023-03-22  71.635194   97.703614  166.742508  79.363039  31.019467   \n",
              "2023-03-23  71.999616   98.885005  167.993447  79.806344  31.339167   \n",
              "2023-03-24  72.357927  100.065710  169.224871  80.240469  31.657152   \n",
              "...               ...         ...         ...        ...        ...   \n",
              "2024-07-06  83.185355  156.286307  216.079947  93.654375  43.976330   \n",
              "2024-07-07  83.127291  155.993437  215.772194  93.552374  43.939366   \n",
              "2024-07-08  83.066360  155.682826  215.449770  93.445723  43.899008   \n",
              "2024-07-09  83.002497  155.360955  215.117931  93.334334  43.855235   \n",
              "2024-07-10  82.935634  155.024098  214.803521  93.218119  43.808021   \n",
              "\n",
              "               a001017    a001018    a001020    a001021    a001022  ...  \\\n",
              "Date                                                                ...   \n",
              "2023-03-20         NaN  60.615294  73.873859  34.826828  58.214068  ...   \n",
              "2023-03-21         NaN  60.969196  74.338863  34.992887  58.406191  ...   \n",
              "2023-03-22         NaN  61.315983  74.799466  35.155068  58.592192  ...   \n",
              "2023-03-23         NaN  61.655443  75.255347  35.313119  58.772028  ...   \n",
              "2023-03-24         NaN  61.987403  75.706208  35.466839  58.945679  ...   \n",
              "...                ...        ...        ...        ...        ...  ...   \n",
              "2024-07-06  247.806622  70.785363  91.611577  38.679248  60.498934  ...   \n",
              "2024-07-07  247.875290  70.759369  91.490535  38.647102  60.474055  ...   \n",
              "2024-07-08  247.946040  70.731730  91.364191  38.613683  60.448477  ...   \n",
              "2024-07-09  248.018628  70.702395  91.232514  38.578960  60.422127  ...   \n",
              "2024-07-10  248.092797  70.671310  91.095472  38.542906  60.394926  ...   \n",
              "\n",
              "               a001623     a001624    a001625     a001633    a001638  \\\n",
              "Date                                                                   \n",
              "2023-03-20         NaN         NaN        NaN         NaN        NaN   \n",
              "2023-03-21         NaN         NaN        NaN         NaN        NaN   \n",
              "2023-03-22         NaN         NaN        NaN         NaN        NaN   \n",
              "2023-03-23         NaN         NaN        NaN         NaN        NaN   \n",
              "2023-03-24         NaN         NaN        NaN         NaN        NaN   \n",
              "...                ...         ...        ...         ...        ...   \n",
              "2024-07-06  103.585458  114.677673  76.194899  105.923430  75.765813   \n",
              "2024-07-07  103.498158  114.612248  76.043658  105.761748  75.661299   \n",
              "2024-07-08  103.406466  114.543562  75.883166  105.591870  75.550270   \n",
              "2024-07-09  103.310337  114.471552  75.713471  105.413825  75.432732   \n",
              "2024-07-10  103.209727  114.396151  75.534625  105.227649  75.312210   \n",
              "\n",
              "              g001002     g001003     g001004     g001005     g001006  \n",
              "Date                                                                   \n",
              "2023-03-20  41.749462  100.159754   82.698311  124.438180   80.333151  \n",
              "2023-03-21  41.966318  100.552129   83.536288  126.037822   81.302952  \n",
              "2023-03-22  42.180589  100.936241   84.372736  127.634776   82.268455  \n",
              "2023-03-23  42.391936  101.311083   85.206992  129.227605   83.228758  \n",
              "2023-03-24  42.600060  101.675845   86.038310  130.814868   84.183033  \n",
              "...               ...         ...         ...         ...         ...  \n",
              "2024-07-06  49.036988  107.727989  127.335693  206.992937  129.228550  \n",
              "2024-07-07  48.997777  107.648891  127.197147  206.633137  128.959225  \n",
              "2024-07-08  48.956724  107.567283  127.047433  206.249675  128.675676  \n",
              "2024-07-09  48.913794  107.483042  126.886526  205.842658  128.377928  \n",
              "2024-07-10  48.868951  107.396039         NaN  205.412205  128.066011  \n",
              "\n",
              "[479 rows x 326 columns]"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cacheFilename_systemsData_EstimatedMaxDailyEnergy = os.path.join(dataCacheDirpath, 'systemsData_EstimatedMaxDailyEnergy.pkl')\n",
        "\n",
        "# if useCached and os.path.exists(cacheFilename_systemsData_EstimatedMaxDailyEnergy):\n",
        "if True and os.path.exists(cacheFilename_systemsData_EstimatedMaxDailyEnergy):\n",
        "    # TODO how to deal if the cached data is not up to date and some systems have been added or removed?\n",
        "    print(f\"Loading cached data in {cacheFilename_systemsData_EstimatedMaxDailyEnergy}\")\n",
        "    systemsData_EstimatedMaxDailyEnergy = pd.read_pickle(cacheFilename_systemsData_EstimatedMaxDailyEnergy)\n",
        "else:\n",
        "    systemsData_EstimatedMaxDailyEnergy_dic = {}\n",
        "    systemsData_Tuning_EstimatedMaxDailyEnergy_untuned_dic = {}\n",
        "    systemsData_Tuning_MeasureMax_dic = {}\n",
        "    systemsData_Tuning_Outliers_dic = {}\n",
        "\n",
        "    unfitted_systems = []\n",
        "    for systemName in tqdm(systemsName_Valid):\n",
        "        tuned = not tuneMaxProductionEstimators  # If we don't want to tune the estimators, we say that the estimator is already tuned\n",
        "        # reset the loss in the metadata if we want to tune the estimators\n",
        "        if tuneMaxProductionEstimators:\n",
        "            systemsMetadata[systemName]['metadata']['loss'] = 0\n",
        "\n",
        "        while True:  # emulate do while loop\n",
        "\n",
        "            ## ------------------ ##\n",
        "            ## Create ModelChains ##\n",
        "            ## ------------------ ##\n",
        "            estimator = generate_max_production_estimator(systemsMetadata[systemName])\n",
        "\n",
        "            ## ------------------- ##\n",
        "            ## Simulate production ##\n",
        "            ## ------------------- ##\n",
        "            measured_series = systemsData_MeasuredDailyEnergy[systemName]\n",
        "            startDate = measured_series[~measured_series.isna()].index.min()\n",
        "            endDate = measured_series[~measured_series.isna()].index.max()\n",
        "            estimatedMaxDailyEnergy = generate_max_production_estimate(startDate, endDate, estimator, samplingFreq='1h')\n",
        "\n",
        "            # fill remaining days with NaN\n",
        "            estimatedMaxDailyEnergy = estimatedMaxDailyEnergy.reindex(measured_series.index, fill_value=np.nan)\n",
        "\n",
        "            # add the series to the dictionary\n",
        "            systemsData_EstimatedMaxDailyEnergy_dic[systemName] = estimatedMaxDailyEnergy\n",
        "\n",
        "            ## --------------- ##\n",
        "            ## Tune estimators ##\n",
        "            ## --------------- ##\n",
        "            if tuned:\n",
        "                break\n",
        "\n",
        "            loss, std, measuredMax, outliersMask = tune_max_production_estimator(measured_series, estimatedMaxDailyEnergy)\n",
        "\n",
        "            if loss is None:\n",
        "                unfitted_systems.append(systemName)\n",
        "                break\n",
        "\n",
        "            systemsData_Tuning_EstimatedMaxDailyEnergy_untuned_dic[systemName] = estimatedMaxDailyEnergy\n",
        "            systemsData_Tuning_MeasureMax_dic[systemName] = measuredMax\n",
        "            systemsData_Tuning_Outliers_dic[systemName] = measured_series[outliersMask]\n",
        "\n",
        "            # If the std is greater than 1, we remove the system from the list of systems to be processed.\n",
        "            # This is to avoid to have a system that is not well fitted by the maximum energy estimator model, and that could impact the training of the RF model.\n",
        "            if std is None or std > 1 or measured_series.count() == 0:\n",
        "                unfitted_systems.append(systemName)\n",
        "                break\n",
        "\n",
        "            # write the loss in systemsMetadata\n",
        "            systemsMetadata[systemName]['metadata']['loss'] = loss\n",
        "\n",
        "            tuned = True\n",
        "\n",
        "    systemsData_EstimatedMaxDailyEnergy = pd.concat(systemsData_EstimatedMaxDailyEnergy_dic, axis=1)\n",
        "    systemsData_EstimatedMaxDailyEnergy.index = pd.to_datetime(systemsData_EstimatedMaxDailyEnergy.index)\n",
        "    systemsData_EstimatedMaxDailyEnergy.sort_index(inplace=True)\n",
        "\n",
        "    systemsData_Tuning_EstimatedMaxDailyEnergy_untuned = pd.concat(systemsData_Tuning_EstimatedMaxDailyEnergy_untuned_dic, axis=1)\n",
        "    systemsData_Tuning_EstimatedMaxDailyEnergy_untuned.index = pd.to_datetime(systemsData_Tuning_EstimatedMaxDailyEnergy_untuned.index)\n",
        "    systemsData_Tuning_EstimatedMaxDailyEnergy_untuned.sort_index(inplace=True)\n",
        "\n",
        "    systemsData_Tuning_MeasureMax = pd.concat(systemsData_Tuning_MeasureMax_dic, axis=1)\n",
        "    systemsData_Tuning_MeasureMax.index = pd.to_datetime(systemsData_Tuning_MeasureMax.index)\n",
        "    systemsData_Tuning_MeasureMax.sort_index(inplace=True)\n",
        "\n",
        "    systemsData_Tuning_Outliers = pd.concat(systemsData_Tuning_Outliers_dic, axis=1)\n",
        "    systemsData_Tuning_Outliers.index = pd.to_datetime(systemsData_Tuning_Outliers.index)\n",
        "    systemsData_Tuning_Outliers.sort_index(inplace=True)\n",
        "\n",
        "    # Remove unfitted systems from systemsName_Valid, systemsName_Valid, systemsData_EstimatedMaxDailyEnergy, systemsData_MeasuredDailyEnergy\n",
        "    for systemName in unfitted_systems:\n",
        "        remove_system(systemName, f\"System {systemName} : We can't find the model corresponding to the measured data. This system is removed from the list of systems to be processed.\")\n",
        "\n",
        "    # Save the dataframe to a CSV file\n",
        "    systemsData_EstimatedMaxDailyEnergy.to_pickle(cacheFilename_systemsData_EstimatedMaxDailyEnergy)\n",
        "\n",
        "    # Save metadata with tuned parameters\n",
        "    if tuneMaxProductionEstimators:\n",
        "        with open(metadataFilepath, 'w') as f:\n",
        "            json.dump(systemsMetadata, f, indent=4)\n",
        "\n",
        "    # save systemsData_EstimatedMaxDailyEnergy in cacheFilename_systemsData_EstimatedMaxDailyEnergy\n",
        "    systemsData_EstimatedMaxDailyEnergy.to_pickle(cacheFilename_systemsData_EstimatedMaxDailyEnergy)\n",
        "\n",
        "\n",
        "# Print the dataframe\n",
        "systemsData_EstimatedMaxDailyEnergy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remove outliers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System a001087 : The system has less than 7 days of data. This system is removed from the list of systems to be processed.\n",
            "System a001223 : The system has less than 7 days of data. This system is removed from the list of systems to be processed.\n",
            "System a001433 : The system has less than 7 days of data. This system is removed from the list of systems to be processed.\n",
            "System a001456 : The system has less than 7 days of data. This system is removed from the list of systems to be processed.\n",
            "System a001486 : The system has less than 7 days of data. This system is removed from the list of systems to be processed.\n",
            "System a001489 : The system has less than 7 days of data. This system is removed from the list of systems to be processed.\n"
          ]
        }
      ],
      "source": [
        "rel_mesasured_series = systemsData_MeasuredDailyEnergy_train / systemsData_EstimatedMaxDailyEnergy\n",
        "\n",
        "# remove the outliers in measured data that are greater than 1.1 times (+10%) the maximum estimated value, or less than 1% of the maximum estimated value\n",
        "inliers = (rel_mesasured_series < 1.1) & (rel_mesasured_series > 0.01)\n",
        "systemsData_MeasuredDailyEnergy_train_outliers = systemsData_MeasuredDailyEnergy_train[~inliers]\n",
        "systemsData_MeasuredDailyEnergy_train = systemsData_MeasuredDailyEnergy_train[inliers]\n",
        "\n",
        "# remove the systems that have less than 7 days\n",
        "for systemName in systemsData_MeasuredDailyEnergy_train.loc[:, systemsData_MeasuredDailyEnergy_train.count() < minTrainingDays].columns:\n",
        "    remove_system(systemName, f\"System {systemName} : The system has less than {minTrainingDays} days of data. This system is removed from the list of systems to be processed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Relative production\n",
        "\n",
        "True production scaled by the maximum production from the simulator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the relative energy for each system\n",
        "systemsData_RelativeMeasuredDailyEnergy_train = systemsData_MeasuredDailyEnergy_train / systemsData_EstimatedMaxDailyEnergy\n",
        "systemsData_RelativeMeasuredDailyEnergy = systemsData_MeasuredDailyEnergy / systemsData_EstimatedMaxDailyEnergy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare the difference between simulation with hourly and 10min sampling rate\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Simulate the daily production for each system with 1h and 10min sampling rate\n",
        "dailyProductions = {}\n",
        "\n",
        "for systemName, modelChain in modelChains.items():\n",
        "    try:\n",
        "        dailyProduction_hour = simulateDailyProduction(systemName, systemsData, modelChains, samplingFreq='1h')\n",
        "        dailyProduction_min = simulateDailyProduction(systemName, systemsData, modelChains, samplingFreq='10min')\n",
        "        dailyProductions[systemName] = pd.DataFrame({'Simulator hour': dailyProduction_hour, 'Simulator 10min': dailyProduction_min})\n",
        "    except Exception as e:\n",
        "        print(f\"Error for system {systemName}: {e}\")\n",
        "        continue\n",
        "\n",
        "\n",
        "# Compute the descriptive statistics of the difference between the 1h and 10min simulations on all systems\n",
        "allSimulations = pd.concat(dailyProductions.values())\n",
        "\n",
        "allSimulations['Difference'] = allSimulations['Simulator hour'] - allSimulations['Simulator 10min']\n",
        "allSimulations['Percentage'] = allSimulations['Difference'] / allSimulations['Simulator 10min'] * 100\n",
        "\n",
        "descriptiveStat = allSimulations[['Difference', 'Percentage']].describe()\n",
        "print(descriptiveStat)\n",
        "\n",
        "\n",
        "# Plot the histogram of the percentage of the difference\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Histogram(x=allSimulations['Percentage'], nbinsx=1000))\n",
        "fig.add_vline(x=descriptiveStat.loc['mean','Percentage'], line_color='red', line_width=2, annotation_text='mean')\n",
        "fig.add_vline(x=descriptiveStat.loc['25%','Percentage'], line_color='green', line_width=2, annotation_text='25%')\n",
        "fig.add_vline(x=descriptiveStat.loc['75%','Percentage'], line_color='green', line_width=2, annotation_text='75%')\n",
        "fig.update_xaxes(dtick=0.1)\n",
        "fig.update_xaxes(range=[-1, 1])\n",
        "fig.update_layout(width=1000, height=666)\n",
        "fig.update_layout(xaxis_title='Percentage of the difference (%)')\n",
        "fig.show()\n",
        "\n",
        "\n",
        "# Plot the daily production of a system with 1h and 10min sampling rate\n",
        "systemName = 'a001096'\n",
        "dailyProduction_hour = simulateDailyProduction(systemName, systemsData, modelChains, samplingFreq='1h')\n",
        "dailyProduction_min = simulateDailyProduction(systemName, systemsData, modelChains, samplingFreq='10min')\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=dailyProduction_min.index, y=dailyProduction_min, mode='markers', name='10min sampling rate'))\n",
        "fig.add_trace(go.Scatter(x=dailyProduction_hour.index, y=dailyProduction_hour, mode='markers', name='Hourly sampling rate'))\n",
        "\n",
        "fig.update_layout(title=f'Simulated daily max AC energy of system {systemName}', xaxis_title='Time', yaxis_title='Energy (kWh)')\n",
        "fig.update_layout(width=1000, height=666)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "test"
        ]
      },
      "source": [
        "## Correlation between Systems\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "correlation_matrix = systemsData_MeasuredDailyEnergy.corr(method='pearson', min_periods=minTrainingDays)\n",
        "# set all negative value (therefore when the value of one system increasse, the other systme decrease) to 0\n",
        "correlation_matrix[correlation_matrix < 0] = 0"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Plot correlation matrix\n",
        "\n",
        "# Set the size of the figure\n",
        "plt.figure(figsize=(100, 80))\n",
        "\n",
        "# Creating the heatmap without modifying the default behavior of axis labels\n",
        "sns.heatmap(correlation_matrix, cmap='coolwarm', xticklabels=True, yticklabels=True)\n",
        "\n",
        "# Rotating the tick labels for readability\n",
        "plt.xticks(rotation=90,)\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot linear regression between 2 systems\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.linear_model import RANSACRegressor, LinearRegression\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Remove observation where a001266 and a001036 dataframes have NA value\n",
        "system1 = 'a001236'\n",
        "system2 = 'a001097'\n",
        "\n",
        "df = pd.concat([systemsData_RelativeMeasuredDailyEnergy_train[system1], systemsData_RelativeMeasuredDailyEnergy_train[system2]], axis=1).dropna()\n",
        "\n",
        "x_values = np.linspace(df[system1].min(), df[system1].max(), 100)\n",
        "\n",
        "# Fit line using RANSAC\n",
        "ransac = RANSACRegressor(LinearRegression(), min_samples=20, residual_threshold=None, random_state=42)\n",
        "ransac.fit(df[[system1]], df[system2] )\n",
        "y_values_ransac = ransac.predict(x_values.reshape(-1, 1))\n",
        "\n",
        "# Plot the data\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=df[system1], y=df[system2], mode='markers', name='Data'))\n",
        "fig.add_trace(go.Scatter(x=x_values, y=y_values_ransac, mode='lines', name='RANSAC Regression'))\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Half-Sibling Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_system_data(targetName, set='train', relative=True, max_neighbors=None):\n",
        "    # take the max_neighbors best neighbours from the correlation matrix\n",
        "    # if none, take all the neighbours\n",
        "    if max_neighbors == None or max_neighbors > len(systemsName_Valid) - 1:\n",
        "        max_neighbors = len(systemsName_Valid) - 1\n",
        "    best_neighbours = correlation_matrix.loc[targetName, systemsName_Valid].sort_values(ascending=False).index[1:max_neighbors + 1]\n",
        "    # Create the feature matrix X and the target vector y\n",
        "    if set == 'train' and relative:\n",
        "        X = systemsData_RelativeMeasuredDailyEnergy_train[best_neighbours]\n",
        "        y = systemsData_RelativeMeasuredDailyEnergy_train[targetName]\n",
        "    elif set == 'train' and not relative:\n",
        "        X = systemsData_MeasuredDailyEnergy_train[best_neighbours]\n",
        "        y = systemsData_MeasuredDailyEnergy_train[targetName]\n",
        "    elif set == 'test' and relative:\n",
        "        X = systemsData_RelativeMeasuredDailyEnergy_test[best_neighbours]\n",
        "        y = systemsData_RelativeMeasuredDailyEnergy_test[targetName]\n",
        "    elif set == 'test' and not relative:\n",
        "        X = systemsData_MeasuredDailyEnergy_test[best_neighbours]\n",
        "        y = systemsData_MeasuredDailyEnergy_test[targetName]\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid set value: {set}\")\n",
        "    # remove the observations where their is no target value\n",
        "    X = X[~y.isna()]\n",
        "    y = y[~y.isna()]\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def mean_absolute_percentage_error_mean_denominator(\n",
        "        y_true, y_pred, *, sample_weight=None, multioutput=\"uniform_average\"):\n",
        "    # Copy of the function mean_absolute_percentage_error from sklearn.metrics._regression, with the denominator of the MAPE changed to the mean of the true values\n",
        "    import sklearn\n",
        "    y_type, y_true, y_pred, multioutput = sklearn.metrics._regression._check_reg_targets(\n",
        "        y_true, y_pred, multioutput)\n",
        "    sklearn.utils.validation.check_consistent_length(y_true, y_pred, sample_weight)\n",
        "    epsilon = np.finfo(np.float64).eps\n",
        "    mape = np.abs(y_pred - y_true) / np.maximum(np.mean(np.abs(y_true)), epsilon)\n",
        "    output_errors = np.average(mape, weights=sample_weight, axis=0)\n",
        "    if isinstance(multioutput, str):\n",
        "        if multioutput == \"raw_values\":\n",
        "            return output_errors\n",
        "        elif multioutput == \"uniform_average\":\n",
        "            # pass None as weights to np.average: uniform mean\n",
        "            multioutput = None\n",
        "    return np.average(output_errors, weights=multioutput)\n",
        "\n",
        "\n",
        "def mean_absolute_percentage_error_epsilon(y_true, y_pred, epsilon=np.finfo(np.float64).eps, *, sample_weight=None, multioutput=\"uniform_average\"):\n",
        "    # Copy of the function mean_absolute_percentage_error from sklearn.metrics._regression, with epsilon as a parameter\n",
        "    import sklearn\n",
        "    y_type, y_true, y_pred, multioutput = sklearn.metrics._regression._check_reg_targets(\n",
        "        y_true, y_pred, multioutput)\n",
        "    sklearn.utils.validation.check_consistent_length(y_true, y_pred, sample_weight)\n",
        "    mape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), epsilon)\n",
        "    output_errors = np.average(mape, weights=sample_weight, axis=0)\n",
        "    if isinstance(multioutput, str):\n",
        "        if multioutput == \"raw_values\":\n",
        "            return output_errors\n",
        "        elif multioutput == \"uniform_average\":\n",
        "            # pass None as weights to np.average: uniform mean\n",
        "            multioutput = None\n",
        "    return np.average(output_errors, weights=multioutput)\n",
        "\n",
        "\n",
        "def mad(arr):\n",
        "    return abs(arr - arr.median()).median()\n",
        "\n",
        "\n",
        "def modified_z_score(arr):\n",
        "    # based on https://www.ibm.com/docs/en/cognos-analytics/11.1.0?topic=terms-modified-z-score\n",
        "    mad_value = mad(arr)\n",
        "    if mad_value == 0:\n",
        "        MeanAD = np.mean(np.abs(arr - np.mean(arr)))\n",
        "        denominator = 1.253314 * MeanAD\n",
        "    else:\n",
        "        denominator = 1.486 * mad_value\n",
        "    return (arr - np.median(arr)) / denominator\n",
        "\n",
        "\n",
        "def metrics(y_true, y_pred):\n",
        "    return {'MAPE': mean_absolute_percentage_error(y_true, y_pred), 'MAPE-MD': mean_absolute_percentage_error_mean_denominator(y_true, y_pred), 'MAE': mean_absolute_error(y_true, y_pred), 'RMSE': root_mean_squared_error(y_true, y_pred), 'R2': r2_score(y_true, y_pred)}\n",
        "\n",
        "\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "mape_eps_scorer = make_scorer(mean_absolute_percentage_error_epsilon, greater_is_better=False)\n",
        "# Return the metrics for a RFR model trained on the given data.\n",
        "# The entire dataset is used for training, and the OOB prediction is used to compute the metrics.\n",
        "\n",
        "\n",
        "def oob_metrics(X, y, metricFct, rf_parames={}):\n",
        "    model = RandomForestRegressor(oob_score=True, **rf_parames)\n",
        "    y_pred = model.fit(X, y).oob_prediction_\n",
        "    return metricFct(y, y_pred)\n",
        "# Return the metrics for a RFR model trained on the given data.\n",
        "# KFold cross-validation is used train the model and to compute the metrics.\n",
        "\n",
        "\n",
        "def kfold_metrics(X, y, metricFct, rf_parames={}, n_folds=5):\n",
        "    model = RandomForestRegressor(**rf_parames)\n",
        "    metrics_list = []\n",
        "    for train_index, test_index in KFold(n_splits=n_folds).split(X):\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "        y_pred = model.fit(X_train, y_train).predict(X_test)\n",
        "        metrics_list.append(metricFct(y_test, y_pred))\n",
        "    if isinstance(metrics_list[0], dict):\n",
        "        # Convert list of dictionaries to a DataFrame\n",
        "        metrics_df = pd.DataFrame(metrics_list)\n",
        "        # Compute mean for each column\n",
        "        aggregated_metrics = metrics_df.mean().to_dict()\n",
        "        return aggregated_metrics\n",
        "    else:\n",
        "        # Compute mean of the list for numerical metrics\n",
        "        return np.mean(metrics_list)\n",
        "\n",
        "\n",
        "def _accumulate_prediction(predict, X, out, lock):\n",
        "    \"\"\"    This is a utility function for joblib's Parallel.    It can't go locally in ForestClassifier or ForestRegressor, because joblib    complains that it cannot pickle it when placed there.    \"\"\"\n",
        "    prediction = predict(X, check_input=False)\n",
        "    with lock:\n",
        "        out.append(prediction)\n",
        "\n",
        "\n",
        "def predict_w_std(self, X):\n",
        "    \"\"\"    Predict regression target and standard deviation for X.    The predicted regression target of an input sample is computed as the    mean predicted regression targets of the trees in the forest. The standard    deviation of the predicted regression targets of the trees in the forest    is also computed to provide an estimate of the prediction uncertainty.    Parameters    ----------    X : {array-like, sparse matrix} of shape (n_samples, n_features)        The input samples. Internally, its dtype will be converted to        ``dtype=np.float32``. If a sparse matrix is provided, it will be        converted into a sparse ``csr_matrix``.    Returns    -------    mean_predictions : ndarray of shape (n_samples,)        The predicted values (mean of the predictions from all estimators).    std_predictions : ndarray of shape (n_samples,)        The standard deviation of the predicted values (standard deviation of the        predictions from all estimators).    Raises    ------    NotImplementedError        If the model was trained for multi-output regression.    Notes    -----    This function does not support multi-output regression. If the model was    trained for multi-output regression, an exception will be raised.    \"\"\"\n",
        "    if self.n_outputs_ > 1:\n",
        "        raise NotImplementedError(\"Variance for multi-output regression is not supported now\")\n",
        "    check_is_fitted(self)\n",
        "    # Check data\n",
        "    X = self._validate_X_predict(X)\n",
        "    # Assign chunk of trees to jobs\n",
        "    n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)\n",
        "    # avoid storing the output of every estimator by summing them here\n",
        "    # Initialize a list to collect predictions from each estimator\n",
        "    all_predictions = []\n",
        "    # Parallel loop\n",
        "    lock = threading.Lock()\n",
        "    Parallel(n_jobs=n_jobs, verbose=self.verbose, require=\"sharedmem\")(delayed(_accumulate_prediction)(e.predict, X, all_predictions, lock)\n",
        "                                                                       for e in self.estimators_)\n",
        "    # Convert list to numpy array for easier manipulation\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    # Compute mean and variance across predictions from all estimators\n",
        "    mean_predictions = np.mean(all_predictions, axis=0)\n",
        "    std_predictions = np.std(all_predictions, axis=0)\n",
        "    return mean_predictions, std_predictions\n",
        "\n",
        "\n",
        "RandomForestRegressor.predict_w_std = predict_w_std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameters tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hyperparameters tuning\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Number of trees in random forest. from 1 to 200, with 20 steps\n",
        "n_estimators = np.unique(np.append(\n",
        "    [int(x) for x in np.linspace(start=1, stop=400, num=5)],\n",
        "    [int(x) for x in np.linspace(start=1, stop=100, num=10)]\n",
        "))\n",
        "# # Number of features to consider at every split\n",
        "max_features = [1, 2, 'log2', 'sqrt', None]\n",
        "# # Maximum number of levels in tree\n",
        "# max_depth = [None]\n",
        "# # Minimum number of samples required to split a node\n",
        "# min_samples_split = [2]\n",
        "# # Minimum number of samples required at each leaf node\n",
        "# min_samples_leaf = [1]\n",
        "\n",
        "\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               # 'max_depth': max_depth,\n",
        "               # 'min_samples_split': min_samples_split,\n",
        "               # 'min_samples_leaf': min_samples_leaf\n",
        "               }"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# GRID SEARCH\n",
        "from tqdm import trange\n",
        "\n",
        "gs_results = {}\n",
        "\n",
        "for systemName_i in trange(100):\n",
        "    systemName = systemsName_Valid[systemName_i]\n",
        "    # remove the target column from the features\n",
        "    X, y = get_system_data(systemName, set='train', relative=True, max_neighbors=10)\n",
        "    # Use the random grid to search for best hyperparameters\n",
        "    # First create the base model to tune\n",
        "    rf = RandomForestRegressor(random_state=random_state, max_features=1)\n",
        "    # Random search of parameters, using 5 fold cross validation,\n",
        "    # search across 100 different combinations, and use all available cores\n",
        "    rf_grid = GridSearchCV(estimator=rf, param_grid=random_grid, cv=5, n_jobs=-1, refit=False, verbose=2, return_train_score=True, scoring=mae_scorer)\n",
        "    # Fit the random search model\n",
        "    gs_results.update({systemName: rf_grid.fit(X, y).cv_results_})"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "mean_test_scores = pd.DataFrame(columns=gs_results.keys(), index=n_estimators)\n",
        "mean_train_scores = pd.DataFrame(columns=gs_results.keys(), index=n_estimators)\n",
        "mean_fit_times = pd.DataFrame(columns=gs_results.keys(), index=n_estimators)\n",
        "\n",
        "# Extract values\n",
        "for systemName in gs_results:\n",
        "    mean_fit_times[systemName] = gs_results[systemName]['mean_fit_time']\n",
        "    mean_test_scores[systemName] = gs_results[systemName]['mean_test_score']\n",
        "    mean_train_scores[systemName] = gs_results[systemName]['mean_train_score']\n",
        "\n",
        "# Compute stats\n",
        "mean_test_scores_mean = mean_test_scores.mean(axis=1)\n",
        "mean_test_scores_std = mean_test_scores.std(axis=1)\n",
        "\n",
        "mean_train_scores_mean = mean_train_scores.mean(axis=1)\n",
        "mean_train_scores_std = mean_train_scores.std(axis=1)\n",
        "\n",
        "mean_fit_times_mean = mean_fit_times.mean(axis=1)\n",
        "mean_fit_times_std = mean_fit_times.std(axis=1)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Plot results\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add the Test MAPE trace\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=n_estimators,\n",
        "    y=mean_test_scores_mean * (-100),\n",
        "    mode='lines+markers',\n",
        "    name='Test MAPE',\n",
        "    error_y=dict(type='data', array=mean_test_scores_std * 100, visible=True),\n",
        "    yaxis=\"y1\"\n",
        "))\n",
        "\n",
        "# Add the Train MAPE trace\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=n_estimators,\n",
        "    y=mean_train_scores_mean * (-100),\n",
        "    mode='lines+markers',\n",
        "    name='Train MAPE',\n",
        "    error_y=dict(type='data', array=mean_train_scores_std * 100, visible=True),\n",
        "    yaxis=\"y1\"\n",
        "))\n",
        "\n",
        "# Add the Training Time trace with secondary y-axis\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=n_estimators,\n",
        "    y=mean_fit_times_mean,\n",
        "    mode='lines+markers',\n",
        "    name='Training time',\n",
        "    error_y=dict(type='data', array=mean_fit_times_std, visible=True),\n",
        "    yaxis=\"y2\"\n",
        "))\n",
        "\n",
        "# Update layout to include the secondary y-axis\n",
        "fig.update_layout(\n",
        "    yaxis_title=\"MAPE (%)\",\n",
        "    xaxis_title=\"Number of trees\",\n",
        "    autosize=False,\n",
        "    width=1000,\n",
        "    height=666,\n",
        "    yaxis2=dict(\n",
        "        title=\"Training time (seconds)\",\n",
        "        overlaying=\"y\",\n",
        "        side=\"right\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train regressors\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Random Forest Regressor hyperparameters\n",
        "n_estimators = 100  # Number of trees in random forest\n",
        "max_features = 'log2'  # Number of features to consider at every split\n",
        "max_depth = None  # Maximum number of levels in tree\n",
        "min_samples_split = 2  # Minimum number of samples required to split a node\n",
        "min_samples_leaf = 1  # Minimum number of samples required at each leaf node\n",
        "\n",
        "targetName = systemsName_Valid[0]\n",
        "\n",
        "\n",
        "X, y = get_system_data(targetName, set='train')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "rf_regressor = RandomForestRegressor(oob_score=True, random_state=random_state, n_estimators=n_estimators, max_features=max_features, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred, y_pred_std = rf_regressor.predict_w_std(X_test)\n",
        "\n",
        "# plot with squater poot the prediction y_pred with the standard deviation y_pred_std, as well as the measured values y_val\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=X_test.index, y=y_test, mode='markers', name='Measured'))\n",
        "fig.add_trace(go.Scatter(x=X_test.index, y=y_pred, mode='markers', name='Predicted'))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training regressors: 100%|██████████| 326/326 [02:12<00:00,  2.46it/s]\n"
          ]
        }
      ],
      "source": [
        "serializer = PickleSerializer()\n",
        "\n",
        "rf_regressors = {}\n",
        "\n",
        "# Random Forest Regressor hyperparameters\n",
        "n_estimators = 100  # Number of trees in random forest\n",
        "max_features = 'log2'  # Number of features to consider at every split\n",
        "max_depth = None  # Maximum number of levels in tree\n",
        "min_samples_split = 2  # Minimum number of samples required to split a node\n",
        "min_samples_leaf = 1  # Minimum number of samples required at each leaf node\n",
        "max_neighbors = 20  # Maximum number of best neighbors to consider\n",
        "\n",
        "if useCached and not forceTrain:\n",
        "    # load the models in dataCacheDirpath/rf_regressors. The file name is the system name.\n",
        "    for systemName in systemsName_Valid:\n",
        "        try:\n",
        "            rf_regressors[systemName] = serializer.retrieve_model(os.path.join(dataCacheDirpath, 'rf_regressors'), systemName)\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "    print(f\"Loaded {len(rf_regressors)}/{len(systemsName_Valid)} models. {len(systemsName_Valid) - len(rf_regressors)} models to train.\")\n",
        "\n",
        "systemsData_RelativeExpectedDailyEnergy_train_List = []\n",
        "for targetName in tqdm(set(systemsName_Valid) - set(rf_regressors), desc='Training regressors'):\n",
        "    rf_regressor = RandomForestRegressor(oob_score=True, random_state=random_state, n_estimators=n_estimators, max_features=max_features, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
        "    X_train, y_train = get_system_data(targetName, set='train', relative=True, max_neighbors=max_neighbors)\n",
        "\n",
        "    # split the data into training and testing sets\n",
        "    rf_regressor.fit(X_train, y_train)\n",
        "    rf_regressors[targetName] = rf_regressor\n",
        "    # save the rf_regressor.oob_prediction_ in systemsData_RelativeExpectedDailyEnergy_train_List\n",
        "    systemsData_RelativeExpectedDailyEnergy_train_List.append(pd.Series(rf_regressor.oob_prediction_, index=X_train.index, name=targetName))\n",
        "    # save the model in dataCacheDirpath/rf_regressors. The file name is the system name.\n",
        "    serializer.save_model(rf_regressor, os.path.join(dataCacheDirpath, 'rf_regressors'), targetName)\n",
        "\n",
        "# Concatenate all the columns in the list to create one dataframe\n",
        "systemsData_RelativeExpectedDailyEnergy_train = pd.concat(systemsData_RelativeExpectedDailyEnergy_train_List, axis=1)\n",
        "systemsData_RelativeExpectedDailyEnergy_train.index = pd.to_datetime(systemsData_RelativeExpectedDailyEnergy_train.index)\n",
        "systemsData_RelativeExpectedDailyEnergy_train.sort_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute absolute expected daily energy\n",
        "systemsData_ExpectedDailyEnergy_train = systemsData_RelativeExpectedDailyEnergy_train * systemsData_EstimatedMaxDailyEnergy"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Compute absolute expected daily energy\n",
        "systemsData_ExpectedDailyEnergy_train = systemsData_RelativeExpectedDailyEnergy_train\n",
        "systemsData_RelativeExpectedDailyEnergy_train = systemsData_ExpectedDailyEnergy_train/systemsData_EstimatedMaxDailyEnergy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split between two dates\n",
        "\n",
        "TODO\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "targetName = 'a001286'\n",
        "# Create the feature matrix X and the target vector y\n",
        "X = systemsData_RelativeMeasuredDailyEnergy.drop(columns=targetName)\n",
        "y = systemsData_RelativeMeasuredDailyEnergy[targetName]\n",
        "# remove the observations where their is no target value\n",
        "X = X[~y.isna()]\n",
        "y = y[~y.isna()]\n",
        "\n",
        "# split the data into training and testing sets. data after the 1st June 2023 are used for training\n",
        "X_test, X_train  = X.loc[X.index < pd.Timestamp('2023-06-01')], X.loc[X.index >= pd.Timestamp('2023-06-01')]\n",
        "y_test, y_train = y.loc[y.index < pd.Timestamp('2023-06-01')], y.loc[y.index >= pd.Timestamp('2023-06-01')]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rf_regressor = RandomForestRegressor(oob_score=True, random_state=random_state, n_estimators=n_estimators, max_features=max_features, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "y_pred, y_std = rf_regressor.predict_w_std(X_test)\n",
        "\n",
        "# plot y_test, y_train, and y_pred with error bar y_std with two colors\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=y_test.index, y=y_test, mode='markers', name='y_test'))\n",
        "fig.add_trace(go.Scatter(x=y_train.index, y=y_train, mode='markers', name='y_train'))\n",
        "fig.add_trace(go.Scatter(x=y_test.index, y=y_pred, mode='markers', name='y_pred'))\n",
        "# fig.add_trace(go.Scatter(x=y_test.index, y=y_pred + y_std, mode='lines', line=dict(width=0), showlegend=False))\n",
        "# fig.add_trace(go.Scatter(x=y_test.index, y=y_pred - y_std, mode='lines', fill='tonexty', fillcolor='rgba(0,100,80,0.2)', line=dict(width=0), showlegend=False))\n",
        "fig.update_layout(title=f'{targetName} prediction with error bars', xaxis_title='Date', yaxis_title='Relative energy')\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compupte training metrics of the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "cacheFilename_regressorsMetrics_train = os.path.join(dataCacheDirpath, 'metrics_train.csv')\n",
        "\n",
        "if useCached and os.path.exists(cacheFilename_regressorsMetrics_train):\n",
        "    # TODO how to deal if the cached data is not up to date and some systems have been added or removed?\n",
        "    print(f\"Loading cached data in {cacheFilename_regressorsMetrics_train}\")\n",
        "    regressorsMetrics = pd.read_csv(cacheFilename_regressorsMetrics_train, index_col=0).squeeze()\n",
        "else:\n",
        "\n",
        "    # regressorsMetrics_mape_train = pd.Series(index=systemsName_Valid, name='Train MAE')\n",
        "    regressorsMetrics_mape_scaled_train = pd.Series(index=systemsName_Valid, name='Train MAPE Normalized')\n",
        "\n",
        "    for targetName in systemsName_Valid:\n",
        "\n",
        "        X_train, y_train = get_system_data(targetName, set='train', relative=True)\n",
        "\n",
        "        rf_regressor = rf_regressors[targetName]\n",
        "        y_pred = pd.Series(rf_regressor.oob_prediction_, index=X_train.index, name=targetName)\n",
        "        y_train_scaled = y_train  # (y_train / systemsData_EstimatedMaxDailyEnergy[targetName]).dropna()\n",
        "        y_pred_scaled = y_pred  # (y_pred / systemsData_EstimatedMaxDailyEnergy[targetName]).dropna()\n",
        "\n",
        "        # regressorsMetrics_mae_train.loc[targetName] = mean_absolute_error(y_train, y_pred)\n",
        "        # regressorsMetrics_mape_train.loc[targetName] = mean_absolute_percentage_error_epsilon(y_train, y_pred, epsilon=1)\n",
        "\n",
        "        regressorsMetrics_mape_scaled_train.loc[targetName] = mean_absolute_error(y_train_scaled, y_pred_scaled)\n",
        "        # regressorsMetrics_mape_scaled_train.loc[targetName] = mean_absolute_percentage_error_epsilon(y_train_scaled, y_pred_scaled, epsilon=0.01)\n",
        "\n",
        "    # save the metrics\n",
        "\n",
        "    # regressorsMetrics_mae_train.to_csv(cacheFilename_regressorsMetrics_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute feature importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "compute_permutation_importance = False\n",
        "\n",
        "cacheFilename_features_importance = os.path.join(dataCacheDirpath, 'features_importance.csv')\n",
        "cacheFilename_permutation_importance_mean = os.path.join(dataCacheDirpath, 'permutation_importance_mean.csv')\n",
        "cacheFilename_permutation_importance_std = os.path.join(dataCacheDirpath, 'permutation_importance_std.csv')\n",
        "\n",
        "# start = time.time()\n",
        "\n",
        "if useCached and os.path.exists(cacheFilename_features_importance):\n",
        "    print(f\"Loading cached data in {cacheFilename_features_importance}\")\n",
        "    features_importance_df = pd.read_csv(cacheFilename_features_importance, index_col=0)\n",
        "else:\n",
        "    features_importance_df = pd.DataFrame(index=systemsName_Valid, columns=systemsName_Valid)\n",
        "    for targetName in systemsName_Valid:\n",
        "        rf_regressor = rf_regressors[targetName]\n",
        "        features_importance_df.loc[targetName, rf_regressor.feature_names_in_] = rf_regressor.feature_importances_\n",
        "    # save the feature importances\n",
        "    features_importance_df.to_csv(cacheFilename_features_importance)\n",
        "\n",
        "\n",
        "if compute_permutation_importance:\n",
        "    if useCached and os.path.exists(cacheFilename_permutation_importance_mean) and os.path.exists(cacheFilename_permutation_importance_std):\n",
        "        print(f\"Loading cached data in {cacheFilename_permutation_importance_mean}\")\n",
        "        permutation_importance_mean_df = pd.read_csv(cacheFilename_permutation_importance_mean, index_col=0)\n",
        "        print(f\"Loading cached data in {cacheFilename_permutation_importance_std}\")\n",
        "        permutation_importance_std_df = pd.read_csv(cacheFilename_permutation_importance_std, index_col=0)\n",
        "    else:\n",
        "        permutation_importance_mean_df = pd.DataFrame(index=systemsName_Valid, columns=systemsName_Valid)\n",
        "        permutation_importance_std_df = pd.DataFrame(index=systemsName_Valid, columns=systemsName_Valid)\n",
        "        for targetName in tqdm(systemsName_Valid):\n",
        "            X, y = get_system_data(targetName)\n",
        "            rf_regressor = rf_regressors[targetName]\n",
        "            permutation_importance_results = permutation_importance(rf_regressor, X, y, n_repeats=5, random_state=random_state, n_jobs=-1, scoring=mae_scorer)\n",
        "            permutation_importance_mean_df.loc[targetName, X.columns] = permutation_importance_results.importances_mean\n",
        "            permutation_importance_std_df.loc[targetName, X.columns] = permutation_importance_results.importances_std\n",
        "        # save the permutation importances\n",
        "        permutation_importance_mean_df.to_csv(cacheFilename_permutation_importance_mean)\n",
        "        permutation_importance_std_df.to_csv(cacheFilename_permutation_importance_std)\n",
        "\n",
        "\n",
        "# print(f\"Time elapsed: {time.time() - start} - Time per system: {(time.time() - start) / len(systemsName_Valid)}\")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "serializer = PickleSerializer()\n",
        "\n",
        "rf_regressors = {}\n",
        "forceTrain = True\n",
        "if not forceTrain:\n",
        "    # load the models in dataCacheDirpath/rf_regressors. The file name is the system name.\n",
        "    for systemName in systemsName_Valid:\n",
        "        try:\n",
        "            rf_regressors[systemName] = serializer.retrieve_model(os.path.join(dataCacheDirpath, 'rf_regressors'), systemName)\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "    print(f\"Loaded {len(rf_regressors)}/{len(systemsName_Valid)} models. {len(systemsName_Valid) - len(rf_regressors)} models to train.\")\n",
        "\n",
        "# Train a Random Forest Regressor model to predict the daily energy production of a system based on the daily energy production of the other systems\n",
        "metrics_df = pd.DataFrame(index=systemsName_Valid, columns=['MAPE', 'MAPE-MD', 'MAE', 'RMSE', 'R2'])\n",
        "features_importance_df = pd.DataFrame(index=systemsName_Valid, columns=systemsName_Valid)\n",
        "permutation_importance_df = pd.DataFrame(index=systemsName_Valid, columns=systemsName_Valid)\n",
        "\n",
        "for targetName in tqdm(set(systemsName_Valid) - set(rf_regressors), desc='Training regressors'):\n",
        "    rf_regressor = RandomForestRegressor(n_estimators=100, oob_score=True, random_state=random_state)\n",
        "    # remove the target column from the features\n",
        "    X = systemsData_MeasuredRelativeDailyEnergy.drop(columns=targetName)\n",
        "    y = systemsData_MeasuredRelativeDailyEnergy[targetName]\n",
        "    # remove the observations where their is no target value\n",
        "    X = X[~systemsData_MeasuredRelativeDailyEnergy[targetName].isna()]\n",
        "    y = y[~systemsData_MeasuredRelativeDailyEnergy[targetName].isna()]\n",
        "    # split the data into training and testing sets\n",
        "    # TODo utiliser OOB\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=random_state) # Not necessary to split the data, as the OOB can be used to estimate the error\n",
        "    # train the regressor\n",
        "    y_oob_pred = rf_regressor.fit(X, y).oob_prediction_\n",
        "    # save the feature importances\n",
        "    features_importance_df.loc[targetName, X.columns] = rf_regressor.feature_importances_\n",
        "    # permutation_importance_df.loc[targetName, X.columns] = permutation_importance(rf_regressor, X_test, y_test, n_repeats=10, random_state=random_state, n_jobs=-1).importances_mean\n",
        "\n",
        "    # test the regressor\n",
        "    # y_mean = rf_regressor.predict(X_test)\n",
        "\n",
        "    # y_pred_V_IJ_unbiased = fci.random_forest_error(rf_regressor, X_train, X_test)\n",
        "\n",
        "    # compute the metrics\n",
        "    metrics_df.loc[targetName] = metrics(y, y_oob_pred)\n",
        "\n",
        "    # save the model\n",
        "    # serializer.save_model(rf_regressor, os.path.join(dataCacheDirpath, 'rf_regressors'), targetName)\n",
        "    rf_regressors[targetName] = rf_regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate expected value for each systems\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Systems not trained: []\n"
          ]
        }
      ],
      "source": [
        "# valid system that have been trained\n",
        "systems_trained = [systemName for systemName in systemsName_Valid if systemName in rf_regressors]\n",
        "\n",
        "print(\"Systems not trained:\", [systemName for systemName in systemsName_Valid if systemName not in rf_regressors])"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# generate max value\n",
        "\n",
        "systemsData_EstimatedMaxDailyEnergy_test_dic = {}\n",
        "for systemName in tqdm(systemsName_Valid):\n",
        "    estimator = generate_max_production_estimator(systemsMetadata[systemName])\n",
        "    measured_series = systemsData_MeasuredDailyEnergy_test[systemName]\n",
        "    if measured_series.count() == 0:\n",
        "        continue\n",
        "    startDate = measured_series[~measured_series.isna()].index.min()\n",
        "    endDate = measured_series[~measured_series.isna()].index.max()\n",
        "    estimatedMaxDailyEnergy = generate_max_production_estimate(startDate, endDate, estimator, samplingFreq='1h')\n",
        "\n",
        "    # fill remaining days with NaN\n",
        "    estimatedMaxDailyEnergy = estimatedMaxDailyEnergy.reindex(measured_series.index, fill_value=np.nan)\n",
        "\n",
        "    # add the series to the dictionary\n",
        "    systemsData_EstimatedMaxDailyEnergy_test_dic[systemName] = estimatedMaxDailyEnergy\n",
        "\n",
        "\n",
        "# Concatenate all the columns in the list to create one dataframe\n",
        "systemsData_EstimatedMaxDailyEnergy_test = pd.concat(systemsData_EstimatedMaxDailyEnergy_test_dic, axis=1)\n",
        "systemsData_EstimatedMaxDailyEnergy_test.index = pd.to_datetime(systemsData_EstimatedMaxDailyEnergy_test.index)\n",
        "systemsData_EstimatedMaxDailyEnergy_test.sort_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check that all the index in systemsData_MeasuredDailyEnergy_test are in systemsData_EstimatedMaxDailyEnergy\n",
        "if not systemsData_MeasuredDailyEnergy_test.index.isin(systemsData_EstimatedMaxDailyEnergy.index).all():\n",
        "    raise ValueError(\"Some index in systemsData_MeasuredDailyEnergy_test are not in systemsData_EstimatedMaxDailyEnergy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute relative value\n",
        "systemsData_RelativeMeasuredDailyEnergy_test = systemsData_MeasuredDailyEnergy_test / systemsData_EstimatedMaxDailyEnergy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 326/326 [00:07<00:00, 42.16it/s]\n"
          ]
        }
      ],
      "source": [
        "# compute estimate and metrics\n",
        "\n",
        "systemsData_RelativeExpectedDailyEnergy_test_mean_List = []\n",
        "systemsData_RelativeExpectedDailyEnergy_test_std_List = []\n",
        "\n",
        "# regressorsMetrics_mae_test = pd.Series(index=systems_trained, name='Test MAE')\n",
        "# regressorsMetrics_mape_test = pd.Series(index=systems_trained, name='Test MAPE')\n",
        "regressorsMetrics_mape_scaled_test = pd.Series(index=systems_trained, name='Test MAPE Normalized')\n",
        "# regressorsMetrics_mape_scaled_test = pd.Series(index=systems_trained, name='Test MAPE Scaled')\n",
        "\n",
        "for targetName in tqdm(systems_trained):\n",
        "    X_test, y_test = get_system_data(targetName, set='test', relative=True)\n",
        "    # check that there is at least one observation\n",
        "    if y_test.count() == 0:\n",
        "        continue\n",
        "    regressor = rf_regressors[targetName]\n",
        "    fitted_features = regressor.feature_names_in_\n",
        "\n",
        "    # adjust the feature in the validation set to match the feature in the training set\n",
        "    # Identify extra columns in X_test that are not used by the regressor\n",
        "    extra_features = set(X_test.columns) - set(fitted_features)\n",
        "    # Drop extra columns from X_val\n",
        "    X_test = X_test.drop(columns=list(extra_features), errors='ignore')\n",
        "    # Identify missing columns in X_test and add them as empty columns\n",
        "    missing_features = set(fitted_features) - set(X_test.columns)\n",
        "    for feature in missing_features:\n",
        "        X_test[feature] = np.nan\n",
        "\n",
        "    y_mean, y_std = regressor.predict_w_std(X_test)\n",
        "    y_mean = pd.Series(y_mean, index=X_test.index, name=targetName)\n",
        "    y_std = pd.Series(y_std, index=X_test.index, name=targetName)\n",
        "    systemsData_RelativeExpectedDailyEnergy_test_mean_List.append(y_mean)\n",
        "    systemsData_RelativeExpectedDailyEnergy_test_std_List.append(y_std)\n",
        "\n",
        "    # metrics\n",
        "    y_test_scaled = y_test  # (y_test / systemsData_EstimatedMaxDailyEnergy[targetName]).dropna()\n",
        "    y_mean_scaled = y_mean  # (y_mean / systemsData_EstimatedMaxDailyEnergy[targetName]).dropna()\n",
        "\n",
        "    # regressorsMetrics_mae_test.loc[targetName] = mean_absolute_error(y_test, y_mean)\n",
        "    # regressorsMetrics_mape_test.loc[targetName] = mean_absolute_percentage_error_epsilon(y_test, y_mean, epsilon=1)\n",
        "\n",
        "    regressorsMetrics_mape_scaled_test.loc[targetName] = mean_absolute_error(y_test_scaled, y_mean_scaled)\n",
        "    # regressorsMetrics_mape_scaled_test.loc[targetName] = mean_absolute_percentage_error_epsilon(y_test_scaled, y_mean_scaled, epsilon=0.01)\n",
        "\n",
        "\n",
        "systemsData_RelativeExpectedDailyEnergy_test_mean = pd.concat(systemsData_RelativeExpectedDailyEnergy_test_mean_List, axis=1)\n",
        "systemsData_RelativeExpectedDailyEnergy_test_mean.index = pd.to_datetime(systemsData_RelativeExpectedDailyEnergy_test_mean.index)\n",
        "systemsData_RelativeExpectedDailyEnergy_test_mean.sort_index(inplace=True)\n",
        "\n",
        "systemsData_RelativeExpectedDailyEnergy_test_std = pd.concat(systemsData_RelativeExpectedDailyEnergy_test_std_List, axis=1)\n",
        "systemsData_RelativeExpectedDailyEnergy_test_std.index = pd.to_datetime(systemsData_RelativeExpectedDailyEnergy_test_std.index)\n",
        "systemsData_RelativeExpectedDailyEnergy_test_std.sort_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute absolute expected daily energy\n",
        "systemsData_ExpectedDailyEnergy_test_mean = systemsData_RelativeExpectedDailyEnergy_test_mean * systemsData_EstimatedMaxDailyEnergy\n",
        "systemsData_ExpectedDailyEnergy_test_std = systemsData_RelativeExpectedDailyEnergy_test_std * systemsData_EstimatedMaxDailyEnergy"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Compute absolute expected daily energy\n",
        "systemsData_ExpectedDailyEnergy_test_mean = systemsData_RelativeExpectedDailyEnergy_test_mean # * systemsData_EstimatedMaxDailyEnergy\n",
        "systemsData_RelativeExpectedDailyEnergy_test_mean = systemsData_ExpectedDailyEnergy_test_mean / systemsData_EstimatedMaxDailyEnergy\n",
        "systemsData_ExpectedDailyEnergy_test_std = systemsData_RelativeExpectedDailyEnergy_test_std # * systemsData_EstimatedMaxDailyEnergy\n",
        "systemsData_RelativeExpectedDailyEnergy_test_std = systemsData_ExpectedDailyEnergy_test_std / systemsData_EstimatedMaxDailyEnergy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statics on the models metrics\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Do statistics about the series regressorsMetrics_train\n",
        "metrics = pd.concat([regressorsMetrics_mape_scaled_train_not_norm, regressorsMetrics_mape_scaled_train, regressorsMetrics_mape_scaled_test_not_norm, regressorsMetrics_mape_scaled_test], axis=1)\n",
        "print((metrics*100).describe())\n",
        "\n",
        "# box plot of the metrics\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=metrics * 100)\n",
        "plt.ylabel('Mean Absolute Percentage Error (%)')\n",
        "# plt.title('Box plot of the Mean Absolute Error (%) of the regressors on the validation set')\n",
        "plt.ylim(0, 40)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "systemsData_RelativeDelta_test = systemsData_RelativeExpectedDailyEnergy_test_mean - systemsData_RelativeMeasuredDailyEnergy_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Detector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {},
      "outputs": [],
      "source": [
        "# eronate data are the data where the relative delta is lower than regressorsMetrics_test\n",
        "max_z_score = 1.65  # 90% confidence interval\n",
        "systemsData_RelativeDelta_test_detected_anomalies_mask = systemsData_RelativeDelta_test.loc[:, regressorsMetrics_mape_scaled_test.index] > max_z_score * regressorsMetrics_mape_scaled_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detector metrics - confusion matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Positives (TP): 1587\n",
            "True Negatives (TN): 30602\n",
            "False Positives (FP): 43\n",
            "False Negatives (FN): 368\n"
          ]
        }
      ],
      "source": [
        "# Calculate TP, TN, FP, FN\n",
        "aligned_simulated_anomalies_mask, aligned_detected_anomalies_mask = anomalies_mask.align(systemsData_RelativeDelta_test_detected_anomalies_mask, join='inner', axis=None)\n",
        "\n",
        "TP = ((aligned_detected_anomalies_mask == True) & (aligned_simulated_anomalies_mask == True)).sum().sum()   \n",
        "TN = ((aligned_detected_anomalies_mask == False) & (aligned_simulated_anomalies_mask == False)).sum().sum()   \n",
        "FP = ((aligned_detected_anomalies_mask == False) & (aligned_simulated_anomalies_mask == True)).sum().sum()  \n",
        "FN = ((aligned_detected_anomalies_mask == True) & (aligned_simulated_anomalies_mask == False)).sum().sum()\n",
        "\n",
        "# Display the results\n",
        "print(f\"True Positives (TP): {TP}\")\n",
        "print(f\"True Negatives (TN): {TN}\")\n",
        "print(f\"False Positives (FP): {FP}\")\n",
        "print(f\"False Negatives (FN): {FN}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Positives (TP): 122\n",
            "True Negatives (TN): 133\n",
            "False Positives (FP): 132\n",
            "False Negatives (FN): 113\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Example boolean masks as DataFrames (replace these with your actual data)\n",
        "simulated_anomalies_mask = pd.DataFrame(np.random.choice([True, False], size=(100, 10)))\n",
        "detected_anomalies_mask = pd.DataFrame(np.random.choice([True, False], size=(50, 20)))\n",
        "\n",
        "# Align the two DataFrames to have the same rows and columns\n",
        "aligned_simulated, aligned_detected = simulated_anomalies_mask.align(detected_anomalies_mask, join='inner', axis=None)\n",
        "\n",
        "# Calculate TP, TN, FP, FN on the aligned DataFrames\n",
        "TP = ((aligned_simulated == True) & (aligned_detected == True)).sum().sum()\n",
        "TN = ((aligned_simulated == False) & (aligned_detected == False)).sum().sum()\n",
        "FP = ((aligned_simulated == False) & (aligned_detected == True)).sum().sum()\n",
        "FN = ((aligned_simulated == True) & (aligned_detected == False)).sum().sum()\n",
        "\n",
        "# Display the results\n",
        "print(f\"True Positives (TP): {TP}\")\n",
        "print(f\"True Negatives (TN): {TN}\")\n",
        "print(f\"False Positives (FP): {FP}\")\n",
        "print(f\"False Negatives (FN): {FN}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scaling technics & Outliers removal\n",
        "\n",
        "Compute the:\n",
        "\n",
        "- Global mean\n",
        "- Global median\n",
        "- Global standard deviation\n",
        "- Rolling mean\n",
        "- Rolling median\n",
        "- Rolling standard deviation\n",
        "- Simulate max production without info\n",
        "- SImulated max production with info\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "targetName = \"a001395\"\n",
        "\n",
        "X = systemsData_MeasuredDailyEnergy.drop(columns=targetName)\n",
        "y = systemsData_MeasuredDailyEnergy[targetName]\n",
        "y.index = pd.to_datetime(y.index)\n",
        "# Global mean of the daily energy production of the target system\n",
        "globalMean = y.mean()\n",
        "\n",
        "# Global std of the daily energy production of the target system\n",
        "globalStd = y.std()\n",
        "\n",
        "# Gloabl median of the daily energy production of the target system\n",
        "globalMedian = y.median()\n",
        "\n",
        "roll = y.rolling(window='30D', min_periods=1, center=True)\n",
        "# Rolling mean of the daily energy production of the target system. The window is 1 month\n",
        "rollingMean = roll.mean()\n",
        "\n",
        "# Rolling std of the daily energy production of the target system. The window is 7 days.\n",
        "rollingStd = roll.std()\n",
        "\n",
        "# Rolling Mean Absolute Deviation of the daily energy production of the target system. the function is mad with the arguments how='median' and center='median'\n",
        "rollingMAD = roll.apply(mad)\n",
        "# Rolling median of the daily energy production of the target system. The window is 7 days.\n",
        "rollingMedian = roll.median()\n",
        "\n",
        "# Rolling z score of the daily energy production of the target system. The function is modified_z_score\n",
        "# modifiedZScore = 0.673 * (y - rollingMedian) / rollingMAD\n",
        "\n",
        "# Plot the global and rolling mean, std, and median of the daily energy production of the target system\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=y.index, y=y, mode='markers', name='Daily energy production'))\n",
        "# fig.add_trace(go.Scatter(x=y.index, y=[globalMean]*len(y), mode='lines', name='Global mean'))\n",
        "# fig.add_trace(go.Scatter(x=y.index, y=[globalMean+globalStd]*len(y), mode='lines', name='Global mean + std'))\n",
        "# fig.add_trace(go.Scatter(x=y.index, y=[globalMean-globalStd]*len(y), mode='lines', name='Global mean - std'))\n",
        "# fig.add_trace(go.Scatter(x=y.index, y=[globalMedian]*len(y), mode='lines', name='Global median'))\n",
        "fig.add_trace(go.Scatter(x=rollingMean.index, y=rollingMean, mode='lines', name='Rolling mean'))\n",
        "# fig.add_trace(go.Scatter(x=rollingMean.index, y=rollingMean+rollingStd, mode='lines', name='Rolling mean + std'))\n",
        "# fig.add_trace(go.Scatter(x=rollingMean.index, y=rollingMean-rollingStd, mode='lines', name='Rolling mean - std'))\n",
        "fig.add_trace(go.Scatter(x=rollingMedian.index, y=rollingMedian, mode='lines', name='Rolling median'))\n",
        "fig.add_trace(go.Scatter(x=rollingMAD.index, y=rollingMedian + 4 * rollingMAD, mode='lines', name='Rolling Median + 4*Rolling MAD'))\n",
        "fig.add_trace(go.Scatter(x=rollingMAD.index, y=rollingMAD, mode='lines', name='Rolling MAD'))\n",
        "\n",
        "# fig.add_trace(go.Scatter(x=modifiedZScore.index, y=rollingMedian+modifiedZScore, mode='lines', name='Rolling Median + Rolling Z score'))\n",
        "\n",
        "fig.update_layout(title=f'Global and rolling mean, std, and median of the daily energy production of system {targetName}', yaxis_title='Daily energy production (kWh)')\n",
        "# fig.update_layout(width=1000, height=666)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "def zscore(s, window, thresh=3, return_all=False):\n",
        "    roll = s.rolling(window=window, min_periods=1, center=True)\n",
        "    avg = roll.mean()\n",
        "    std = roll.std(ddof=0)\n",
        "    z = s.sub(avg).div(std)\n",
        "    m = z.between(-thresh, thresh)\n",
        "\n",
        "    if return_all:\n",
        "        return z, avg, std, m\n",
        "    return s.where(m, avg)\n",
        "\n",
        "\n",
        "z, avg, std, m = zscore(y, window=50, return_all=True)\n",
        "\n",
        "ax = plt.subplot()\n",
        "\n",
        "y.plot(label='data')\n",
        "avg.plot(label='mean')\n",
        "y.loc[~m].plot(label='outliers', marker='o', ls='')\n",
        "# avg[~m].plot(label='replacement', marker='o', ls='')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"650\"\n",
              "            src=\"http://127.0.0.1:8050/\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x2511c781dc0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize the Dash app\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "tab_height = '2em'\n",
        "app.layout = html.Div([\n",
        "    html.Div([\n",
        "        dcc.Dropdown(\n",
        "            id='system-dropdown',\n",
        "            options=[{'label': name, 'value': name} for name in systemsName_Valid],\n",
        "            value=systemsName_Valid[0],\n",
        "            style={'width': '50%'}  # Adjust width and font size\n",
        "        ),\n",
        "        html.Div(id='metric-text-container', style={'display': 'inline-block', 'margin-left': '20px'})  # Container for the metric text\n",
        "    ], style={'display': 'flex', 'align-items': 'center'}),  # Align items horizontally\n",
        "    dcc.Tabs(id='plot-tabs', value='tab-energy', children=[\n",
        "        dcc.Tab(label='Absolute Energy', value='tab-energy', style={'padding': '0px', 'lineHeight': tab_height}, selected_style={'padding': '0px', 'lineHeight': tab_height, 'fontWeight': 'bold'}),  # Adjust height and line height\n",
        "        dcc.Tab(label='Normalized Energy', value='tab-rel-energy', style={'padding': '0px', 'lineHeight': tab_height}, selected_style={'padding': '0px', 'lineHeight': tab_height, 'fontWeight': 'bold'}),  # Adjust height and line height\n",
        "        dcc.Tab(label='Normalizer Tuning', value='tab-norm-tuning', style={'padding': '0px', 'lineHeight': tab_height}, selected_style={'padding': '0px', 'lineHeight': tab_height, 'fontWeight': 'bold'}),  # Adjust height and line height\n",
        "        # plot systemsData_RelativeDelta_test\n",
        "        dcc.Tab(label='All Normalized Energy', value='tab-rel-energy-all', style={'padding': '0px', 'lineHeight': tab_height}, selected_style={'padding': '0px', 'lineHeight': tab_height, 'fontWeight': 'bold'}),  # Adjust height and line height\n",
        "        dcc.Tab(label='Dynamic Losses', value='tab-delta-rel-energy', style={'padding': '0px', 'lineHeight': tab_height}, selected_style={'padding': '0px', 'lineHeight': tab_height, 'fontWeight': 'bold'}),  # Adjust height and line height\n",
        "        dcc.Tab(label='All Missing Value', value='tab-miss-val-all', style={'padding': '0px', 'lineHeight': tab_height}, selected_style={'padding': '0px', 'lineHeight': tab_height, 'fontWeight': 'bold'}),  # Adjust height and line height\n",
        "        dcc.Tab(label='Similar neighboring systems', value='tab-neighbors', style={'padding': '0px', 'lineHeight': tab_height}, selected_style={'padding': '0px', 'lineHeight': tab_height, 'fontWeight': 'bold'}),  # Adjust height and line height\n",
        "\n",
        "    ]),  # Adjust height for tabs\n",
        "    html.Div(id='tabs-content', style={'flex': '1 1 auto'})  # Allow the tabs-content div to grow\n",
        "], style={'display': 'flex', 'flexDirection': 'column', 'height': '100vh'})  # Make the outer container fill the screen height\n",
        "\n",
        "\n",
        "@app.callback(\n",
        "    [Output('tabs-content', 'children'),\n",
        "     Output('metric-text-container', 'children')],\n",
        "    [Input('plot-tabs', 'value'),\n",
        "     Input('system-dropdown', 'value')]\n",
        ")\n",
        "def render_content(tab, selected_system):\n",
        "    # Statistic text\n",
        "    try:\n",
        "        mae_train = regressorsMetrics_mape_scaled_train.loc[selected_system]\n",
        "    except:\n",
        "        mae_train = np.nan\n",
        "    try:\n",
        "        mae_test = regressorsMetrics_mape_scaled_test.loc[selected_system]\n",
        "    except:\n",
        "        mae_test = np.nan\n",
        "    try:\n",
        "        loss = systemsMetadata[selected_system]['metadata']['loss']\n",
        "    except:\n",
        "        loss = np.nan\n",
        "\n",
        "    mae_train_text = f\"Half Sibling Regressor - Train set MAPE: {mae_train * 100:.2f}%\"\n",
        "    mae_test_text = f\"Half Sibling Regressor - Test set MAPE: {mae_test * 100:.2f}%\"\n",
        "    loss_text = f\"Normalizer Tuning - Static System Loss : {loss * 100:.2f}%\"\n",
        "\n",
        "    metric_text_div = html.Div([\n",
        "        html.Div(mae_train_text),\n",
        "        html.Div(mae_test_text),\n",
        "        html.Div(loss_text)\n",
        "    ], style={'fontSize': 16})\n",
        "\n",
        "    if tab == 'tab-energy':\n",
        "        fig1 = go.Figure(layout_yaxis_title=\"Daily Energy (kWh)\")\n",
        "\n",
        "        # remove nan from systemsData_EstimatedMaxDailyEnergy[selected_system]\n",
        "\n",
        "        try:\n",
        "            estimatedMaxDailyEnergy = systemsData_EstimatedMaxDailyEnergy[selected_system].dropna()\n",
        "            fig1.add_trace(go.Scatter(\n",
        "                x=estimatedMaxDailyEnergy.index,\n",
        "                y=estimatedMaxDailyEnergy,\n",
        "                mode='lines',\n",
        "                name='Estimated Max Daily Energy',\n",
        "                marker_color='LightSeaGreen'\n",
        "            ))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            measuredDailyEnergy = systemsData_MeasuredDailyEnergy[selected_system].dropna()\n",
        "            anomalous_days = anomalies_mask[anomalies_mask[selected_system] == True].index\n",
        "            fig1.add_trace(go.Bar(\n",
        "                x=measuredDailyEnergy.index,\n",
        "                y=measuredDailyEnergy,\n",
        "                # mode='markers',\n",
        "                name='Measured Daily Energy',\n",
        "                marker_color=np.where(measuredDailyEnergy.index.isin(anomalous_days), 'green', 'blue')\n",
        "            ))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # try:\n",
        "        # measuredDailyEnergy_train_outliers = systemsData_MeasuredDailyEnergy_train_outliers[selected_system].dropna()\n",
        "        #     fig1.add_trace(go.Scatter(\n",
        "        #         x=measuredDailyEnergy_train_outliers.index,\n",
        "        #         y=measuredDailyEnergy_train_outliers,\n",
        "        #         mode='markers',\n",
        "        #         name='Outliers',\n",
        "        #         marker_color='yellow'\n",
        "        #     ))\n",
        "        # except:\n",
        "        #     pass\n",
        "        # try:\n",
        "        #     expectedDailyEnergy_train = systemsData_ExpectedDailyEnergy_train[selected_system].dropna()\n",
        "        #     fig1.add_trace(go.Scatter(\n",
        "        #         x=expectedDailyEnergy_train.index,\n",
        "        #         y=expectedDailyEnergy_train,\n",
        "        #         mode='markers',\n",
        "        #         name='Expected Daily Energy',\n",
        "        #         marker_color='red'\n",
        "        #     ))\n",
        "        # except:\n",
        "        #     pass\n",
        "\n",
        "        try:\n",
        "            expectedDailyEnergy_test_mean = systemsData_ExpectedDailyEnergy_test_mean[selected_system].dropna()\n",
        "            expectedDailyEnergy_test_std = systemsData_ExpectedDailyEnergy_test_std[selected_system].dropna()\n",
        "            fig1.add_trace(go.Bar(\n",
        "                x=expectedDailyEnergy_test_mean.index,\n",
        "                y=expectedDailyEnergy_test_mean,\n",
        "                # mode='markers',\n",
        "                name='Expected Daily Energy',\n",
        "                marker_color='red'\n",
        "                # error_y=dict(\n",
        "                #     type='data',\n",
        "                #     array=expectedDailyEnergy_test_std,\n",
        "                #     visible=True\n",
        "                # )\n",
        "            ))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Update layout for legend position\n",
        "        fig1.update_layout(\n",
        "            legend=dict(\n",
        "                x=0.99,\n",
        "                y=0.99,\n",
        "                xanchor='right',\n",
        "                yanchor='top',\n",
        "                orientation='h'\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return dcc.Graph(figure=fig1, style={'height': '100%', 'width': '100%'}), metric_text_div  # Adjust height and width of the figure\n",
        "\n",
        "    elif tab == 'tab-norm-tuning':\n",
        "        fig1 = go.Figure(layout_yaxis_title=\"Daily Energy (kWh)\")\n",
        "        try:\n",
        "            measuredDailyEnergy = systemsData_MeasuredDailyEnergy[selected_system].dropna()\n",
        "            fig1.add_trace(go.Scatter(\n",
        "                x=measuredDailyEnergy.index,\n",
        "                y=measuredDailyEnergy,\n",
        "                mode='markers',\n",
        "                name='Measured Daily Energy',\n",
        "                marker_color='blue'\n",
        "            ))\n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            measuredMax = systemsData_Tuning_MeasureMax[selected_system].dropna()\n",
        "            fig1.add_trace(go.Scatter(\n",
        "                x=measuredMax.index,\n",
        "                y=measuredMax,\n",
        "                mode='markers',\n",
        "                name='Max Measured Daily Energy (7 days window)',\n",
        "                marker_color='red'\n",
        "            ))\n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            outliers = systemsData_Tuning_Outliers[selected_system].dropna()\n",
        "            fig1.add_trace(go.Scatter(\n",
        "                x=outliers.index,\n",
        "                y=outliers,\n",
        "                mode='markers',\n",
        "                name='Tuning Outliers',\n",
        "                marker_color='yellow'\n",
        "            ))\n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            estimatedMaxDailyEnergy_untuned = systemsData_Tuning_EstimatedMaxDailyEnergy_untuned[selected_system].dropna()\n",
        "            fig1.add_trace(go.Scatter(\n",
        "                x=estimatedMaxDailyEnergy_untuned.index,\n",
        "                y=estimatedMaxDailyEnergy_untuned,\n",
        "                mode='lines',\n",
        "                name='Estimated Max Daily Energy (Untuned)',\n",
        "                marker_color='violet'\n",
        "            ))\n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            estimatedMaxDailyEnergy = systemsData_EstimatedMaxDailyEnergy[selected_system].dropna()\n",
        "            fig1.add_trace(go.Scatter(\n",
        "                x=estimatedMaxDailyEnergy.index,\n",
        "                y=estimatedMaxDailyEnergy,\n",
        "                mode='lines',\n",
        "                name='Estimated Max Daily Energy (Tuned)',\n",
        "                marker_color='LightSeaGreen'\n",
        "            ))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        fig1.update_layout(\n",
        "            legend=dict(\n",
        "                x=0.99,\n",
        "                y=0.99,\n",
        "                xanchor='right',\n",
        "                yanchor='top',\n",
        "                orientation='h'\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return dcc.Graph(figure=fig1, style={'height': '100%', 'width': '100%'}), metric_text_div\n",
        "    elif tab == 'tab-rel-energy':\n",
        "        fig1 = go.Figure(layout_yaxis_title=\"Normalized Daily Energy (%)\")\n",
        "        # add a line at 100% for the Estimated Max Daily Energy\n",
        "        estimatedMaxDailyEnergy = systemsData_EstimatedMaxDailyEnergy[selected_system].dropna()\n",
        "        fig1.add_shape(\n",
        "            type=\"line\",\n",
        "            x0=estimatedMaxDailyEnergy.index.min(),\n",
        "            y0=100,\n",
        "            x1=estimatedMaxDailyEnergy.index.max(),\n",
        "            y1=100,\n",
        "            name='Estimated Max Daily Energy',\n",
        "            line_color='LightSeaGreen'\n",
        "            # line=dict(\n",
        "            #     color=\"LightSeaGreen\",\n",
        "            #     width=2,\n",
        "            #     dash=\"dashdot\",\n",
        "            # ),\n",
        "        )\n",
        "        try:\n",
        "            relativeMeasuredDailyEnergy = systemsData_RelativeMeasuredDailyEnergy[selected_system].dropna()\n",
        "            fig1.add_trace(go.Bar(\n",
        "                x=relativeMeasuredDailyEnergy.index,\n",
        "                y=relativeMeasuredDailyEnergy * 100,\n",
        "                # mode='markers',\n",
        "                name='Measured Daily Energy',\n",
        "                marker_color='blue'\n",
        "            ))\n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            relativeExpectedDailyEnergy_test_mean = systemsData_RelativeExpectedDailyEnergy_test_mean[selected_system].dropna()\n",
        "            fig1.add_trace(go.Bar(\n",
        "                x=relativeExpectedDailyEnergy_test_mean.index,\n",
        "                y=relativeExpectedDailyEnergy_test_mean * 100,\n",
        "                # mode='markers',\n",
        "                name='Expected Daily Energy',\n",
        "                marker_color='red'\n",
        "                # error_y=dict(\n",
        "                #     type='data',\n",
        "                #     array=systemsData_RelativeExpectedDailyEnergy_test_std[selected_system] * 100,\n",
        "                #     visible=True\n",
        "                # )\n",
        "            ))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        fig1.update_layout(\n",
        "            legend=dict(\n",
        "                x=0.99,\n",
        "                y=0.99,\n",
        "                xanchor='right',\n",
        "                yanchor='top',\n",
        "                orientation='h'\n",
        "            )\n",
        "        )\n",
        "        return dcc.Graph(figure=fig1, style={'height': '100%', 'width': '100%'}), metric_text_div  # Adjust height and width of the figure\n",
        "    elif tab == 'tab-delta-rel-energy':\n",
        "        fig1 = go.Figure(layout_yaxis_title=\"Normalized Daily Energy Loss (%)\")\n",
        "        try:\n",
        "            relativeDelta_test = systemsData_RelativeDelta_test[selected_system].dropna()\n",
        "            fig1.add_trace(go.Scatter(\n",
        "                x=relativeDelta_test.index,\n",
        "                y=relativeDelta_test * 100,\n",
        "                mode='markers',\n",
        "                name='Normalized Delta Energy',\n",
        "            ))\n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            relativeDelta_test_detected = systemsData_RelativeDelta_test[systemsData_RelativeDelta_test_detected_anomalies_mask][selected_system].dropna()\n",
        "            fig1.add_trace(go.Scatter(\n",
        "                x=relativeDelta_test_detected.index,\n",
        "                y=relativeDelta_test_detected * 100,\n",
        "                mode='markers',\n",
        "                name='Detected Anomaly',\n",
        "                marker_color='red'\n",
        "            ))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        fig1.update_layout(\n",
        "            legend=dict(\n",
        "                x=0.99,\n",
        "                y=0.99,\n",
        "                xanchor='right',\n",
        "                yanchor='top',\n",
        "                orientation='h'\n",
        "            )\n",
        "        )\n",
        "        return dcc.Graph(figure=fig1, style={'height': '100%', 'width': '100%'}), metric_text_div  # Adjust height and width of the figure\n",
        "    elif tab == 'tab-rel-energy-all':\n",
        "        fig1 = go.Figure(layout_yaxis_title=\"Normalized Daily Energy (%)\")\n",
        "        # try:\n",
        "        features_importance_norm = features_importance_df.loc[selected_system] / features_importance_df.loc[selected_system].max()\n",
        "\n",
        "        for systemName in systemsName_Valid:\n",
        "            if systemName != selected_system:\n",
        "                if features_importance_norm[systemName] > 0.1:\n",
        "                    fig1.add_trace(go.Scatter(\n",
        "                        x=systemsData_RelativeMeasuredDailyEnergy[systemName].index,\n",
        "                        y=systemsData_RelativeMeasuredDailyEnergy[systemName] * 100,\n",
        "                        mode='markers',\n",
        "                        name=f'{systemName}',\n",
        "                        marker_color='blue',\n",
        "                        marker_opacity=features_importance_norm[systemName]\n",
        "                    ))\n",
        "        fig1.add_trace(go.Scatter(\n",
        "            x=systemsData_RelativeMeasuredDailyEnergy[selected_system].index,\n",
        "            y=systemsData_RelativeMeasuredDailyEnergy[selected_system] * 100,\n",
        "            mode='markers',\n",
        "            name=f'{selected_system}',\n",
        "            marker_color='red'\n",
        "        ))\n",
        "        fig1.update_layout(yaxis=dict(range=[-5, 120]))\n",
        "\n",
        "        # except:\n",
        "        #     pass\n",
        "\n",
        "        return dcc.Graph(figure=fig1, style={'height': '100%', 'width': '100%'}), metric_text_div  # Adjust height and width of the figure\n",
        "    elif tab == 'tab-miss-val-all':\n",
        "        measures = systemsData_MeasuredDailyEnergy\n",
        "\n",
        "        # Sort columns by the number of missing values\n",
        "        sorted_columns = measures.isnull().sum().sort_values().index\n",
        "        sorted_measures = measures[sorted_columns]\n",
        "\n",
        "        # Create a boolean DataFrame where True indicates missing values\n",
        "        missing_values = (~sorted_measures.isnull()).astype(int)\n",
        "\n",
        "        # Plot heatmap\n",
        "        fig = go.Figure(data=go.Heatmap(\n",
        "            z=missing_values,\n",
        "            x=missing_values.columns,\n",
        "            y=missing_values.index,\n",
        "            showscale=False,\n",
        "            colorscale='Greys'  # Set colorscale to black and white\n",
        "        ))\n",
        "        fig.update_layout(\n",
        "            yaxis=dict(\n",
        "                showticklabels=True,  # Show y-axis tick labels\n",
        "                autorange='reversed'  # Invert the y-axis\n",
        "            ),\n",
        "            yaxis_tickmode='array',\n",
        "            yaxis_tickvals=pd.date_range(start=missing_values.index.min(), end=missing_values.index.max(), freq='ME'),\n",
        "            yaxis_ticktext=pd.date_range(start=missing_values.index.min(), end=missing_values.index.max(), freq='ME').strftime('%b %Y')\n",
        "        )\n",
        "\n",
        "        return dcc.Graph(figure=fig, style={'height': '100%', 'width': '100%'}), metric_text_div  # Adjust height and width of the figure\n",
        "\n",
        "    elif tab == 'tab-neighbors':\n",
        "        fig2 = go.Figure()\n",
        "\n",
        "        # Add initial traces with secondary y-axis\n",
        "        try:\n",
        "            fig2.add_trace(go.Bar(\n",
        "                x=features_importance_df.columns,\n",
        "                y=features_importance_df.loc[selected_system],\n",
        "                name='Impurity-based Importance',\n",
        "                yaxis='y1',\n",
        "                offsetgroup=1\n",
        "            ))\n",
        "            fig2.update_layout(\n",
        "                yaxis1=dict(\n",
        "                    title='Impurity-based Importance',\n",
        "                    range=[0, features_importance_df.loc[selected_system].max()],\n",
        "                )\n",
        "            )\n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            fig2.add_trace(go.Bar(\n",
        "                x=permutation_importance_mean_df.columns,\n",
        "                y=permutation_importance_mean_df.loc[selected_system],\n",
        "                name='Permutation Importance',\n",
        "                yaxis='y2',\n",
        "                offsetgroup=2\n",
        "            ))\n",
        "            fig2.update_layout(\n",
        "                yaxis2=dict(\n",
        "                    title='Permutation Importance',\n",
        "                    overlaying='y',\n",
        "                    side='right',\n",
        "                    range=[0, permutation_importance_mean_df.loc[selected_system].max()],\n",
        "                )\n",
        "            )\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return dcc.Graph(figure=fig2, style={'height': '100%', 'width': '100%'}), metric_text_div  # Adjust height and width of the figure\n",
        "\n",
        "\n",
        "def open_browser():\n",
        "    webbrowser.open(\"http://127.0.0.1:8050/\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Open the Dash app in a new browser window\n",
        "    Timer(1, open_browser).start()\n",
        "    app.run_server(debug=True, use_reloader=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpreting result from a prediction\n",
        "\n",
        "https://towardsdatascience.com/interpreting-random-forests-638bca8b49ea\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparisons between regressors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show stats for the series regressorsMetrics_mape_scaled_test\n",
        "test_mape = regressorsMetrics_mape_scaled_test[regressorsMetrics_mape_scaled_test < 1] * 100\n",
        "print(test_mape.describe())\n",
        "# show a box plot\n",
        "# test_mape.plot.box()\n",
        "\n",
        "# show histogram\n",
        "test_mape.plot.hist(bins=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Impact of number of neighbors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest Regressor hyperparameters\n",
        "n_estimators = 100  # Number of trees in random forest\n",
        "max_features = 'log2'  # Number of features to consider at every split\n",
        "max_depth = None  # Maximum number of levels in tree\n",
        "min_samples_split = 2  # Minimum number of samples required to split a node\n",
        "min_samples_leaf = 1  # Minimum number of samples required at each leaf node\n",
        "\n",
        "\n",
        "df_mape_n_neighbors_test = pd.DataFrame(index=systemsName_Valid, columns=[1, 2, 5, 10, 50, 150, 300])\n",
        "for max_neighbors in df_mape_n_neighbors_test.columns:\n",
        "    for targetName in tqdm(systemsName_Valid):\n",
        "        # train\n",
        "        rf_regressor = RandomForestRegressor(oob_score=True, random_state=random_state, n_estimators=n_estimators, max_features=max_features, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
        "        X_train, y_train = get_system_data(targetName, set='train', relative=True, max_neighbors=max_neighbors)\n",
        "        # split the data into training and testing sets\n",
        "        rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "        # predict\n",
        "        X_test, y_test = get_system_data(targetName, set='test', relative=True)\n",
        "        if y_test.count() == 0:\n",
        "            continue\n",
        "\n",
        "        # adjust the feature in the validation set to match the feature in the training set\n",
        "        fitted_features = rf_regressor.feature_names_in_\n",
        "        # Identify extra columns in X_test that are not used by the regressor\n",
        "        extra_features = set(X_test.columns) - set(fitted_features)\n",
        "        # Drop extra columns from X_val\n",
        "        X_test = X_test.drop(columns=list(extra_features), errors='ignore')\n",
        "        # Identify missing columns in X_test and add them as empty columns\n",
        "        missing_features = set(fitted_features) - set(X_test.columns)\n",
        "        for feature in missing_features:\n",
        "            X_test[feature] = np.nan\n",
        "\n",
        "        y_mean_array = rf_regressor.predict(X_test)\n",
        "        y_pred = pd.Series(y_mean_array, index=X_test.index, name=targetName)\n",
        "        df_mape_n_neighbors_test.loc[targetName, max_neighbors] = mean_absolute_error(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# do a figure with 3 box plot, one for each column of the dataframe df_mape_train_save.\n",
        "df_mape_n_neighbors_test_filtered = df_mape_n_neighbors_test[df_mape_n_neighbors_test < 0.5] * 100\n",
        "fig = go.Figure()\n",
        "for column in df_mape_n_neighbors_test_filtered.columns:\n",
        "    fig.add_trace(go.Box(y=df_mape_n_neighbors_test_filtered[column], name=column, boxmean=True))\n",
        "\n",
        "# remove legend\n",
        "fig.update_layout(showlegend=False)\n",
        "# set the x axis name to \"Training Days\"\n",
        "fig.update_xaxes(title_text='Neighbors systems')\n",
        "# set y axis name to \"MAPE (%)\"\n",
        "fig.update_yaxes(title_text='MAPE (%)')\n",
        "# set the fig size to 1000 x 666\n",
        "fig.update_layout(width=1000, height=666)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Impact of the number of training data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest Regressor hyperparameters\n",
        "n_estimators = 100  # Number of trees in random forest\n",
        "max_features = 'log2'  # Number of features to consider at every split\n",
        "max_depth = None  # Maximum number of levels in tree\n",
        "min_samples_split = 2  # Minimum number of samples required to split a node\n",
        "min_samples_leaf = 1  # Minimum number of samples required at each leaf node\n",
        "\n",
        "\n",
        "df_mape_n_history_test = pd.DataFrame(index=systemsName_Valid, columns=[2])\n",
        "for max_training_days in df_mape_n_history_test.columns:\n",
        "    for targetName in tqdm(systemsName_Valid):\n",
        "        # train\n",
        "        rf_regressor = RandomForestRegressor(oob_score=False, random_state=random_state, n_estimators=n_estimators, max_features=max_features, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
        "        X_train, y_train = get_system_data(targetName, set='train', relative=True)\n",
        "\n",
        "        # keep only the last max_training_days days\n",
        "        X_train = X_train.iloc[-max_training_days:]\n",
        "        y_train = y_train.iloc[-max_training_days:]\n",
        "\n",
        "        # split the data into training and testing sets\n",
        "        rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "        # predict\n",
        "        X_test, y_test = get_system_data(targetName, set='test', relative=True)\n",
        "        if y_test.count() == 0:\n",
        "            continue\n",
        "\n",
        "        # adjust the feature in the validation set to match the feature in the training set\n",
        "        fitted_features = rf_regressor.feature_names_in_\n",
        "        # Identify extra columns in X_test that are not used by the regressor\n",
        "        extra_features = set(X_test.columns) - set(fitted_features)\n",
        "        # Drop extra columns from X_val\n",
        "        X_test = X_test.drop(columns=list(extra_features), errors='ignore')\n",
        "        # Identify missing columns in X_test and add them as empty columns\n",
        "        missing_features = set(fitted_features) - set(X_test.columns)\n",
        "        for feature in missing_features:\n",
        "            X_test[feature] = np.nan\n",
        "\n",
        "        y_mean_array = rf_regressor.predict(X_test)\n",
        "        y_pred = pd.Series(y_mean_array, index=X_test.index, name=targetName)\n",
        "        df_mape_n_history_test.loc[targetName, max_training_days] = mean_absolute_error(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# do a figure with 3 box plot, one for each column of the dataframe df_mape_train_save.\n",
        "df_mape_n_history_test_filtered = df_mape_n_history_test_save[df_mape_n_history_test_save < 0.5] * 100\n",
        "fig = go.Figure()\n",
        "for column in df_mape_n_history_test_filtered.columns:\n",
        "    fig.add_trace(go.Box(y=df_mape_n_history_test_filtered[column], name=column, boxmean=True))\n",
        "\n",
        "# remove legend\n",
        "fig.update_layout(showlegend=False)\n",
        "# set the x axis name to \"Training Days\"\n",
        "fig.update_xaxes(title_text='Training Days')\n",
        "# set y axis name to \"MAPE (%)\"\n",
        "fig.update_yaxes(title_text='MAPE (%)')\n",
        "fig.update_layout(width=1000, height=666)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_mape_n_history_test_filtered.astype(float).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "# Step 1: Convert 'training_time' from 'HH:MM' format to minutes\n",
        "data = {'days': [2, 7, 14, 30, 180, 365],\n",
        "        'training_time': ['02:36', '02:27', '02:29', '02:47', '03:30', '04:10']}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Convert training time to minutes\n",
        "df['training_time_seconds'] = df['training_time'].apply(lambda x: int(x.split(':')[0]) * 60 + int(x.split(':')[1])) / 326\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simulate anomalies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
