{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pvlib\n",
    "import json\n",
    "import os\n",
    "from pvlib.pvsystem import PVSystem, Array, FixedMount\n",
    "from pvlib.location import Location\n",
    "from pvlib.modelchain import ModelChain\n",
    "from pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, root_mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import forestci as fci\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import threading\n",
    "from sklearn.metrics import make_scorer\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "import plotly.graph_objects as go\n",
    "from dash.dependencies import Input, Output\n",
    "import webbrowser\n",
    "from threading import Timer\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.utils.parallel import Parallel, delayed\n",
    "from sklearn.utils.validation import (\n",
    "    check_is_fitted,\n",
    ")\n",
    "from sklearn.ensemble._base import _partition_estimators\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.renderers.default = \"browser\"  # render plotly figures in browser\n",
    "\n",
    "PARENT_DATA_DIR = os.getenv('PARENT_DATA_DIR')\n",
    "if PARENT_DATA_DIR is None:\n",
    "    raise ValueError(\"PARENT_DATA_DIR environment variable is not set\")\n",
    "\n",
    "\n",
    "data_dirpath = PARENT_DATA_DIR + r\"\\PRiOT\\dataExport_2\"  # \"/Applications/Documents/TM Maxime/dataExport_3400_daily\"#\n",
    "cache_dirpath = os.path.join(data_dirpath, \"cache\")\n",
    "logs_dirpath = \"../logs\"\n",
    "\n",
    "if not os.path.exists(logs_dirpath):\n",
    "    os.makedirs(logs_dirpath)\n",
    "\n",
    "if not os.path.exists(cache_dirpath):\n",
    "    os.makedirs(cache_dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = False\n",
    "force_train_hsr = True\n",
    "force_tune_MaxProductionNormalizer = True\n",
    "random_state = 42\n",
    "\n",
    "max_training_days = None # None = Maximum possible\n",
    "min_training_days = 14\n",
    "testing_days = 14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/model_persistence.html\n",
    "class ModelSerializer:\n",
    "    def _save_model(self, model, serial_type, save_params):\n",
    "        serial_type.dump(model, save_params)\n",
    "\n",
    "    def _retrieve_model(self, serial_type, retrieve_params):\n",
    "        return serial_type.load(retrieve_params)\n",
    "\n",
    "\n",
    "class JoblibSerializer(ModelSerializer):\n",
    "    def save_model(self, model, save_model_path, filename):\n",
    "        super()._save_model(model, joblib, os.path.join(save_model_path, filename + \".joblib\"))\n",
    "\n",
    "    def retrieve_model(self, save_model_path, filename):\n",
    "        return super()._retrieve_model(joblib, os.path.join(save_model_path, filename + '.joblib'))\n",
    "\n",
    "\n",
    "class PickleSerializer(ModelSerializer):\n",
    "    def save_model(self, model, save_model_path, filename):\n",
    "        with open(os.path.join(save_model_path, filename + \".pkl\"), 'wb') as f:\n",
    "            super()._save_model(model, pickle, f)\n",
    "\n",
    "    def retrieve_model(self, save_model_path, filename):\n",
    "        with open(os.path.join(save_model_path, filename + \".pkl\"), 'rb') as f:\n",
    "            return super()._retrieve_model(pickle, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_altitude_from_wgs84(longitude, latitude):\n",
    "    # Convert WGS84 to LV95\n",
    "    lv95_url = \"https://geodesy.geo.admin.ch/reframe/wgs84tolv95\"\n",
    "    params_lv95 = {\n",
    "        \"easting\": longitude,\n",
    "        \"northing\": latitude,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "\n",
    "    response_lv95 = requests.get(lv95_url, params=params_lv95)\n",
    "    if response_lv95.status_code != 200:\n",
    "        raise Exception(\"Error converting WGS84 to LV95: \" + response_lv95.text)\n",
    "\n",
    "    lv95_data = response_lv95.json()\n",
    "    lv95_easting = lv95_data[\"easting\"]\n",
    "    lv95_northing = lv95_data[\"northing\"]\n",
    "\n",
    "    # Get altitude from LV95 coordinates\n",
    "    altitude_url = \"https://api3.geo.admin.ch/rest/services/height\"\n",
    "    params_altitude = {\n",
    "        \"easting\": lv95_easting,\n",
    "        \"northing\": lv95_northing\n",
    "    }\n",
    "\n",
    "    response_altitude = requests.get(altitude_url, params=params_altitude)\n",
    "    if response_altitude.status_code != 200:\n",
    "        raise Exception(\"Error retrieving altitude: \" + response_altitude.text)\n",
    "\n",
    "    altitude_data = response_altitude.json()\n",
    "    altitude = altitude_data[\"height\"]\n",
    "\n",
    "    return float(altitude)\n",
    "\n",
    "\n",
    "# Convert the power production with a given frequency to the total daily energy\n",
    "def daily_energy(df_power):\n",
    "    # Get the frequency in minutes\n",
    "    freq_in_minutes = pd.Timedelta(df_power.index.freq).seconds / 60\n",
    "    # Convert power from kW to kWh\n",
    "    df_energy = df_power * (freq_in_minutes / 60)\n",
    "    # Resample to daily frequency and sum the values\n",
    "    daily_energy = df_energy.resample('D').sum()\n",
    "\n",
    "    return daily_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "\n",
    "    def __init__(self, data_dirpath, cache_dirpath):\n",
    "        self.data_dirpath = data_dirpath\n",
    "        self.cache_dirpath = cache_dirpath\n",
    "        self.metadata_filepath = os.path.join(self.data_dirpath, \"metadata.json\")\n",
    "\n",
    "        self._measures = None\n",
    "        self._train_index = None\n",
    "        self._test_index = None\n",
    "        self._metadata = None\n",
    "        \n",
    "        self.valid_systems = None\n",
    "        \n",
    "        self.estimated_max_production = None\n",
    "\n",
    "        self.tuner_estimated_max_productions_untuned = None\n",
    "        self.tuner_measures_max_mask = None\n",
    "        self.tuner_measures_outliers_mask = None\n",
    "\n",
    "        self.hsr_outliers_mask = None\n",
    "\n",
    "    def get_metadata(self, system_name=None):\n",
    "        if self._metadata is None:\n",
    "            raise ValueError(\"Metadata not loaded. Please load the metadata first.\")\n",
    "        if system_name is not None:\n",
    "            return self._metadata[system_name]\n",
    "        return self._metadata\n",
    "    \n",
    "    def get_measures(self, set='all', systems_name=None):\n",
    "        if self._measures is None:\n",
    "            raise ValueError(\"Measures not loaded. Please load the measures first.\")\n",
    "        if self.valid_systems is None:\n",
    "            raise ValueError(\"Valid systems not set. Please check the data integrity first.\")\n",
    "        \n",
    "        systems_name = systems_name if systems_name is not None else self.valid_systems\n",
    "\n",
    "        if set == 'all':\n",
    "            return self._measures.loc[:, systems_name].dropna(axis='index', how='all').copy()\n",
    "        elif set == 'train':\n",
    "            if self._train_index is None:\n",
    "                raise ValueError(\"Train index not set. Please create a train-test set first.\")\n",
    "            if self.hsr_outliers_mask is None:\n",
    "                raise ValueError(\"Outliers not checked. Please check the outliers first.\")\n",
    "            # take the observation of the training set, for the desired systems, without the outliers values\n",
    "            return self._measures.loc[self._train_index, systems_name][~self.hsr_outliers_mask].dropna(axis='index', how='all').copy()\n",
    "        elif set == 'test':\n",
    "            if self._test_index is None:\n",
    "                raise ValueError(\"Test index not set. Please create a train-test set first.\")\n",
    "            return self._measures.loc[self._test_index, systems_name].dropna(axis='index', how='all').copy()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid set value. Please use 'all', 'train' or 'test'.\")\n",
    "            \n",
    "    def get_missing_value(self, sorted=True):\n",
    "        if sorted:\n",
    "            # Sort columns by number of missing values\n",
    "            sorted_columns = self._measures.isnull().sum().sort_values().index\n",
    "            sorted_measures = self._measures[sorted_columns]\n",
    "\n",
    "            # Create a boolean DataFrame where True indicates missing values\n",
    "            missing_values = sorted_measures.isnull()\n",
    "        else:\n",
    "            missing_values = self._measures.isnull()\n",
    "        return missing_values\n",
    "    \n",
    "    def normalize(self, data):\n",
    "        if self.estimated_max_production is None:\n",
    "            raise ValueError(\"Estimated max production not set. Please estimate the max production first.\")\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            # Normalize DataFrame\n",
    "            data_norm = data / self.estimated_max_production.loc[data.index, data.columns]\n",
    "        elif isinstance(data, pd.Series):\n",
    "            # Normalize Series\n",
    "            data_norm = data / self.estimated_max_production.loc[data.index, data.name]\n",
    "        else:\n",
    "            raise ValueError(\"Data must be a DataFrame or Series\")\n",
    "        \n",
    "        return data_norm\n",
    "\n",
    "\n",
    "    def load_metadata(self):\n",
    "        with open(self.metadata_filepath, 'r') as f:\n",
    "            self._metadata = json.load(f)\n",
    "\n",
    "        for _, system_metadata in tqdm(self._metadata.items(), desc=\"Post-processing metadata\"):\n",
    "            # Add altitude to metadata, if not already present (TODO : imporove with multi threading)\n",
    "            if \"loc_altitude\" not in system_metadata['metadata']:\n",
    "                if \"loc_longitude\" in system_metadata['metadata'] and \"loc_latitude\" in system_metadata['metadata']:\n",
    "                    system_metadata['metadata'][\"loc_altitude\"] = get_altitude_from_wgs84(system_metadata['metadata'][\"loc_longitude\"], system_metadata['metadata'][\"loc_latitude\"])\n",
    "\n",
    "            # Add the default loss to metadata if not already present\n",
    "            if 'loss' not in system_metadata['metadata']:\n",
    "                system_metadata['metadata']['loss'] = 0\n",
    "\n",
    "            # Convert key with \"modX\" in the name (x is the array number) to a dictionary with the array number as key\n",
    "            keys_to_delete = []\n",
    "            for key, value in system_metadata['metadata'].items():\n",
    "                if 'mod' in key:\n",
    "                    # Extract the module number\n",
    "                    array_num = key.split('_')[1][-1]\n",
    "                    # Remove the module number from the key\n",
    "                    new_key = '_'.join(key.split('_')[:1] + key.split('_')[2:])\n",
    "                    # Add the key-value pair to the appropriate module dictionary\n",
    "                    if 'arrays' not in system_metadata:\n",
    "                        system_metadata['arrays'] = {}\n",
    "                    if array_num not in system_metadata['arrays']:\n",
    "                        system_metadata['arrays'][array_num] = {}\n",
    "                    system_metadata['arrays'][array_num][new_key] = value\n",
    "                    keys_to_delete.append(key)\n",
    "            for key in keys_to_delete:\n",
    "                del system_metadata['metadata'][key]\n",
    "\n",
    "        # Save metadata with new format and value\n",
    "        self.save_metadata()\n",
    "\n",
    "    def save_metadata(self):\n",
    "        with open(self.metadata_filepath, 'w') as f:\n",
    "            json.dump(self._metadata, f, indent=4)\n",
    "\n",
    "    def load_csv(self):\n",
    "        measures_dic = {}\n",
    "        duplicates_list = []\n",
    "        for filename in tqdm(os.listdir(self.data_dirpath), desc=\"Loading CSV files\"):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                system_name = filename.split('_')[0]\n",
    "                system_measures = pd.read_csv(os.path.join(self.data_dirpath, filename))\n",
    "                # convert the timestamp to datetime with correct timezone\n",
    "                system_measures['Datetime'] = pd.to_datetime(system_measures['Timestamp'], unit='ms', utc=True).dt.tz_convert('Europe/Zurich')\n",
    "                # Convert the datetime to only the date, as the production is the daily production. The +1h is to manage the saving time. Normally PRiOT exports the data at midnight (local time) for the day after (e.g. the energy for the July 1st is saved at July 1st 00:00 Europe/Zurich). However it seams that the saving time is not always correctly handled, and sometime the export is done at 23:00 the day before (e.g. the energy for the July 1st is saved at June 30th 23:00 Europe/Zurich). This is why we add 1h to the datetime to be sure to have the correct date.\n",
    "                system_measures['Date'] = pd.to_datetime((system_measures['Datetime'] + pd.Timedelta(hours=1)).dt.date)\n",
    "                # Set the date as index\n",
    "                system_measures.set_index('Date', inplace=True)\n",
    "                # Append in duplicates_list all the rows with duplicated index, for logging purpose\n",
    "                if len(system_measures.index.duplicated(keep=False)):\n",
    "                    duplicates_list.append(system_measures[system_measures.index.duplicated(keep=False)])\n",
    "                # keep only the measures tt_forward_active_energy_total_toDay as a Series\n",
    "                system_measures = system_measures['tt_forward_active_energy_total_toDay']\n",
    "                # Group by the index (Date) and sum the system_measures for each date to handle duplicates\n",
    "                system_measures = system_measures.groupby('Date').sum()\n",
    "\n",
    "                measures_dic[system_name] = system_measures\n",
    "        # convert the dictionary of series to a pandas dataframe\n",
    "        self._measures = pd.DataFrame(measures_dic)\n",
    "        # Log the duplicates\n",
    "        duplicates_df = pd.concat(duplicates_list)\n",
    "        log_filename = os.path.join(logs_dirpath, \"measureDuplicates.csv\")\n",
    "        print(f\"Number of duplicate dates found: {len(duplicates_df)} (see log file {log_filename} for more details)\")\n",
    "        duplicates_df.to_csv(log_filename, index=True)\n",
    "\n",
    "    def check_integrity(self):\n",
    "        # Check if the metadata is loaded\n",
    "        if self._metadata is None:\n",
    "            raise ValueError(\"Metadata not loaded. Please load the metadata first.\")\n",
    "\n",
    "        # Check if the measures are loaded\n",
    "        if self._measures is None:\n",
    "            raise ValueError(\"Measures not loaded. Please load the measures first.\")\n",
    "\n",
    "        self.valid_systems = self._measures.columns\n",
    "\n",
    "        for system_name in tqdm(self._measures.columns, desc=\"Checking data integrity\"):\n",
    "            valid_system = True\n",
    "\n",
    "            # Check if the system has measures\n",
    "            if system_name not in self._measures or self._measures[system_name].count() == 0:\n",
    "                valid_system = False\n",
    "                print(f\"System {system_name} : No measures found\")\n",
    "            # Check if the system has metadata\n",
    "            if system_name not in self._metadata:\n",
    "                valid_system = False\n",
    "                print(f\"System {system_name} : No metadata found\")\n",
    "            else:\n",
    "                # Check metadata for the entire system\n",
    "                system_metadata = self._metadata[system_name]\n",
    "                for key in ['loc_latitude', 'loc_longitude', 'loc_altitude', 'pv_kwp']:\n",
    "                    # test that the key is present\n",
    "                    if key not in system_metadata['metadata']:\n",
    "                        valid_system = False\n",
    "                        print(f\"System {system_name} : No '{key}' found\")\n",
    "                    # if present, convert the value to a number, if possible\n",
    "                    elif not isinstance(system_metadata['metadata'][key], (int, float)):\n",
    "                        try:\n",
    "                            system_metadata['metadata'][key] = int(system_metadata['metadata'][key])\n",
    "                        except ValueError:\n",
    "                            try:\n",
    "                                system_metadata['metadata'][key] = float(system_metadata['metadata'][key])\n",
    "                            except ValueError:\n",
    "                                valid_system = False\n",
    "                                print(f\"System {system_name} : The key-value '{key}:{system_metadata['metadata'][key]}' is not a number\")\n",
    "\n",
    "                # Check metadata for the arrays\n",
    "                if 'arrays' not in system_metadata or len(system_metadata['arrays']) == 0:\n",
    "                    print(f\"System {system_name} : No PV arrays found\")\n",
    "                    valid_system = False\n",
    "                else:\n",
    "                    for array_num, array_data in system_metadata['arrays'].items():\n",
    "                        for key in ['pv_tilt', 'pv_azimut', 'pv_wp', 'pv_number']:\n",
    "                            if key not in array_data:\n",
    "                                valid_system = False\n",
    "                                print(f\"System {system_name} : No '{key}' found for array {array_num}\")\n",
    "                            # test that the value is a number, and convert it if possible\n",
    "                            elif not isinstance(array_data[key], (int, float)):\n",
    "                                try:\n",
    "                                    array_data[key] = int(array_data[key])\n",
    "                                except ValueError:\n",
    "                                    try:\n",
    "                                        array_data[key] = float(array_data[key])\n",
    "                                    except ValueError:\n",
    "                                        valid_system = False\n",
    "                                        print(f\"System {system_name} : The key-value '{key}:{array_data[key]}' is not a number for array {array_num}\")\n",
    "            if not valid_system:\n",
    "                self.valid_systems = self.valid_systems.drop(system_name)\n",
    "\n",
    "        print(f\"Number of systems with all the necessary data: {len(self.valid_systems)}/{len(self._measures.columns)}\")\n",
    "    \n",
    "    def create_train_test_set(self, test_size=None, max_train_size=None, random_state=None, shuffle=True):\n",
    "        if self.hsr_outliers_mask is None:\n",
    "            raise ValueError(\"Outliers not checked. Please check the outliers first.\")\n",
    "        \n",
    "        self._train_index, self._test_index = train_test_split(self._measures.index, test_size=test_size, random_state=random_state, shuffle=shuffle)\n",
    "        # remove hsr_outliers_mask from the training set\n",
    "        if max_train_size is not None and max_train_size < len(self._train_index):\n",
    "            # Now we want to randomly select \"train_size\" observation from the training set, with the least number of missing values\n",
    "            # We will do this by looking at the number of missing values of the \"train_size\" th element in the sorted list of observation by number of missing values\n",
    "            # This way, we will know the maximum number of missing values that the selected observation will have\n",
    "            # Then, we will randomly select \"train_size\" observation from all the observation with this number of missing value or less\n",
    "\n",
    "            # Get the training set.            \n",
    "            training_set = self.get_measures(set='train')\n",
    "            \n",
    "            nbr_missing_values_per_day = training_set.isnull().sum(axis=1)\n",
    "            # Get the maximum number of missing values that the selected observation will have\n",
    "            max_missing_value = nbr_missing_values_per_day.sort_values().iloc[max_train_size-1]\n",
    "            # Get all the observation with this number of missing value or less\n",
    "            valid_observations = training_set[nbr_missing_values_per_day <= max_missing_value]\n",
    "            # Randomly select \"train_size\" observation\n",
    "            self._train_index = valid_observations.sample(n=max_train_size, random_state=random_state).index\n",
    "\n",
    "    def check_outliers(self, max_threshold=1.1, min_threshold=0.01):\n",
    "        norm_measures = self.normalize(self.get_measures('all'))\n",
    "        self.hsr_outliers_mask = (norm_measures > max_threshold) | (norm_measures < min_threshold)\n",
    "        # If 10% of the values are outliers, remove the system from the list of valid systems\n",
    "        outliers_count = self.hsr_outliers_mask.sum(axis=0)\n",
    "        invalid_systems = outliers_count[outliers_count > 0.1 * len(norm_measures)].index\n",
    "        self.valid_systems = self.valid_systems.drop(invalid_systems)\n",
    "        for system_name in invalid_systems:\n",
    "            print(f\"System {system_name} : More than 10% of the values are outliers. This system is removed from the list of systems to be trained.\")\n",
    "\n",
    "    def check_training_size(self, min_training_days=14):\n",
    "        # Calculate the number of valid (non-null) values\n",
    "        valid_values_count = self.get_measures(set='train').notnull().sum(axis=0)\n",
    "\n",
    "        # Get the boolean Series where valid values count is less than 14\n",
    "        invalid_systems = valid_values_count[valid_values_count < min_training_days].index\n",
    "\n",
    "        # Remove invalid systems from the valid_system list\n",
    "        self.valid_systems = self.valid_systems.drop(invalid_systems)\n",
    "\n",
    "        for system_name in invalid_systems:\n",
    "            print(f\"System {system_name} : The system has less than {min_training_days} days of training data. This system is removed from the list of systems to be trained.\")\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max production Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxProductionNormalizer:\n",
    "    def __init__(self):\n",
    "        self.estimated_max_production = None\n",
    "        self.model = None\n",
    "\n",
    "    def create_model(self, system_metadata):\n",
    "        latitude = system_metadata['metadata']['loc_latitude']\n",
    "        longitude = system_metadata['metadata']['loc_longitude']\n",
    "        altitude = system_metadata['metadata']['loc_altitude']\n",
    "        Wp_Tot = system_metadata['metadata']['pv_kwp'] * 1000\n",
    "        loss = system_metadata['metadata']['loss'] * 100\n",
    "\n",
    "        arrays = []\n",
    "        for array_num, arrayData in system_metadata['arrays'].items():\n",
    "            array = Array(\n",
    "                mount=FixedMount(surface_tilt=arrayData['pv_tilt'], surface_azimuth=arrayData['pv_azimut'], racking_model='open_rack'),\n",
    "                module_parameters={'pdc0': arrayData['pv_wp'], 'gamma_pdc': -0.004},\n",
    "                module_type='glass_polymer',\n",
    "                modules_per_string=arrayData['pv_number'],\n",
    "                strings=1,\n",
    "                temperature_model_parameters=TEMPERATURE_MODEL_PARAMETERS['sapm']['open_rack_glass_polymer'],\n",
    "            )\n",
    "            arrays.append(array)\n",
    "\n",
    "        location = Location(latitude=latitude, longitude=longitude, altitude=altitude, tz='Europe/Zurich')\n",
    "        system = PVSystem(arrays=arrays,\n",
    "                        inverter_parameters={'pdc0': Wp_Tot, 'eta_inv_nom': 0.96},\n",
    "                        losses_parameters={'nameplate_rating': loss, 'soiling': 0, 'shading': 0, 'snow': 0, 'mismatch': 0, 'wiring': 0, 'connections': 0, 'lid': 0, 'age': 0, 'availability': 0})\n",
    "        self.model = ModelChain(system, location, clearsky_model='ineichen', aoi_model='no_loss', spectral_model=\"no_loss\", losses_model='pvwatts')\n",
    "\n",
    "    def generate_estimation_data(self, dates, sampling_freq='1h'):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not set. Please create a model first.\")\n",
    "        \n",
    "        # The end date needs to be estimated completly(end date at 23:59). But \"endDate\" is considered as 00:00 by pd.date_range().\n",
    "        # So we add 1 day to the end date to include the entire end date in the date_range(), and then we exclude the last value with the inclusive='left' proprety, to remove \"endDate+1\" at 00:00) in the date_range().\n",
    "        start_date = dates.min()\n",
    "        end_date = dates.max() + pd.Timedelta(days=1)\n",
    "\n",
    "        all_datetimes = pd.date_range(start=start_date, end=end_date, freq=sampling_freq, tz=self.model.location.tz, inclusive='left')\n",
    "\n",
    "        # Get the clear sky irradiance for the given dates\n",
    "        weather_clearsky = self.model.location.get_clearsky(all_datetimes)  # In W/m2\n",
    "        # TODO adjust the clear sky model to take into account the horizon https://pvlib-python.readthedocs.io/en/stable/gallery/shading/plot_simple_irradiance_adjustment_for_horizon_shading.html\n",
    "        \n",
    "        # Run the model to get the estimated production\n",
    "        self.model.run_model(weather_clearsky)\n",
    "        production_sampling_rate = self.model.results.ac / 1000  # Convert W to kW\n",
    "        self.estimated_max_production = daily_energy(production_sampling_rate)\n",
    "        self.estimated_max_production.index = pd.to_datetime(self.estimated_max_production.index.date)\n",
    "\n",
    "    def tune(self, system_measures, window=7):\n",
    "        if self.estimated_max_production is None:\n",
    "            raise ValueError(\"Estimated max production not set. Please estimate the max production first.\")\n",
    "\n",
    "        # Remove the obvious outliers. It's important before calculating the std, which can be strongly impacted by the strong outliers.\n",
    "        outliers_mask = system_measures > 2 * self.estimated_max_production[system_measures.index]\n",
    "        \n",
    "        valid_system_measures = system_measures[~outliers_mask]\n",
    "        # if 10% of the data is removed as outliers, we consider that the system is not valid\n",
    "        if outliers_mask.sum() / system_measures.size > 0.1:\n",
    "            return None, None, None, None\n",
    "        # Keep only the max measured value\n",
    "        # Iterate over windows of a given size, and keep only the maximum value in each window\n",
    "        max_measured_mask = pd.Series(False, index=system_measures.index)\n",
    "        for i in range(0, len(system_measures), window):\n",
    "            window_data = valid_system_measures.iloc[i:i + window]\n",
    "            if not window_data.empty and not window_data.isna().all():\n",
    "                max_measured_mask[window_data.idxmax(skipna=True)] = True\n",
    "\n",
    "        # Calculate the relative difference between the maximum measured and maximum estimated value\n",
    "        realtive_difference = system_measures[max_measured_mask] / self.estimated_max_production\n",
    "\n",
    "        # Compute statistics\n",
    "        std = realtive_difference.std()\n",
    "        mean = realtive_difference.mean()\n",
    "\n",
    "        # Remove the outilers that have a z-score greater than 1\n",
    "        z_scores = np.abs(realtive_difference - mean) / std\n",
    "\n",
    "        # Add the measure with a z-score greater than 1 to the previous outliers (AND operation)\n",
    "        outliers_mask = outliers_mask | (z_scores > 1)\n",
    "\n",
    "        # Get the loss that overestimate the estimate maximum daily energy\n",
    "        loss = 1 - realtive_difference[~outliers_mask].max()\n",
    "\n",
    "        return loss, std, max_measured_mask, outliers_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxProductionNormalizers:\n",
    "    def run(self, data_handler, tune=True):\n",
    "        estimated_max_productions_dic = {}\n",
    "        tuner_estimated_max_productions_untuned_dic = {}\n",
    "        tuner_measures_max_mask_dic = {}\n",
    "        tuner_measures_outliers_mask_dic = {}\n",
    "        unfitted_systems = []\n",
    "\n",
    "        for system_name in tqdm(data_handler.valid_systems, desc=\"Max production normalizers\"):\n",
    "            system_metadata = data_handler.get_metadata(system_name)\n",
    "            # If we don't want to tune the estimators, we say that the estimator is already tuned\n",
    "            tuned = not tune\n",
    "\n",
    "            # reset the loss in the metadata if we want to tune the estimators\n",
    "            if tune:\n",
    "                system_metadata['metadata']['loss'] = 0   \n",
    "\n",
    "            max_production_normalizer = MaxProductionNormalizer()\n",
    "            while True:  # emulate do while loop\n",
    "                max_production_normalizer.create_model(system_metadata)\n",
    "                measures = data_handler.get_measures(systems_name=system_name).dropna()\n",
    "                dates = measures.index\n",
    "                max_production_normalizer.generate_estimation_data(dates, sampling_freq='1h')\n",
    "\n",
    "                # add the estimation to the dictionary\n",
    "                estimated_max_productions_dic[system_name] = max_production_normalizer.estimated_max_production\n",
    "\n",
    "                # Tune estimators\n",
    "                if tuned:\n",
    "                    break\n",
    "\n",
    "                loss, std, max_measured_mask, outliers_mask = max_production_normalizer.tune(measures, window=7)\n",
    "\n",
    "                if loss is None or std is None or max_measured_mask is None or outliers_mask is None:\n",
    "                    unfitted_systems.append(system_name)\n",
    "                    break\n",
    "\n",
    "                # If the std is greater than 1, we remove the system from the list of systems to be processed.\n",
    "                # This is to avoid to have a system that is not well fitted by the maximum energy estimator model, and that could impact the training of the RF model.\n",
    "                if std > 1 :\n",
    "                    unfitted_systems.append(system_name)\n",
    "                    break\n",
    "\n",
    "                # write the loss in systemsMetadata\n",
    "                system_metadata['metadata']['loss'] = loss\n",
    "\n",
    "                # save the untuned estimation to plot the difference before/aftre tuning\n",
    "                tuner_estimated_max_productions_untuned_dic[system_name] = max_production_normalizer.estimated_max_production\n",
    "                tuner_measures_max_mask_dic[system_name] = max_measured_mask\n",
    "                tuner_measures_outliers_mask_dic[system_name] = outliers_mask\n",
    "\n",
    "                tuned = True\n",
    "\n",
    "        # Concatenate all the dictionaries to create dataframe\n",
    "        data_handler.estimated_max_production = pd.concat(estimated_max_productions_dic, axis=1)\n",
    "        data_handler.tuner_estimated_max_productions_untuned = pd.concat(tuner_estimated_max_productions_untuned_dic, axis=1)\n",
    "        data_handler.tuner_measures_max_mask = pd.concat(tuner_measures_max_mask_dic, axis=1)\n",
    "        data_handler.tuner_measures_outliers_mask = pd.concat(tuner_measures_outliers_mask_dic, axis=1)\n",
    "\n",
    "        # remove unfitted_systems from the valid_systems\n",
    "        data_handler.valid_systems = data_handler.valid_systems.drop(unfitted_systems)\n",
    "        for system_name in unfitted_systems:\n",
    "            print(f\"System {system_name} : We can't find the model corresponding to the measured data. This system is removed from the list of systems to be processed.\")\n",
    "\n",
    "        # Save the metadata with the new loss value\n",
    "        data_handler.save_metadata()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming data_handler.get_measures() returns a DataFrame\n",
    "measures = data_handler.get_measures()\n",
    "\n",
    "# Sort columns by the number of missing values\n",
    "sorted_columns = measures.isnull().sum().sort_values().index\n",
    "sorted_measures = measures[sorted_columns]\n",
    "\n",
    "# Create a boolean DataFrame where True indicates missing values\n",
    "missing_values = (~sorted_measures.isnull()).astype(int)\n",
    "\n",
    "# Plot heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=missing_values,\n",
    "    x=missing_values.columns,\n",
    "    y=missing_values.index,\n",
    "    showscale=False,\n",
    "    colorscale='Greys'  # Set colorscale to black and white\n",
    "))\n",
    "fig.update_layout(\n",
    "    yaxis=dict(\n",
    "        showticklabels=True,  # Show y-axis tick labels\n",
    "        autorange='reversed'  # Invert the y-axis\n",
    "    ),\n",
    "    yaxis_tickmode='array',\n",
    "    yaxis_tickvals=pd.date_range(start=missing_values.index.min(), end=missing_values.index.max(), freq='ME'),\n",
    "    yaxis_ticktext=pd.date_range(start=missing_values.index.min(), end=missing_values.index.max(), freq='ME').strftime('%b %Y')\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Post-processing metadata: 100%|██████████| 481/481 [00:00<00:00, 237487.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading CSV files: 100%|██████████| 454/454 [00:10<00:00, 41.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate dates found: 1004 (see log file ../logs\\measureDuplicates.csv for more details)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking data integrity: 100%|██████████| 451/451 [00:00<00:00, 6855.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System 2026239 : No measures found\n",
      "System 2026239 : No 'pv_kwp' found\n",
      "System a001001 : No 'pv_wp' found for array 1\n",
      "System a001001 : No 'pv_number' found for array 1\n",
      "System a001028 : The key-value 'pv_azimut:39-129-219' is not a number for array 1\n",
      "System a001038 : The key-value 'pv_azimut:58-138-238' is not a number for array 1\n",
      "System a001103 : No 'pv_tilt' found for array 1\n",
      "System a001116 : No 'pv_wp' found for array 2\n",
      "System a001118 : The key-value 'pv_azimut:55-235' is not a number for array 1\n",
      "System a001122 : No 'pv_tilt' found for array 2\n",
      "System a001122 : No 'pv_tilt' found for array 1\n",
      "System a001164 : No measures found\n",
      "System a001165 : The key-value 'pv_azimut:90° / 270°' is not a number for array 1\n",
      "System a001199 : No 'pv_azimut' found for array 1\n",
      "System a001222 : The key-value 'pv_azimut:56-146-236' is not a number for array 1\n",
      "System a001226 : No 'pv_tilt' found for array 1\n",
      "System a001226 : No 'pv_azimut' found for array 1\n",
      "System a001226 : No 'pv_number' found for array 1\n",
      "System a001258 : No 'pv_tilt' found for array 1\n",
      "System a001258 : No 'pv_azimut' found for array 1\n",
      "System a001258 : No 'pv_number' found for array 1\n",
      "System a001280 : No 'pv_tilt' found for array 2\n",
      "System a001280 : No 'pv_tilt' found for array 1\n",
      "System a001324 : No 'pv_azimut' found for array 1\n",
      "System a001380 : No 'pv_tilt' found for array 1\n",
      "System a001382 : No 'pv_tilt' found for array 3\n",
      "System a001382 : No 'pv_azimut' found for array 3\n",
      "System a001382 : No 'pv_wp' found for array 3\n",
      "System a001404 : No 'pv_tilt' found for array 2\n",
      "System a001404 : No 'pv_tilt' found for array 1\n",
      "System a001411 : No 'pv_tilt' found for array 2\n",
      "System a001412 : No 'pv_tilt' found for array 3\n",
      "System a001412 : No 'pv_tilt' found for array 4\n",
      "System a001415 : The key-value 'pv_azimut:152/332' is not a number for array 1\n",
      "System a001415 : No 'pv_azimut' found for array 2\n",
      "System a001415 : No 'pv_wp' found for array 2\n",
      "System a001415 : No 'pv_number' found for array 2\n",
      "System a001422 : No 'pv_number' found for array 1\n",
      "System a001423 : No measures found\n",
      "System a001423 : No 'loc_latitude' found\n",
      "System a001423 : No 'loc_longitude' found\n",
      "System a001423 : No 'loc_altitude' found\n",
      "System a001426 : No 'pv_tilt' found for array 1\n",
      "System a001434 : No 'loc_latitude' found\n",
      "System a001434 : No 'loc_longitude' found\n",
      "System a001434 : No 'loc_altitude' found\n",
      "System a001446 : The key-value 'pv_azimut:150/330' is not a number for array 1\n",
      "System a001460 : No 'pv_azimut' found for array 2\n",
      "System a001460 : The key-value 'pv_azimut:47-137-227-317' is not a number for array 1\n",
      "System a001466 : No 'pv_kwp' found\n",
      "System a001466 : No PV arrays found\n",
      "System a001471 : The key-value 'pv_azimut:51 / 231' is not a number for array 1\n",
      "System a001477 : The key-value 'pv_azimut:141 / 321' is not a number for array 1\n",
      "System a001488 : No 'pv_tilt' found for array 3\n",
      "System a001488 : No 'pv_azimut' found for array 3\n",
      "System a001488 : No 'pv_tilt' found for array 2\n",
      "System a001488 : No 'pv_azimut' found for array 2\n",
      "System a001488 : No 'pv_tilt' found for array 4\n",
      "System a001488 : No 'pv_azimut' found for array 4\n",
      "System a001488 : No 'pv_tilt' found for array 1\n",
      "System a001488 : The key-value 'pv_azimut:Nordost 46 Südost 136 Südwest 226' is not a number for array 1\n",
      "System a001495 : The key-value 'pv_azimut:Südost: 147°' is not a number for array 1\n",
      "System a001499 : The key-value 'pv_azimut:West 270° / Ost 90°' is not a number for array 1\n",
      "System a001501 : The key-value 'pv_azimut:95/275' is not a number for array 1\n",
      "System a001502 : The key-value 'pv_azimut:238 / 148 / 58' is not a number for array 1\n",
      "System a001507 : The key-value 'pv_azimut:239 /59' is not a number for array 1\n",
      "System a001509 : The key-value 'pv_azimut: 141 / 321' is not a number for array 1\n",
      "System a001514 : No 'pv_tilt' found for array 1\n",
      "System a001514 : No 'pv_azimut' found for array 1\n",
      "System a001514 : No 'pv_number' found for array 1\n",
      "System a001520 : The key-value 'pv_azimut:322/142' is not a number for array 1\n",
      "System a001528 : The key-value 'pv_azimut:146 / 326' is not a number for array 1\n",
      "System a001539 : The key-value 'pv_azimut:West: 293° / Ost: 113°' is not a number for array 1\n",
      "System a001541 : No 'pv_tilt' found for array 1\n",
      "System a001541 : The key-value 'pv_azimut:75/255' is not a number for array 1\n",
      "System a001544 : No 'pv_azimut' found for array 2\n",
      "System a001544 : The key-value 'pv_azimut:299/119' is not a number for array 1\n",
      "System a001545 : The key-value 'pv_azimut:141 / 51' is not a number for array 1\n",
      "System a001550 : No measures found\n",
      "System a001556 : The key-value 'pv_azimut:90 / 270' is not a number for array 1\n",
      "System a001558 : No measures found\n",
      "System a001558 : No 'loc_latitude' found\n",
      "System a001558 : No 'loc_longitude' found\n",
      "System a001558 : No 'loc_altitude' found\n",
      "System a001558 : No 'pv_kwp' found\n",
      "System a001558 : No PV arrays found\n",
      "System a001562 : The key-value 'pv_azimut:174 /354' is not a number for array 1\n",
      "System a001564 : The key-value 'pv_azimut:116 / 296' is not a number for array 1\n",
      "System a001565 : No 'pv_wp' found for array 1\n",
      "System a001565 : No 'pv_wp' found for array 2\n",
      "System a001567 : The key-value 'pv_azimut:134 /224' is not a number for array 1\n",
      "System a001568 : No 'pv_tilt' found for array 1\n",
      "System a001568 : No 'pv_azimut' found for array 1\n",
      "System a001568 : No 'pv_number' found for array 1\n",
      "System a001573 : The key-value 'pv_azimut:67 / 157 /247' is not a number for array 1\n",
      "System a001574 : No 'pv_azimut' found for array 2\n",
      "System a001574 : The key-value 'pv_azimut:156 / 336 ' is not a number for array 1\n",
      "System a001576 : The key-value 'pv_azimut:100 / 190 /280' is not a number for array 1\n",
      "System a001578 : No 'loc_latitude' found\n",
      "System a001578 : No 'loc_longitude' found\n",
      "System a001578 : No 'loc_altitude' found\n",
      "System a001578 : No 'pv_kwp' found\n",
      "System a001578 : No PV arrays found\n",
      "System a001579 : No 'pv_kwp' found\n",
      "System a001579 : No PV arrays found\n",
      "System a001585 : The key-value 'pv_azimut:315 / 135' is not a number for array 1\n",
      "System a001586 : No measures found\n",
      "System a001586 : The key-value 'pv_azimut:59 / 239' is not a number for array 1\n",
      "System a001588 : The key-value 'pv_azimut:153/333' is not a number for array 1\n",
      "System a001594 : The key-value 'pv_azimut:243 / 63' is not a number for array 1\n",
      "System a001595 : The key-value 'pv_azimut:256+56' is not a number for array 1\n",
      "System a001597 : No measures found\n",
      "System a001597 : No 'loc_latitude' found\n",
      "System a001597 : No 'loc_longitude' found\n",
      "System a001597 : No 'loc_altitude' found\n",
      "System a001597 : No 'pv_kwp' found\n",
      "System a001597 : No PV arrays found\n",
      "System a001601 : The key-value 'pv_azimut:106 / 286' is not a number for array 1\n",
      "System a001602 : The key-value 'pv_azimut:243 / 63' is not a number for array 1\n",
      "System a001603 : The key-value 'pv_azimut:116 / 296' is not a number for array 1\n",
      "System a001604 : The key-value 'pv_azimut:111 / 291' is not a number for array 1\n",
      "System a001611 : The key-value 'pv_azimut:180 /360' is not a number for array 1\n",
      "System a001612 : The key-value 'pv_azimut:65 /245' is not a number for array 1\n",
      "System a001616 : No 'pv_azimut' found for array 2\n",
      "System a001616 : The key-value 'pv_azimut:180/90/270' is not a number for array 1\n",
      "System a001617 : The key-value 'pv_azimut:28 /208' is not a number for array 1\n",
      "System a001622 : No measures found\n",
      "System a001626 : The key-value 'pv_azimut:132 /45' is not a number for array 1\n",
      "System a001627 : No 'loc_latitude' found\n",
      "System a001627 : No 'loc_longitude' found\n",
      "System a001627 : No 'loc_altitude' found\n",
      "System a001629 : The key-value 'pv_azimut:0-180-62-242' is not a number for array 1\n",
      "System a001631 : The key-value 'pv_azimut:270 / 90' is not a number for array 1\n",
      "System a001632 : The key-value 'pv_azimut:267 / 87' is not a number for array 1\n",
      "System a001635 : The key-value 'pv_azimut:100 / 280' is not a number for array 1\n",
      "System a001636 : The key-value 'pv_azimut:260/180' is not a number for array 1\n",
      "System a001640 : The key-value 'pv_azimut:82 / 262' is not a number for array 1\n",
      "System a001643 : The key-value 'pv_azimut:236 /146' is not a number for array 1\n",
      "System a001644 : The key-value 'pv_azimut:130/310' is not a number for array 1\n",
      "System a001653 : The key-value 'pv_azimut:106 / 286' is not a number for array 1\n",
      "System a001654 : No measures found\n",
      "System a001666 : No 'pv_azimut' found for array 1\n",
      "System a001694 : The key-value 'pv_azimut:41 / 221' is not a number for array 1\n",
      "System a001711 : The key-value 'pv_azimut:113 / 293' is not a number for array 1\n",
      "System a001719 : The key-value 'pv_azimut:22 /202' is not a number for array 1\n",
      "System a001720 : The key-value 'pv_azimut:106 / 286' is not a number for array 1\n",
      "System a001721 : The key-value 'pv_azimut:22 / 202' is not a number for array 1\n",
      "System a001779 : No measures found\n",
      "System a001779 : No 'pv_kwp' found\n",
      "System a001779 : No PV arrays found\n",
      "System a001794 : No measures found\n",
      "System a001794 : The key-value 'pv_azimut:69 /249' is not a number for array 1\n",
      "System a001811 : No measures found\n",
      "System a001811 : No 'loc_latitude' found\n",
      "System a001811 : No 'loc_longitude' found\n",
      "System a001811 : No 'loc_altitude' found\n",
      "System a001811 : No 'pv_kwp' found\n",
      "System a001811 : No PV arrays found\n",
      "System g001001 : The key-value 'pv_azimut:269-89' is not a number for array 1\n",
      "Number of systems with all the necessary data: 356/451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Max production normalizers:  56%|█████▋    | 201/356 [01:55<01:29,  1.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m data_handler\u001b[38;5;241m.\u001b[39mcheck_integrity()\n\u001b[0;32m      7\u001b[0m max_production_normalizers \u001b[38;5;241m=\u001b[39m MaxProductionNormalizers()\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmax_production_normalizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_tune_MaxProductionNormalizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 23\u001b[0m, in \u001b[0;36mMaxProductionNormalizers.run\u001b[1;34m(self, data_handler, tune)\u001b[0m\n\u001b[0;32m     21\u001b[0m measures \u001b[38;5;241m=\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mget_measures(systems_name\u001b[38;5;241m=\u001b[39msystem_name)\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m     22\u001b[0m dates \u001b[38;5;241m=\u001b[39m measures\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m---> 23\u001b[0m \u001b[43mmax_production_normalizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_estimation_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1h\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# add the estimation to the dictionary\u001b[39;00m\n\u001b[0;32m     26\u001b[0m estimated_max_productions_dic[system_name] \u001b[38;5;241m=\u001b[39m max_production_normalizer\u001b[38;5;241m.\u001b[39mestimated_max_production\n",
      "Cell \u001b[1;32mIn[7], line 47\u001b[0m, in \u001b[0;36mMaxProductionNormalizer.generate_estimation_data\u001b[1;34m(self, dates, sampling_freq)\u001b[0m\n\u001b[0;32m     43\u001b[0m weather_clearsky \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlocation\u001b[38;5;241m.\u001b[39mget_clearsky(all_datetimes)  \u001b[38;5;66;03m# In W/m2\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# TODO adjust the clear sky model to take into account the horizon https://pvlib-python.readthedocs.io/en/stable/gallery/shading/plot_simple_irradiance_adjustment_for_horizon_shading.html\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Run the model to get the estimated production\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweather_clearsky\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m production_sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mresults\u001b[38;5;241m.\u001b[39mac \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# Convert W to kW\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimated_max_production \u001b[38;5;241m=\u001b[39m daily_energy(production_sampling_rate)\n",
      "File \u001b[1;32mc:\\Users\\conta\\Documents\\TM_PV_ADC\\.venv\\Lib\\site-packages\\pvlib\\modelchain.py:1842\u001b[0m, in \u001b[0;36mModelChain.run_model\u001b[1;34m(self, weather)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;124;03mRun the model chain starting with broadband global, diffuse and/or\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;124;03mdirect irradiance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1839\u001b[0m \u001b[38;5;124;03mpvlib.modelchain.ModelChain.run_model_from_effective_irradiance\u001b[39;00m\n\u001b[0;32m   1840\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1841\u001b[0m weather \u001b[38;5;241m=\u001b[39m _to_tuple(weather)\n\u001b[1;32m-> 1842\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweather\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maoi_model()\n\u001b[0;32m   1844\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspectral_model()\n",
      "File \u001b[1;32mc:\\Users\\conta\\Documents\\TM_PV_ADC\\.venv\\Lib\\site-packages\\pvlib\\modelchain.py:1586\u001b[0m, in \u001b[0;36mModelChain.prepare_inputs\u001b[1;34m(self, weather)\u001b[0m\n\u001b[0;32m   1583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_df(weather, required\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mghi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdni\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdhi\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m   1584\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_weather(weather)\n\u001b[1;32m-> 1586\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prep_inputs_solar_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweather\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1587\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prep_inputs_airmass()\n\u001b[0;32m   1588\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prep_inputs_albedo(weather)\n",
      "File \u001b[1;32mc:\\Users\\conta\\Documents\\TM_PV_ADC\\.venv\\Lib\\site-packages\\pvlib\\modelchain.py:1398\u001b[0m, in \u001b[0;36mModelChain._prep_inputs_solar_pos\u001b[1;34m(self, weather)\u001b[0m\n\u001b[0;32m   1395\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   1396\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m-> 1398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults\u001b[38;5;241m.\u001b[39msolar_position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_solarposition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1399\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolar_position_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1400\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\conta\\Documents\\TM_PV_ADC\\.venv\\Lib\\site-packages\\pvlib\\location.py:193\u001b[0m, in \u001b[0;36mLocation.get_solarposition\u001b[1;34m(self, times, pressure, temperature, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pressure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m     pressure \u001b[38;5;241m=\u001b[39m atmosphere\u001b[38;5;241m.\u001b[39malt2pres(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maltitude)\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msolarposition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_solarposition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatitude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mlongitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlongitude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43maltitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maltitude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mpressure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpressure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\conta\\Documents\\TM_PV_ADC\\.venv\\Lib\\site-packages\\pvlib\\solarposition.py:112\u001b[0m, in \u001b[0;36mget_solarposition\u001b[1;34m(time, latitude, longitude, altitude, pressure, method, temperature, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m     ephem_df \u001b[38;5;241m=\u001b[39m spa_python(time, latitude, longitude, altitude,\n\u001b[0;32m    109\u001b[0m                           pressure, temperature,\n\u001b[0;32m    110\u001b[0m                           how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumba\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnrel_numpy\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 112\u001b[0m     ephem_df \u001b[38;5;241m=\u001b[39m \u001b[43mspa_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatitude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlongitude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maltitude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mpressure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumpy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyephem\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    116\u001b[0m     ephem_df \u001b[38;5;241m=\u001b[39m pyephem(time, latitude, longitude,\n\u001b[0;32m    117\u001b[0m                        altitude\u001b[38;5;241m=\u001b[39maltitude,\n\u001b[0;32m    118\u001b[0m                        pressure\u001b[38;5;241m=\u001b[39mpressure,\n\u001b[0;32m    119\u001b[0m                        temperature\u001b[38;5;241m=\u001b[39mtemperature, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\conta\\Documents\\TM_PV_ADC\\.venv\\Lib\\site-packages\\pvlib\\solarposition.py:385\u001b[0m, in \u001b[0;36mspa_python\u001b[1;34m(time, latitude, longitude, altitude, pressure, temperature, delta_t, atmos_refract, how, numthreads)\u001b[0m\n\u001b[0;32m    380\u001b[0m spa \u001b[38;5;241m=\u001b[39m _spa_python_import(how)\n\u001b[0;32m    382\u001b[0m delta_t \u001b[38;5;241m=\u001b[39m delta_t \u001b[38;5;129;01mor\u001b[39;00m spa\u001b[38;5;241m.\u001b[39mcalculate_deltat(time\u001b[38;5;241m.\u001b[39myear, time\u001b[38;5;241m.\u001b[39mmonth)\n\u001b[0;32m    384\u001b[0m app_zenith, zenith, app_elevation, elevation, azimuth, eot \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m--> 385\u001b[0m     \u001b[43mspa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolar_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43munixtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpressure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdelta_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matmos_refract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumthreads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    388\u001b[0m result \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapparent_zenith\u001b[39m\u001b[38;5;124m'\u001b[39m: app_zenith, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzenith\u001b[39m\u001b[38;5;124m'\u001b[39m: zenith,\n\u001b[0;32m    389\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapparent_elevation\u001b[39m\u001b[38;5;124m'\u001b[39m: app_elevation,\n\u001b[0;32m    390\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124melevation\u001b[39m\u001b[38;5;124m'\u001b[39m: elevation, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mazimuth\u001b[39m\u001b[38;5;124m'\u001b[39m: azimuth,\n\u001b[0;32m    391\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mequation_of_time\u001b[39m\u001b[38;5;124m'\u001b[39m: eot},\n\u001b[0;32m    392\u001b[0m                       index\u001b[38;5;241m=\u001b[39mtime)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\conta\\Documents\\TM_PV_ADC\\.venv\\Lib\\site-packages\\pvlib\\spa.py:1088\u001b[0m, in \u001b[0;36msolar_position\u001b[1;34m(unixtime, lat, lon, elev, pressure, temp, delta_t, atmos_refract, numthreads, sst, esd)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1086\u001b[0m     do_calc \u001b[38;5;241m=\u001b[39m solar_position_numpy\n\u001b[1;32m-> 1088\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdo_calc\u001b[49m\u001b[43m(\u001b[49m\u001b[43munixtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpressure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matmos_refract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mesd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\conta\\Documents\\TM_PV_ADC\\.venv\\Lib\\site-packages\\pvlib\\spa.py:987\u001b[0m, in \u001b[0;36msolar_position_numpy\u001b[1;34m(unixtime, lat, lon, elev, pressure, temp, delta_t, atmos_refract, numthreads, sst, esd)\u001b[0m\n\u001b[0;32m    985\u001b[0m x4 \u001b[38;5;241m=\u001b[39m moon_ascending_longitude(jce)\n\u001b[0;32m    986\u001b[0m l_o_nutation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mlen\u001b[39m(x0)))\n\u001b[1;32m--> 987\u001b[0m \u001b[43mlongitude_obliquity_nutation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_o_nutation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m delta_psi \u001b[38;5;241m=\u001b[39m l_o_nutation[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    989\u001b[0m delta_epsilon \u001b[38;5;241m=\u001b[39m l_o_nutation[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\conta\\Documents\\TM_PV_ADC\\.venv\\Lib\\site-packages\\pvlib\\spa.py:575\u001b[0m, in \u001b[0;36mlongitude_obliquity_nutation\u001b[1;34m(julian_ephemeris_century, x0, x1, x2, x3, x4, out)\u001b[0m\n\u001b[0;32m    567\u001b[0m     arg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mradians(\n\u001b[0;32m    568\u001b[0m         NUTATION_YTERM_ARRAY[row, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mx0 \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    569\u001b[0m         NUTATION_YTERM_ARRAY[row, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39mx1 \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    572\u001b[0m         NUTATION_YTERM_ARRAY[row, \u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m*\u001b[39mx4\n\u001b[0;32m    573\u001b[0m     )\n\u001b[0;32m    574\u001b[0m     delta_psi_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (a \u001b[38;5;241m+\u001b[39m b \u001b[38;5;241m*\u001b[39m julian_ephemeris_century) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msin(arg)\n\u001b[1;32m--> 575\u001b[0m     delta_eps_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (c \u001b[38;5;241m+\u001b[39m d \u001b[38;5;241m*\u001b[39m julian_ephemeris_century) \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcos\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m delta_psi \u001b[38;5;241m=\u001b[39m delta_psi_sum\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m36000000\u001b[39m\n\u001b[0;32m    577\u001b[0m delta_eps \u001b[38;5;241m=\u001b[39m delta_eps_sum\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m36000000\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_handler = DataHandler(data_dirpath, cache_dirpath)\n",
    "data_handler.load_metadata()\n",
    "data_handler.load_csv()\n",
    "data_handler.check_integrity()\n",
    "\n",
    "\n",
    "max_production_normalizers = MaxProductionNormalizers()\n",
    "max_production_normalizers.run(data_handler, tune=force_tune_MaxProductionNormalizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler.check_outliers(max_threshold=1.1, min_threshold=0.01)\n",
    "data_handler.create_train_test_set(test_size=testing_days, max_train_size=50, random_state=random_state, shuffle=False)\n",
    "data_handler.check_training_size(min_training_days=min_training_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Dash app\n",
    "# no_raise_mode = True\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "tab_height = '2em'\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "        dcc.Dropdown(\n",
    "            id='system-dropdown',\n",
    "            options=[{'label': name, 'value': name} for name in data_handler.valid_systems],\n",
    "            value=data_handler.valid_systems[0],\n",
    "            style={'width': '50%'}  # Adjust width and font size\n",
    "        ),\n",
    "        html.Div(id='metric-text-container', style={'display': 'inline-block', 'margin-left': '20px'})  # Container for the metric text\n",
    "    ], style={'display': 'flex', 'align-items': 'center'}),  # Align items horizontally\n",
    "    dcc.Tabs(id='plot-tabs', value='tab-energy', children=[\n",
    "        dcc.Tab(label='Energy', value='tab-energy', style={'padding': '0px', 'lineHeight': tab_height}, selected_style={'padding': '0px', 'lineHeight': tab_height, 'fontWeight': 'bold'}),  # Adjust height and line height\n",
    "        dcc.Tab(label='Normalizer Tuning', value='tab-norm-tuning', style={'padding': '0px', 'lineHeight': tab_height}, selected_style={'padding': '0px', 'lineHeight': tab_height, 'fontWeight': 'bold'}),  # Adjust height and line height\n",
    "        dcc.Tab(label='Relative Energy', value='tab-rel-energy', style={'padding': '0px', 'lineHeight': tab_height}, selected_style={'padding': '0px', 'lineHeight': tab_height, 'fontWeight': 'bold'}),  # Adjust height and line height\n",
    "        # plot systemsData_RelativeDelta_val\n",
    "        dcc.Tab(label='Delta Error', value='tab-delta-rel-energy', style={'padding': '0px', 'lineHeight': tab_height}, selected_style={'padding': '0px', 'lineHeight': tab_height, 'fontWeight': 'bold'}),  # Adjust height and line height\n",
    "        dcc.Tab(label='All Relative Energy', value='tab-rel-energy-all', style={'padding': '0px', 'lineHeight': tab_height}, selected_style={'padding': '0px', 'lineHeight': tab_height, 'fontWeight': 'bold'}),  # Adjust height and line height\n",
    "        dcc.Tab(label='All Missing Value', value='tab-miss-val-all', style={'padding': '0px', 'lineHeight': tab_height}, selected_style={'padding': '0px', 'lineHeight': tab_height, 'fontWeight': 'bold'}),  # Adjust height and line height\n",
    "        dcc.Tab(label='Similar neighboring systems', value='tab-neighbors', style={'padding': '0px', 'lineHeight': tab_height}, selected_style={'padding': '0px', 'lineHeight': tab_height, 'fontWeight': 'bold'}),  # Adjust height and line height\n",
    "\n",
    "    ]),  # Adjust height for tabs\n",
    "    html.Div(id='tabs-content', style={'flex': '1 1 auto'})  # Allow the tabs-content div to grow\n",
    "], style={'display': 'flex', 'flexDirection': 'column', 'height': '100vh'})  # Make the outer container fill the screen height\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    [Output('tabs-content', 'children'),\n",
    "     Output('metric-text-container', 'children')],\n",
    "    [Input('plot-tabs', 'value'),\n",
    "     Input('system-dropdown', 'value')]\n",
    ")\n",
    "def render_content(tab, selected_system):\n",
    "    # Statistic text\n",
    "    try:\n",
    "        mae_train = regressorsMetrics_train.loc[selected_system]\n",
    "    except:\n",
    "        mae_train = np.nan\n",
    "    try:\n",
    "        mae_val = regressorsMetrics_val.loc[selected_system]\n",
    "    except:\n",
    "        mae_val = np.nan\n",
    "    try:\n",
    "        loss = data_handler.get_metadata(selected_system)['metadata']['loss']\n",
    "    except:\n",
    "        loss = np.nan\n",
    "\n",
    "    mae_train_text = f\"Estimator Train Error : {mae_train * 100:.2f}%\"\n",
    "    mae_test_text = f\"Estimator Test Error  : {mae_val * 100:.2f}%\"\n",
    "    loss_text = f\"System Loss   : {loss * 100:.2f}%\"\n",
    "\n",
    "    metric_text_div = html.Div([\n",
    "        html.Div(mae_train_text),\n",
    "        html.Div(mae_test_text),\n",
    "        html.Div(loss_text)\n",
    "    ], style={'fontSize': 16})\n",
    "\n",
    "    if tab == 'tab-energy':\n",
    "        fig1 = go.Figure(layout_yaxis_title=\"Daily Energy (kWh)\")\n",
    "\n",
    "        # remove nan from systemsData_EstimatedMaxDailyEnergy[selected_system]\n",
    "\n",
    "        try:\n",
    "            data = data_handler.estimated_max_production[selected_system].dropna()\n",
    "            fig1.add_trace(go.Scatter(\n",
    "                x=data.index,\n",
    "                y=data,\n",
    "                mode='markers',\n",
    "                name='Estimated Max Daily Energy',\n",
    "                marker_color='LightSeaGreen'\n",
    "            ))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            data = data_handler.get_measures(set='all', systems_name = selected_system)\n",
    "            fig1.add_trace(go.Scatter(\n",
    "                x=data.index,\n",
    "                y=data,\n",
    "                mode='markers',\n",
    "                name='Measured Daily Energy',\n",
    "                marker_color='blue'\n",
    "            ))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            expectedDailyEnergy_val_mean = systemsData_ExpectedDailyEnergy_val_mean[selected_system].dropna()\n",
    "            expectedDailyEnergy_val_std = systemsData_ExpectedDailyEnergy_val_std[selected_system].dropna()\n",
    "            fig1.add_trace(go.Scatter(\n",
    "                x=expectedDailyEnergy_val_mean.index,\n",
    "                y=expectedDailyEnergy_val_mean,\n",
    "                mode='markers',\n",
    "                name='Expected Daily Energy',\n",
    "                marker_color='red'\n",
    "                # error_y=dict(\n",
    "                #     type='data',\n",
    "                #     array=expectedDailyEnergy_val_std,\n",
    "                #     visible=True\n",
    "                # )\n",
    "            ))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Update layout for legend position\n",
    "        fig1.update_layout(\n",
    "            legend=dict(\n",
    "                x=0.99,\n",
    "                y=0.99,\n",
    "                xanchor='right',\n",
    "                yanchor='top',\n",
    "                orientation='h'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return dcc.Graph(figure=fig1, style={'height': '100%', 'width': '100%'}), metric_text_div  # Adjust height and width of the figure\n",
    "\n",
    "    elif tab == 'tab-norm-tuning':\n",
    "        fig1 = go.Figure(layout_yaxis_title=\"Daily Energy (kWh)\")\n",
    "        try:\n",
    "            data = data_handler.get_measures(set='all', systems_name = selected_system)\n",
    "            fig1.add_trace(go.Scatter(\n",
    "                x=data.index,\n",
    "                y=data,\n",
    "                mode='markers',\n",
    "                name='Measured Daily Energy',\n",
    "                marker_color='blue'\n",
    "            ))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            data = data_handler._measures[data_handler.tuner_measures_max_mask][selected_system].dropna()\n",
    "            fig1.add_trace(go.Scatter(\n",
    "                x=data.index,\n",
    "                y=data,\n",
    "                mode='markers',\n",
    "                name='Max Measured Daily Energy (7 days)',\n",
    "                marker_color='red'\n",
    "            ))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            data = data_handler._measures[data_handler.tuner_measures_outliers_mask][selected_system].dropna()\n",
    "            fig1.add_trace(go.Scatter(\n",
    "                x=data.index,\n",
    "                y=data,\n",
    "                mode='markers',\n",
    "                name='Tuning Outliers',\n",
    "                marker_color='yellow'\n",
    "            ))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            data = data_handler.tuner_estimated_max_productions_untuned[selected_system].dropna()\n",
    "            fig1.add_trace(go.Scatter(\n",
    "                x=data.index,\n",
    "                y=data,\n",
    "                mode='markers',\n",
    "                name='Estimated Max Daily Energy (Untuned)',\n",
    "                marker_color='violet'\n",
    "            ))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            data = data_handler.estimated_max_production[selected_system].dropna()\n",
    "            fig1.add_trace(go.Scatter(\n",
    "                x=data.index,\n",
    "                y=data,\n",
    "                mode='markers',\n",
    "                name='Estimated Max Daily Energy',\n",
    "                marker_color='LightSeaGreen'\n",
    "            ))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        fig1.update_layout(\n",
    "            legend=dict(\n",
    "                x=0.99,\n",
    "                y=0.99,\n",
    "                xanchor='right',\n",
    "                yanchor='top',\n",
    "                orientation='h'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return dcc.Graph(figure=fig1, style={'height': '100%', 'width': '100%'}), metric_text_div\n",
    "    elif tab == 'tab-rel-energy':\n",
    "        fig1 = go.Figure(layout_yaxis_title=\"Proportional Daily Energy (%)\")\n",
    "        # add a line at 100% for the Estimated Max Daily Energy\n",
    "        data = data_handler.estimated_max_production[selected_system].dropna()\n",
    "        fig1.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=data.index.min(),\n",
    "            y0=100,\n",
    "            x1=data.index.max(),\n",
    "            y1=100,\n",
    "            name='Estimated Max Daily Energy',\n",
    "            line_color='LightSeaGreen'\n",
    "        )\n",
    "        # try:\n",
    "        data = data_handler.normalize(data_handler.get_measures(set='all', systems_name = selected_system))\n",
    "        fig1.add_trace(go.Scatter(\n",
    "            x=data.index,\n",
    "            y=data * 100,\n",
    "            mode='markers',\n",
    "            name='Measured Daily Energy',\n",
    "            marker_color='blue'\n",
    "        ))\n",
    "        # except:\n",
    "        #     pass\n",
    "        try:\n",
    "            relativeExpectedDailyEnergy_val_mean = systemsData_RelativeExpectedDailyEnergy_val_mean[selected_system].dropna()\n",
    "            fig1.add_trace(go.Scatter(\n",
    "                x=relativeExpectedDailyEnergy_val_mean.index,\n",
    "                y=relativeExpectedDailyEnergy_val_mean * 100,\n",
    "                mode='markers',\n",
    "                name='Expected Daily Energy',\n",
    "                marker_color='red'\n",
    "                # error_y=dict(\n",
    "                #     type='data',\n",
    "                #     array=systemsData_RelativeExpectedDailyEnergy_val_std[selected_system] * 100,\n",
    "                #     visible=True\n",
    "                # )\n",
    "            ))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return dcc.Graph(figure=fig1, style={'height': '100%', 'width': '100%'}), metric_text_div  # Adjust height and width of the figure\n",
    "    elif tab == 'tab-delta-rel-energy':\n",
    "        fig1 = go.Figure(layout_yaxis_title=\"Proportional Daily Energy Error(%)\")\n",
    "        try:\n",
    "            relativeDelta_val = systemsData_RelativeDelta_val[selected_system].dropna()\n",
    "            fig1.add_trace(go.Scatter(\n",
    "                x=relativeDelta_val.index,\n",
    "                y=relativeDelta_val * 100,\n",
    "                mode='markers',\n",
    "                name='Relative Delta Energy',\n",
    "            ))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            relativeDelta_val_detected = systemsData_RelativeDelta_val_detected[selected_system].dropna()\n",
    "            fig1.add_trace(go.Scatter(\n",
    "                x=relativeDelta_val_detected.index,\n",
    "                y=relativeDelta_val_detected * 100,\n",
    "                mode='markers',\n",
    "                name='Detected Errors',\n",
    "                marker_color='red'\n",
    "            ))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return dcc.Graph(figure=fig1, style={'height': '100%', 'width': '100%'}), metric_text_div  # Adjust height and width of the figure\n",
    "    elif tab == 'tab-rel-energy-all':\n",
    "        fig1 = go.Figure(layout_yaxis_title=\"Proportional Daily Energy (%)\")\n",
    "        try:\n",
    "            all_data = data_handler.normalize(data_handler.get_measures(set='all'))\n",
    "            for system_name in data_handler.valid_systems:\n",
    "                if system_name != selected_system:\n",
    "                    fig1.add_trace(go.Scatter(\n",
    "                        x=all_data[system_name].index,\n",
    "                        y=all_data[system_name] * 100,\n",
    "                        mode='markers',\n",
    "                        name=f'{system_name}',\n",
    "                        marker_color='blue'\n",
    "                    ))\n",
    "            fig1.add_trace(go.Scatter(\n",
    "                x=all_data[selected_system].index,\n",
    "                y=all_data[selected_system] * 100,\n",
    "                mode='markers',\n",
    "                name=f'{selected_system}',\n",
    "                marker_color='red'\n",
    "            ))\n",
    "            fig1.update_layout(yaxis=dict(range=[-5, 120]))\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return dcc.Graph(figure=fig1, style={'height': '100%', 'width': '100%'}), metric_text_div  # Adjust height and width of the figure\n",
    "    elif tab == 'tab-miss-val-all':\n",
    "        data = (~data_handler.get_missing_value(sorted=True)).astype(int)\n",
    "\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=data,\n",
    "            x=data.columns,\n",
    "            y=data.index,\n",
    "            showscale=False,\n",
    "            colorscale='Greys'  # Set colorscale to black and white\n",
    "        ))\n",
    "        fig.update_layout(\n",
    "            yaxis=dict(\n",
    "                autorange='reversed',  # Invert the y-axis\n",
    "                showticklabels=True,\n",
    "                tickmode='array',\n",
    "                tickvals=pd.date_range(start=data.index.min(), end=data.index.max(), freq='ME'),\n",
    "                ticktext=pd.date_range(start=data.index.min(), end=data.index.max(), freq='ME').strftime('%b %Y')\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return dcc.Graph(figure=fig, style={'height': '100%', 'width': '100%'}), metric_text_div  # Adjust height and width of the figure\n",
    "\n",
    "    elif tab == 'tab-neighbors':\n",
    "        fig2 = go.Figure()\n",
    "\n",
    "        # Add initial traces with secondary y-axis\n",
    "        try:\n",
    "            fig2.add_trace(go.Bar(\n",
    "                x=features_importance_df.columns,\n",
    "                y=features_importance_df.loc[selected_system],\n",
    "                name='Impurity-based Importance',\n",
    "                yaxis='y1',\n",
    "                offsetgroup=1\n",
    "            ))\n",
    "            fig2.update_layout(\n",
    "                yaxis1=dict(\n",
    "                    title='Impurity-based Importance',\n",
    "                    range=[0, features_importance_df.loc[selected_system].max()],\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            fig2.add_trace(go.Bar(\n",
    "                x=permutation_importance_mean_df.columns,\n",
    "                y=permutation_importance_mean_df.loc[selected_system],\n",
    "                name='Permutation Importance',\n",
    "                yaxis='y2',\n",
    "                offsetgroup=2\n",
    "            ))\n",
    "            fig2.update_layout(\n",
    "                yaxis2=dict(\n",
    "                    title='Permutation Importance',\n",
    "                    overlaying='y',\n",
    "                    side='right',\n",
    "                    range=[0, permutation_importance_mean_df.loc[selected_system].max()],\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return dcc.Graph(figure=fig2, style={'height': '100%', 'width': '100%'}), metric_text_div  # Adjust height and width of the figure\n",
    "\n",
    "\n",
    "def open_browser():\n",
    "    webbrowser.open(\"http://127.0.0.1:8060/\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Open the Dash app in a new browser window\n",
    "    Timer(1, open_browser).start()\n",
    "    app.run_server(debug=True, use_reloader=False, port=8060)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
