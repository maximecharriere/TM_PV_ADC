{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pvlib\n",
    "import json\n",
    "import os\n",
    "from pvlib.pvsystem import PVSystem, Array, FixedMount\n",
    "from pvlib.location import Location\n",
    "from pvlib.modelchain import ModelChain\n",
    "from pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, root_mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import forestci as fci\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import threading\n",
    "from sklearn.metrics import make_scorer\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "import plotly.graph_objects as go\n",
    "from dash.dependencies import Input, Output\n",
    "import webbrowser\n",
    "from threading import Timer\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.utils.parallel import Parallel, delayed\n",
    "from sklearn.utils.validation import (\n",
    "    check_is_fitted,\n",
    ")\n",
    "from sklearn.ensemble._base import _partition_estimators\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.renderers.default = \"browser\"  # render plotly figures in browser\n",
    "\n",
    "PARENT_DATA_DIR = os.getenv('PARENT_DATA_DIR')\n",
    "if PARENT_DATA_DIR is None:\n",
    "    raise ValueError(\"PARENT_DATA_DIR environment variable is not set\")\n",
    "\n",
    "\n",
    "dataDirpath = PARENT_DATA_DIR + r\"\\PRiOT\\dataExport_1\"  # \"/Applications/Documents/TM Maxime/dataExport_3400_daily\"#\n",
    "dataCacheDirpath = os.path.join(dataDirpath, \"cache\")\n",
    "logsDirpath = \"../logs\"\n",
    "\n",
    "if not os.path.exists(logsDirpath):\n",
    "    os.makedirs(logsDirpath)\n",
    "\n",
    "if not os.path.exists(dataCacheDirpath):\n",
    "    os.makedirs(dataCacheDirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useCached = False\n",
    "forceTrain = True\n",
    "tuneMaxProductionEstimators = True\n",
    "random_state = 42\n",
    "\n",
    "trainingDays = 50\n",
    "minTestingDays = 10\n",
    "minMeasurements = trainingDays + minTestingDays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/model_persistence.html\n",
    "class ModelSerializer:\n",
    "    def _save_model(self, model, serial_type, save_params):\n",
    "        serial_type.dump(model, save_params)\n",
    "\n",
    "    def _retrieve_model(self, serial_type, retrieve_params):\n",
    "        return serial_type.load(retrieve_params)\n",
    "\n",
    "\n",
    "class JoblibSerializer(ModelSerializer):\n",
    "    def save_model(self, model, save_model_path, filename):\n",
    "        super()._save_model(model, joblib, os.path.join(save_model_path, filename + \".joblib\"))\n",
    "\n",
    "    def retrieve_model(self, save_model_path, filename):\n",
    "        return super()._retrieve_model(joblib, os.path.join(save_model_path, filename + '.joblib'))\n",
    "\n",
    "\n",
    "class PickleSerializer(ModelSerializer):\n",
    "    def save_model(self, model, save_model_path, filename):\n",
    "        with open(os.path.join(save_model_path, filename + \".pkl\"), 'wb') as f:\n",
    "            super()._save_model(model, pickle, f)\n",
    "\n",
    "    def retrieve_model(self, save_model_path, filename):\n",
    "        with open(os.path.join(save_model_path, filename + \".pkl\"), 'rb') as f:\n",
    "            return super()._retrieve_model(pickle, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_altitude_from_wgs84(longitude, latitude):\n",
    "    # Convert WGS84 to LV95\n",
    "    lv95_url = \"https://geodesy.geo.admin.ch/reframe/wgs84tolv95\"\n",
    "    params_lv95 = {\n",
    "        \"easting\": longitude,\n",
    "        \"northing\": latitude,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "\n",
    "    response_lv95 = requests.get(lv95_url, params=params_lv95)\n",
    "    if response_lv95.status_code != 200:\n",
    "        raise Exception(\"Error converting WGS84 to LV95: \" + response_lv95.text)\n",
    "\n",
    "    lv95_data = response_lv95.json()\n",
    "    lv95_easting = lv95_data[\"easting\"]\n",
    "    lv95_northing = lv95_data[\"northing\"]\n",
    "\n",
    "    # Get altitude from LV95 coordinates\n",
    "    altitude_url = \"https://api3.geo.admin.ch/rest/services/height\"\n",
    "    params_altitude = {\n",
    "        \"easting\": lv95_easting,\n",
    "        \"northing\": lv95_northing\n",
    "    }\n",
    "\n",
    "    response_altitude = requests.get(altitude_url, params=params_altitude)\n",
    "    if response_altitude.status_code != 200:\n",
    "        raise Exception(\"Error retrieving altitude: \" + response_altitude.text)\n",
    "\n",
    "    altitude_data = response_altitude.json()\n",
    "    altitude = altitude_data[\"height\"]\n",
    "\n",
    "    return float(altitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "\n",
    "    def __init__(self, dataDirpath, dataCacheDirpath):\n",
    "        self.dataDirpath = dataDirpath\n",
    "        self.dataCacheDirpath = dataCacheDirpath\n",
    "\n",
    "        self.measures = None\n",
    "        self.metadata = None\n",
    "\n",
    "        self.metadataFilepath = os.path.join(dataDirpath, \"metadata.json\")\n",
    "\n",
    "    def get_systems_names(self):\n",
    "        return self.measures.columns\n",
    "\n",
    "    def load_metadata(self):\n",
    "        with open(self.metadataFilepath, 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "\n",
    "        for systemName, systemMetadata in tqdm(self.metadata.items(), desc=\"Post-processing metadata\"):\n",
    "            # Add altitude to metadata, if not already present (TODO : imporove with multi threading)\n",
    "            if \"loc_altitude\" not in systemMetadata['metadata']:\n",
    "                if \"loc_longitude\" in systemMetadata['metadata'] and \"loc_latitude\" in systemMetadata['metadata']:\n",
    "                    systemMetadata['metadata'][\"loc_altitude\"] = get_altitude_from_wgs84(systemMetadata['metadata'][\"loc_longitude\"], systemMetadata['metadata'][\"loc_latitude\"])\n",
    "\n",
    "            # Add the default loss to metadata if not already present\n",
    "            if 'loss' not in systemMetadata['metadata']:\n",
    "                systemMetadata['metadata']['loss'] = 0\n",
    "\n",
    "            # Convert key with \"modX\" in the name (x is the array number) to a dictionary with the array number as key\n",
    "            keys_to_delete = []\n",
    "            for key, value in systemMetadata['metadata'].items():\n",
    "                if 'mod' in key:\n",
    "                    # Extract the module number\n",
    "                    array_num = key.split('_')[1][-1]\n",
    "                    # Remove the module number from the key\n",
    "                    new_key = '_'.join(key.split('_')[:1] + key.split('_')[2:])\n",
    "                    # Add the key-value pair to the appropriate module dictionary\n",
    "                    if 'arrays' not in systemMetadata:\n",
    "                        systemMetadata['arrays'] = {}\n",
    "                    if array_num not in systemMetadata['arrays']:\n",
    "                        systemMetadata['arrays'][array_num] = {}\n",
    "                    systemMetadata['arrays'][array_num][new_key] = value\n",
    "                    keys_to_delete.append(key)\n",
    "            for key in keys_to_delete:\n",
    "                del systemMetadata['metadata'][key]\n",
    "\n",
    "        # Save metadata with new format and value\n",
    "        self.save_metadata()\n",
    "\n",
    "    def save_metadata(self):\n",
    "        with open(self.metadataFilepath, 'w') as f:\n",
    "            json.dump(self.metadata, f, indent=4)\n",
    "\n",
    "    def load_csv(self):\n",
    "        measures_dic = {}\n",
    "        duplicates_list = []\n",
    "        for filename in tqdm(os.listdir(self.dataDirpath), desc=\"Loading CSV files\"):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                systemName = filename.split('_')[0]\n",
    "                systemMeasures = pd.read_csv(os.path.join(self.dataDirpath, filename))\n",
    "                # convert the timestamp to datetime with correct timezone\n",
    "                systemMeasures['Datetime'] = pd.to_datetime(systemMeasures['Timestamp'], unit='ms', utc=True).dt.tz_convert('Europe/Zurich')\n",
    "                # Convert the datetime to only the date, as the production is the daily production. The +1h is to manage the saving time. Normally PRiOT exports the data at midnight (local time) for the day after (e.g. the energy for the July 1st is saved at July 1st 00:00 Europe/Zurich). However it seams that the saving time is not always correctly handled, and sometime the export is done at 23:00 the day before (e.g. the energy for the July 1st is saved at June 30th 23:00 Europe/Zurich). This is why we add 1h to the datetime to be sure to have the correct date.\n",
    "                systemMeasures['Date'] = (systemMeasures['Datetime'] + pd.Timedelta(hours=1)).dt.date\n",
    "                # Set the date as index\n",
    "                systemMeasures.set_index('Date', inplace=True)\n",
    "                # Append in duplicates_list all the rows with duplicated index, for logging purpose\n",
    "                if len(systemMeasures.index.duplicated(keep=False)):\n",
    "                    duplicates_list.append(systemMeasures[systemMeasures.index.duplicated(keep=False)])\n",
    "                # keep only the measure tt_forward_active_energy_total_toDay as a Series\n",
    "                systemMeasures = systemMeasures['tt_forward_active_energy_total_toDay']\n",
    "                # Group by the index (Date) and sum the systemMeasures for each date to handle duplicates\n",
    "                systemMeasures = systemMeasures.groupby('Date').sum()\n",
    "\n",
    "                measures_dic[systemName] = systemMeasures\n",
    "        # convert the dictionary of series to a pandas dataframe\n",
    "        self.measures = pd.DataFrame(measures_dic)\n",
    "        # Log the duplicates\n",
    "        duplicates_df = pd.concat(duplicates_list)\n",
    "        logFilename = os.path.join(logsDirpath, \"measureDuplicates.csv\")\n",
    "        print(f\"Number of duplicate dates found: {len(duplicates_df)} (see log file {logFilename} for more details)\")\n",
    "        duplicates_df.to_csv(logFilename, index=True)\n",
    "\n",
    "    def check_integrity(self):\n",
    "        # Check if the metadata is loaded\n",
    "        if self.metadata is None:\n",
    "            raise ValueError(\"Metadata not loaded. Please load the metadata first.\")\n",
    "\n",
    "        # Check if the measures are loaded\n",
    "        if self.measures is None:\n",
    "            raise ValueError(\"Measures not loaded. Please load the measures first.\")\n",
    "\n",
    "        invalidSystems = []\n",
    "\n",
    "        for systemName in tqdm(self.get_systems_names(), desc=\"Checking data integrity\"):\n",
    "            invalidSystem = False\n",
    "\n",
    "            # Check if the system has measures\n",
    "            if systemName not in self.measures or self.measures[systemName].count() == 0:\n",
    "                invalidSystem = True\n",
    "                print(f\"System {systemName} : No measures found\")\n",
    "            # Check if the system has metadata\n",
    "            if systemName not in self.metadata:\n",
    "                invalidSystem = True\n",
    "                print(f\"System {systemName} : No metadata found\")\n",
    "            else:\n",
    "                # Check metadata for the entire system\n",
    "                systemMetadata = self.metadata[systemName]\n",
    "                for key in ['loc_latitude', 'loc_longitude', 'loc_altitude', 'pv_kwp']:\n",
    "                    # test that the key is present\n",
    "                    if key not in systemMetadata['metadata']:\n",
    "                        invalidSystem = True\n",
    "                        print(f\"System {systemName} : No '{key}' found\")\n",
    "                    # if present, convert the value to a number, if possible\n",
    "                    elif not isinstance(systemMetadata['metadata'][key], (int, float)):\n",
    "                        try:\n",
    "                            systemMetadata['metadata'][key] = int(systemMetadata['metadata'][key])\n",
    "                        except ValueError:\n",
    "                            try:\n",
    "                                systemMetadata['metadata'][key] = float(systemMetadata['metadata'][key])\n",
    "                            except ValueError:\n",
    "                                invalidSystem = True\n",
    "                                print(f\"System {systemName} : The key-value '{key}:{systemMetadata['metadata'][key]}' is not a number\")\n",
    "\n",
    "                # Check metadata for the arrays\n",
    "                if 'arrays' not in systemMetadata or len(systemMetadata['arrays']) == 0:\n",
    "                    print(f\"System {systemName} : No PV arrays found\")\n",
    "                    invalidSystem = True\n",
    "                else:\n",
    "                    for array_num, arrayData in systemMetadata['arrays'].items():\n",
    "                        for key in ['pv_tilt', 'pv_azimut', 'pv_wp', 'pv_number']:\n",
    "                            if key not in arrayData:\n",
    "                                invalidSystem = True\n",
    "                                print(f\"System {systemName} : No '{key}' found for array {array_num}\")\n",
    "                            # test that the value is a number, and convert it if possible\n",
    "                            elif not isinstance(arrayData[key], (int, float)):\n",
    "                                try:\n",
    "                                    arrayData[key] = int(arrayData[key])\n",
    "                                except ValueError:\n",
    "                                    try:\n",
    "                                        arrayData[key] = float(arrayData[key])\n",
    "                                    except ValueError:\n",
    "                                        invalidSystem = True\n",
    "                                        print(f\"System {systemName} : The key-value '{key}:{arrayData[key]}' is not a number for array {array_num}\")\n",
    "            if invalidSystem:\n",
    "                invalidSystems.append(systemName)\n",
    "\n",
    "        if len(invalidSystems) > 0:\n",
    "            # remove the invalid systems from the measures\n",
    "            nbrSystems = len(self.get_systems_names())\n",
    "            print(f\"Number of systems with all the necessary data: {nbrSystems - len(invalidSystems)}/{nbrSystems}\")\n",
    "            self.measures.drop(columns=invalidSystems, inplace=True)\n",
    "\n",
    "    def get_missing_value(self, sorted=True):\n",
    "        if sorted:\n",
    "            # Sort columns by number of missing values\n",
    "            sorted_columns = self.measures.isnull().sum().sort_values().index\n",
    "            sorted_measures = self.measures[sorted_columns]\n",
    "\n",
    "            # Create a boolean DataFrame where True indicates missing values\n",
    "            missing_values = sorted_measures.isnull()\n",
    "        else:\n",
    "            missing_values = self.measures.isnull()\n",
    "        return missing_values\n",
    "    \n",
    "    def create_train_test_set(self, test_size=None, train_size=None, random_state=None, shuffle=True):\n",
    "    \n",
    "        measuresTrain, measuresTest = train_test_split(self.measures, test_size=test_size, random_state=random_state, shuffle=shuffle)\n",
    "        if train_size is not None:\n",
    "            # Sort the observation by number of missing values in ascending order. \n",
    "            # Then, look at the number of missing value of the train_size th element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataHandler = DataHandler(dataDirpath, dataCacheDirpath)\n",
    "dataHandler.load_metadata()\n",
    "dataHandler.load_csv()\n",
    "dataHandler.check_integrity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
