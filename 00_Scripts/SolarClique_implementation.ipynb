{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup & Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pvlib\n",
    "import json\n",
    "import os\n",
    "from pvlib.pvsystem import PVSystem, Array, FixedMount\n",
    "from pvlib.location import Location\n",
    "from pvlib.modelchain import ModelChain\n",
    "from pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "import forestci as fci\n",
    "\n",
    "pio.renderers.default = \"browser\"  # render plotly figures in browser\n",
    "\n",
    "PARENT_DATA_DIR = os.getenv('PARENT_DATA_DIR')\n",
    "if PARENT_DATA_DIR is None:\n",
    "    raise ValueError(\"PARENT_DATA_DIR environment variable is not set\")\n",
    "\n",
    "\n",
    "dataDirpath = PARENT_DATA_DIR + r\"\\PRiOT\\dataExport_3400_daily\"  # \"/Applications/Documents/TM Maxime/dataExport_3400_daily\"#\n",
    "dataCacheDirpath = os.path.join(dataDirpath, \"cache\")\n",
    "logsDirpath = \"../logs\"\n",
    "forceImport = True\n",
    "forceTrain = True\n",
    "random_state = 42\n",
    "\n",
    "if not os.path.exists(logsDirpath):\n",
    "    os.makedirs(logsDirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/model_persistence.html\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "\n",
    "class ModelSerializer:\n",
    "    def _save_model(self, model, serial_type, save_params):\n",
    "        serial_type.dump(model, save_params)\n",
    "\n",
    "    def _retrieve_model(self, serial_type, retrieve_params):\n",
    "        return serial_type.load(retrieve_params)\n",
    "\n",
    "\n",
    "# save_model_path = \"Serialized_models\\\\\"\n",
    "\n",
    "\n",
    "class JoblibSerializer(ModelSerializer):\n",
    "    def save_model(self, model, save_model_path, filename):\n",
    "        super()._save_model(model, joblib, os.path.join(save_model_path, filename + \".joblib\"))\n",
    "\n",
    "    def retrieve_model(self, save_model_path, filename):\n",
    "        return super()._retrieve_model(joblib, os.path.join(save_model_path, filename + '.joblib'))\n",
    "\n",
    "\n",
    "class PickleSerializer(ModelSerializer):\n",
    "    def save_model(self, model, save_model_path, filename):\n",
    "        with open(os.path.join(save_model_path, filename + \".pkl\"), 'wb') as f:\n",
    "            super()._save_model(model, pickle, f)\n",
    "\n",
    "    def retrieve_model(self, save_model_path, filename):\n",
    "        with open(os.path.join(save_model_path, filename + \".pkl\"), 'rb') as f:\n",
    "            return super()._retrieve_model(pickle, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataFilepath = os.path.join(dataDirpath, \"metadata.json\")\n",
    "\n",
    "with open(metadataFilepath, 'r') as f:\n",
    "    systemsMetadata = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# cacheFilename_systemsData_MeasuredDailyEnergy = os.path.join(dataCacheDirpath,'systemsData_MeasuredDailyEnergy.pkl')\n",
    "\n",
    "# if not forceImport and os.path.exists(cacheFilename_systemsData_MeasuredDailyEnergy):\n",
    "#     print(f\"Loading cached data in {cacheFilename_systemsData_MeasuredDailyEnergy}\")\n",
    "#     systemsData_MeasuredDailyEnergy = pd.read_pickle(cacheFilename_systemsData_MeasuredDailyEnergy)\n",
    "# else:\n",
    "\n",
    "# Load all csv files from the data directory\n",
    "systemsData = {}\n",
    "for file in os.listdir(dataDirpath):\n",
    "    if file.endswith(\".csv\"):\n",
    "        systemName = file.split(\"_\")[0]\n",
    "        systemsData[systemName] = pd.read_csv(os.path.join(dataDirpath, file))\n",
    "        systemsData[systemName]['Datetime'] = pd.to_datetime(systemsData[systemName]['Timestamp'], unit='ms', utc=True).dt.tz_convert('Europe/Zurich')\n",
    "        systemsData[systemName]['Date'] = (systemsData[systemName]['Datetime'] + pd.Timedelta(hours=1)).dt.date  # Convert the datetime to only the date, as the production is the daily production. The +1h is to manage the saving time. Normally PRiOT exports the data at midnight (local time) for the day after (e.g. the energy for the July 1st is saved at July 1st 00:00 Europe/Zurich). However it seams that the saving time is not always correctly handled, and sometime the export is done at 23:00 the day before (e.g. the energy for the July 1st is saved at June 30th 23:00 Europe/Zurich). This is why we add 1h to the datetime to be sure to have the correct date.\n",
    "        # systemsData[systemName]['energy_daily_norm'] = systemsData[systemName]['tt_forward_active_energy_total_toDay'] / metadata[systemName]['metadata']['pv_kwp']\n",
    "\n",
    "systemsName = list(systemsData.keys())\n",
    "\n",
    "df_duplicate_list = list()\n",
    "for systemName, systemData in systemsData.items():\n",
    "    # Save duplicate dates to log list, and the in a log file\n",
    "    duplicates = systemData[systemData['Date'].duplicated(keep=False)]\n",
    "    if len(duplicates) > 0:\n",
    "        df_duplicate_list.append(duplicates)\n",
    "\n",
    "        # Remove duplicate date where tt_forward_active_energy_total_toDay is the smallest\n",
    "        # TODO maybe we should sum the energy of the duplicates instead of removing the smallest one. However, when looking in PRiOT Portal, it seams that in the daily energy, only the biggest value is represented. We do the same here.\n",
    "        systemData.sort_values('tt_forward_active_energy_total_toDay', ascending=True, inplace=True)\n",
    "        systemsData[systemName].drop_duplicates(subset='Date', keep='last', inplace=True)\n",
    "\n",
    "    # Set date as the index and sort the data by date\n",
    "    systemsData[systemName].set_index('Date', inplace=True)\n",
    "    systemData.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "# Save duplicate dates to log file\n",
    "df_duplicate = pd.concat(df_duplicate_list)\n",
    "print(f\"Number of duplicate dates found: {len(df_duplicate)} (see log file for more details)\")\n",
    "df_duplicate.to_csv(os.path.join(logsDirpath, 'duplicateDates.csv'), index=True)\n",
    "\n",
    "## ----------------------------------------------- ##\n",
    "## Convert data & Filter out invalid PRiOT systems ##\n",
    "## ----------------------------------------------- ##\n",
    "\n",
    "systemsName_Valid = systemsName.copy()\n",
    "for systemName in systemsName:\n",
    "    missingData = False\n",
    "    if len(systemsData[systemName]) == 0:\n",
    "        missingData = True\n",
    "        print(f\"No measures found for system {systemName}\")\n",
    "    for key in ['loc_latitude', 'loc_longitude', 'pv_kwp']:\n",
    "        if key not in systemsMetadata[systemName]['metadata']:\n",
    "            missingData = True\n",
    "            print(f\"No {key} found for {systemName}\")\n",
    "        # test that the value is a number\n",
    "        elif not isinstance(systemsMetadata[systemName]['metadata'][key], (int, float)):\n",
    "            try:\n",
    "                systemsMetadata[systemName]['metadata'][key] = int(systemsMetadata[systemName]['metadata'][key])\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    systemsMetadata[systemName]['metadata'][key] = float(systemsMetadata[systemName]['metadata'][key])\n",
    "                except ValueError:\n",
    "                    missingData = True\n",
    "                    print(f\"The key-value '{key}:{systemsMetadata[systemName]['metadata'][key]}' is not a number for system {systemName}\")\n",
    "\n",
    "    if (len(systemsMetadata[systemName]['arrays']) == 0):\n",
    "        print(f\"No PV arrays found for system {systemName}\")\n",
    "        missingData = True\n",
    "    for array_num, arrayData in systemsMetadata[systemName]['arrays'].items():\n",
    "        for key in ['pv_tilt', 'pv_azimut', 'pv_wp', 'pv_number']:\n",
    "            if key not in arrayData:\n",
    "                missingData = True\n",
    "                print(f\"No {key} found for array {array_num} of system {systemName}\")\n",
    "            # test that the value is a number\n",
    "            elif not isinstance(arrayData[key], (int, float)):\n",
    "                try:\n",
    "                    arrayData[key] = int(arrayData[key])\n",
    "                except ValueError:\n",
    "                    try:\n",
    "                        arrayData[key] = float(arrayData[key])\n",
    "                    except ValueError:\n",
    "                        missingData = True\n",
    "                        print(f\"The key-value '{key}:{arrayData[key]}' is not a number for array {array_num} of system {systemName}\")\n",
    "\n",
    "    if missingData:\n",
    "        systemsName_Valid.remove(systemName)\n",
    "        print(f\"-> Removing system {systemName} from the list of systems\")\n",
    "\n",
    "print(f\"Number of systems with all the necessary data: {len(systemsName_Valid)}/{len(systemsName)}\")\n",
    "\n",
    "# Filter out systems with less than 100 days of data\n",
    "minimumDays = 200\n",
    "systemsName_Remaining = systemsName_Valid.copy()\n",
    "for systemName in systemsName_Valid:\n",
    "    if len(systemsData[systemName]) < minimumDays:\n",
    "        systemsName_Remaining.remove(systemName)\n",
    "        print(f\"-> Removing system {systemName} from the list of systems because it has less than {minimumDays} days of data\")\n",
    "\n",
    "print(f\"Number of systems with at least {minimumDays} days of data: {len(systemsName_Remaining)}/{len(systemsName)}\")\n",
    "\n",
    "## ---------------------------------------------------------------------------- ##\n",
    "## Create one 2D DataFrame with the daily production of every remaining systems ##\n",
    "## ---------------------------------------------------------------------------- ##\n",
    "\n",
    "# Create an empty list to store all measured data for each systems\n",
    "systemsData_MeasuredDailyEnergy_List = []\n",
    "\n",
    "# Iterate over each key-value pair in the systemsData dictionary\n",
    "for systemName in systemsName_Remaining:\n",
    "    # Extract the 'tt_forward_active_energy_total_toDay' column from the current dataframe\n",
    "    measuredDailyEnergy = systemsData[systemName]['tt_forward_active_energy_total_toDay']\n",
    "\n",
    "    # Rename the column with the system name\n",
    "    measuredDailyEnergy.rename(systemName, inplace=True)\n",
    "\n",
    "    systemsData_MeasuredDailyEnergy_List.append(measuredDailyEnergy)\n",
    "    # Concatenate the column to the new_dataframe\n",
    "\n",
    "# Concatenate all the columns in the list to create one dataframe\n",
    "systemsData_MeasuredDailyEnergy = pd.concat(systemsData_MeasuredDailyEnergy_List, axis=1)\n",
    "systemsData_MeasuredDailyEnergy.index = pd.to_datetime(systemsData_MeasuredDailyEnergy.index)\n",
    "systemsData_MeasuredDailyEnergy.sort_index(inplace=True)\n",
    "\n",
    "## ------------------ ##\n",
    "## Save the dataframe ##\n",
    "## ------------------ ##\n",
    "# Save the dataframe for later use\n",
    "# systemsData_MeasuredDailyEnergy.to_pickle(cacheFilename_systemsData_MeasuredDailyEnergy)\n",
    "\n",
    "# Print the dataframe\n",
    "systemsData_MeasuredDailyEnergy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show missing value\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import missingno as msno\n",
    "\n",
    "msno.matrix(systemsData_dailyEnergyTotal, filter='bottom', labels=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Plot the number of available values per day. On the X axis is the day, and on the Y axis is the number of available values for this day.\n",
    "\n",
    "# Count the number of available values per day (number of value per index)\n",
    "availableValuesPerDay = systemsData_dailyEnergyTotal.count(axis=1)\n",
    "\n",
    "# Plot the number of available values per day\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=availableValuesPerDay.index, y=availableValuesPerDay.values, mode='lines'))\n",
    "fig.update_layout(title='Number of available system\\'s values per day', yaxis_title='Number of available values')\n",
    "fig.update_layout(width=1000, height=666)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate max production\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the power production with a given frequency to the total daily energy\n",
    "def daily_energy(df_power):\n",
    "    # Get the frequency in minutes\n",
    "    freq_in_minutes = pd.Timedelta(df_power.index.freq).seconds / 60\n",
    "    # Convert power from kW to kWh\n",
    "    df_energy = df_power * (freq_in_minutes / 60)\n",
    "    # Resample to daily frequency and sum the values\n",
    "    daily_energy = df_energy.resample('D').sum()\n",
    "    # daily_energy.index = daily_energy.index.date\n",
    "\n",
    "    return daily_energy\n",
    "\n",
    "# Simulate the daily production of a system from a start date to an end date using the given PVLib ModelChain\n",
    "\n",
    "\n",
    "def simulateMaxDailyEnergy(startDate, endDate, modelChain: ModelChain, samplingFreq='1h'):\n",
    "    # The end date is included in the simulation (end date at 23:59).\n",
    "    # So we add 1 day to the end date to include the entire end date in the date_range(), and then we exclude the last value (end date +1 at 00:00) in the date_range().\n",
    "    # TODO It is possible to take into account the horizon, using this method: https://pvlib-python.readthedocs.io/en/stable/gallery/shading/plot_simple_irradiance_adjustment_for_horizon_shading.html\n",
    "    endDate = endDate + pd.Timedelta(days=1)\n",
    "\n",
    "    times = pd.date_range(start=startDate, end=endDate, freq=samplingFreq, tz=modelChain.location.tz, inclusive='left')\n",
    "    weatherClearSky = modelChain.location.get_clearsky(times)  # In W/m2\n",
    "    modelChain.run_model(weatherClearSky)\n",
    "    production = modelChain.results.ac / 1000  # Convert W to kW\n",
    "    dailyProduction = daily_energy(production)\n",
    "    dailyProduction.index = dailyProduction.index.date\n",
    "    return dailyProduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheFilename_systemsData_SimulatedMaxDailyEnergy = os.path.join(dataCacheDirpath, 'systemsData_SimulatedMaxDailyEnergy.pkl')\n",
    "\n",
    "if not forceImport and os.path.exists(cacheFilename_systemsData_SimulatedMaxDailyEnergy):\n",
    "    print(f\"Loading cached data in {cacheFilename_systemsData_SimulatedMaxDailyEnergy}\")\n",
    "    systemsData_SimulatedMaxDailyEnergy = pd.read_pickle(cacheFilename_systemsData_SimulatedMaxDailyEnergy)\n",
    "else:\n",
    "\n",
    "    ## ------------------ ##\n",
    "    ## Create ModelChains ##\n",
    "    ## ------------------ ##\n",
    "    modelChains = {}\n",
    "    for systemName in systemsData_MeasuredDailyEnergy.columns:\n",
    "        latitude = systemsMetadata[systemName]['metadata']['loc_latitude']\n",
    "        longitude = systemsMetadata[systemName]['metadata']['loc_longitude']\n",
    "        altitude = 533  # TODO: Get the altitude from the metadata or an API\n",
    "        Wp_Tot = systemsMetadata[systemName]['metadata']['pv_kwp'] * 1000\n",
    "\n",
    "        arrays = []\n",
    "        for array_num, arrayData in systemsMetadata[systemName]['arrays'].items():\n",
    "            array = Array(\n",
    "                mount=FixedMount(surface_tilt=arrayData['pv_tilt'], surface_azimuth=arrayData['pv_azimut'], racking_model='open_rack'),\n",
    "                module_parameters={'pdc0': arrayData['pv_wp'], 'gamma_pdc': -0.004},\n",
    "                module_type='glass_polymer',\n",
    "                modules_per_string=arrayData['pv_number'],\n",
    "                strings=1,\n",
    "                temperature_model_parameters=TEMPERATURE_MODEL_PARAMETERS['sapm']['open_rack_glass_polymer'],\n",
    "            )\n",
    "            arrays.append(array)\n",
    "\n",
    "        location = Location(latitude=latitude, longitude=longitude, altitude=altitude, tz='Europe/Zurich', name=systemName)\n",
    "        system = PVSystem(arrays=arrays, inverter_parameters={'pdc0': Wp_Tot, 'eta_inv_nom': 0.96})\n",
    "        modelChain = ModelChain(system, location, clearsky_model='ineichen', aoi_model='no_loss', spectral_model=\"no_loss\")\n",
    "\n",
    "        modelChains[systemName] = modelChain\n",
    "\n",
    "    ## ------------------- ##\n",
    "    ## Simulate production ##\n",
    "    ## ------------------- ##\n",
    "\n",
    "    # Create an empty list to store all measured data for each systems\n",
    "    systemsData_SimulatedMaxDailyEnergy_List = []\n",
    "\n",
    "    # Iterate over each key-value pair in the systemsData dictionary\n",
    "    for systemName, modelChain in modelChains.items():\n",
    "\n",
    "        startDate = systemsData_MeasuredDailyEnergy.loc[~systemsData_MeasuredDailyEnergy[systemName].isna(), systemName].index.min()\n",
    "        endDate = systemsData_MeasuredDailyEnergy.loc[~systemsData_MeasuredDailyEnergy[systemName].isna(), systemName].index.max()\n",
    "        simulatedMaxDailyEnergy = simulateMaxDailyEnergy(startDate, endDate, modelChain, samplingFreq='1h')\n",
    "\n",
    "        # Rename the column with the system name\n",
    "        simulatedMaxDailyEnergy.rename(systemName, inplace=True)\n",
    "\n",
    "        systemsData_SimulatedMaxDailyEnergy_List.append(simulatedMaxDailyEnergy)\n",
    "        # Concatenate the column to the new_dataframe\n",
    "\n",
    "    # Concatenate all the columns in the list to create one dataframe\n",
    "    systemsData_SimulatedMaxDailyEnergy = pd.concat(systemsData_SimulatedMaxDailyEnergy_List, axis=1)\n",
    "    systemsData_SimulatedMaxDailyEnergy.sort_index(inplace=True)\n",
    "\n",
    "    # Save the dataframe to a CSV file\n",
    "    systemsData_SimulatedMaxDailyEnergy.to_pickle(cacheFilename_systemsData_SimulatedMaxDailyEnergy)\n",
    "\n",
    "# Print the dataframe\n",
    "systemsData_SimulatedMaxDailyEnergy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative production\n",
    "\n",
    "True production scaled by the maximum production from the simulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the relative energy for each system\n",
    "systemsData_RelativeDailyEnergy = systemsData_MeasuredDailyEnergy / systemsData_SimulatedMaxDailyEnergy\n",
    "# plot all the relative energy on the same graph\n",
    "fig = go.Figure()\n",
    "for systemName in systemsName_Remaining:\n",
    "    fig.add_trace(go.Scatter(x=systemsData_RelativeDailyEnergy[systemName].index, y=systemsData_RelativeDailyEnergy[systemName], mode='markers', name=systemName))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the difference between simulation with hourly and 10min sampling rate\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Simulate the daily production for each system with 1h and 10min sampling rate\n",
    "dailyProductions = {}\n",
    "\n",
    "for systemName, modelChain in modelChains.items():\n",
    "    try:\n",
    "        dailyProduction_hour = simulateDailyProduction(systemName, systemsData, modelChains, samplingFreq='1h')\n",
    "        dailyProduction_min = simulateDailyProduction(systemName, systemsData, modelChains, samplingFreq='10min')\n",
    "        dailyProductions[systemName] = pd.DataFrame({'Simulator hour': dailyProduction_hour, 'Simulator 10min': dailyProduction_min})\n",
    "    except Exception as e:\n",
    "        print(f\"Error for system {systemName}: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "# Compute the descriptive statistics of the difference between the 1h and 10min simulations on all systems\n",
    "allSimulations = pd.concat(dailyProductions.values())\n",
    "\n",
    "allSimulations['Difference'] = allSimulations['Simulator hour'] - allSimulations['Simulator 10min']\n",
    "allSimulations['Percentage'] = allSimulations['Difference'] / allSimulations['Simulator 10min'] * 100\n",
    "\n",
    "descriptiveStat = allSimulations[['Difference', 'Percentage']].describe()\n",
    "print(descriptiveStat)\n",
    "\n",
    "\n",
    "# Plot the histogram of the percentage of the difference\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=allSimulations['Percentage'], nbinsx=1000))\n",
    "fig.add_vline(x=descriptiveStat.loc['mean','Percentage'], line_color='red', line_width=2, annotation_text='mean')\n",
    "fig.add_vline(x=descriptiveStat.loc['25%','Percentage'], line_color='green', line_width=2, annotation_text='25%')\n",
    "fig.add_vline(x=descriptiveStat.loc['75%','Percentage'], line_color='green', line_width=2, annotation_text='75%')\n",
    "fig.update_xaxes(dtick=0.1)\n",
    "fig.update_xaxes(range=[-1, 1])\n",
    "fig.update_layout(width=1000, height=666)\n",
    "fig.update_layout(xaxis_title='Percentage of the difference (%)')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Plot the daily production of a system with 1h and 10min sampling rate\n",
    "systemName = 'a001096'\n",
    "dailyProduction_hour = simulateDailyProduction(systemName, systemsData, modelChains, samplingFreq='1h')\n",
    "dailyProduction_min = simulateDailyProduction(systemName, systemsData, modelChains, samplingFreq='10min')\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=dailyProduction_min.index, y=dailyProduction_min, mode='markers', name='10min sampling rate'))\n",
    "fig.add_trace(go.Scatter(x=dailyProduction_hour.index, y=dailyProduction_hour, mode='markers', name='Hourly sampling rate'))\n",
    "\n",
    "fig.update_layout(title=f'Simulated daily max AC energy of system {systemName}', xaxis_title='Time', yaxis_title='Energy (kWh)')\n",
    "fig.update_layout(width=1000, height=666)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half-Sibling Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def mean_absolute_percentage_error_mean_denominator(\n",
    "    y_true, y_pred, *, sample_weight=None, multioutput=\"uniform_average\"\n",
    "):\n",
    "    # Copy of the function mean_absolute_percentage_error from sklearn.metrics._regression, with the denominator of the MAPE changed to the mean of the true values\n",
    "    import sklearn\n",
    "\n",
    "    y_type, y_true, y_pred, multioutput = sklearn.metrics._regression._check_reg_targets(\n",
    "        y_true, y_pred, multioutput\n",
    "    )\n",
    "    sklearn.utils.validation.check_consistent_length(y_true, y_pred, sample_weight)\n",
    "    epsilon = np.finfo(np.float64).eps\n",
    "    mape = np.abs(y_pred - y_true) / np.maximum(np.mean(np.abs(y_true)), epsilon)\n",
    "    output_errors = np.average(mape, weights=sample_weight, axis=0)\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == \"raw_values\":\n",
    "            return output_errors\n",
    "        elif multioutput == \"uniform_average\":\n",
    "            # pass None as weights to np.average: uniform mean\n",
    "            multioutput = None\n",
    "\n",
    "    return np.average(output_errors, weights=multioutput)\n",
    "\n",
    "\n",
    "def mad(arr):\n",
    "    return abs(arr - arr.median()).median()\n",
    "\n",
    "\n",
    "def modified_z_score(arr):\n",
    "    # based on https://www.ibm.com/docs/en/cognos-analytics/11.1.0?topic=terms-modified-z-score\n",
    "    mad_value = mad(arr)\n",
    "    if mad_value == 0:\n",
    "        MeanAD = np.mean(np.abs(arr - np.mean(arr)))\n",
    "        denominator = 1.253314 * MeanAD\n",
    "    else:\n",
    "        denominator = 1.486 * mad_value\n",
    "\n",
    "    return (arr - np.median(arr)) / denominator\n",
    "\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'MAPE': mean_absolute_percentage_error(y_true, y_pred),\n",
    "        'MAPE-MD': mean_absolute_percentage_error_mean_denominator(y_true, y_pred),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'RMSE': root_mean_squared_error(y_true, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils.parallel import Parallel, delayed\n",
    "from sklearn.utils.validation import (\n",
    "    check_is_fitted,\n",
    ")\n",
    "from sklearn.ensemble._base import _partition_estimators\n",
    "\n",
    "def _accumulate_prediction(predict, X, out, lock):\n",
    "    \"\"\"\n",
    "    This is a utility function for joblib's Parallel.\n",
    "\n",
    "    It can't go locally in ForestClassifier or ForestRegressor, because joblib\n",
    "    complains that it cannot pickle it when placed there.\n",
    "    \"\"\"\n",
    "    prediction = predict(X, check_input=False)\n",
    "    with lock:\n",
    "            out.append(prediction)\n",
    "\n",
    "def predict_w_std(self, X):\n",
    "        \"\"\"\n",
    "        Predict regression target for X.\n",
    "\n",
    "        The predicted regression target of an input sample is computed as the\n",
    "        mean predicted regression targets of the trees in the forest.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The input samples. Internally, its dtype will be converted to\n",
    "            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
    "            converted into a sparse ``csr_matrix``.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The predicted values.\n",
    "        \"\"\"\n",
    "        if self.n_outputs_ > 1:\n",
    "            raise NotImplementedError(\"Variance for multi-output regression is not supported\")\n",
    "        \n",
    "        check_is_fitted(self)\n",
    "        # Check data\n",
    "        X = self._validate_X_predict(X)\n",
    "\n",
    "        # Assign chunk of trees to jobs\n",
    "        n_jobs, _, _ =  _partition_estimators(self.n_estimators, self.n_jobs)\n",
    "\n",
    "        # avoid storing the output of every estimator by summing them here\n",
    "\n",
    "        # Initialize a list to collect predictions from each estimator\n",
    "        all_predictions = []\n",
    "\n",
    "        # Parallel loop\n",
    "        lock = threading.Lock()\n",
    "        Parallel(n_jobs=n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n",
    "            delayed(_accumulate_prediction)(e.predict, X, all_predictions, lock)\n",
    "            for e in self.estimators_\n",
    "        )\n",
    "\n",
    "        # Convert list to numpy array for easier manipulation\n",
    "        all_predictions = np.array(all_predictions)\n",
    "\n",
    "        # Compute mean and variance across predictions from all estimators\n",
    "        mean_predictions = np.mean(all_predictions, axis=0)\n",
    "        std_predictions = np.std(all_predictions, axis=0)\n",
    "\n",
    "        return mean_predictions, std_predictions\n",
    "\n",
    "RandomForestRegressor.predict_w_std = predict_w_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train regressors & Compute metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systemsName_Target = [\"a001159\"]\n",
    "serializer = PickleSerializer()\n",
    "\n",
    "rf_regressors = {}\n",
    "forceTrain = True\n",
    "if not forceTrain:\n",
    "    # load the models in dataCacheDirpath/rf_regressors. The file name is the system name.\n",
    "    for systemName in systemsName_Target:\n",
    "        try:\n",
    "            rf_regressors[systemName] = serializer.retrieve_model(os.path.join(dataCacheDirpath, 'rf_regressors'), systemName)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "    print(f\"Loaded {len(rf_regressors)}/{len(systemsName_Target)} models. {len(systemsName_Target) - len(rf_regressors)} models to train.\")\n",
    "\n",
    "# Train a Random Forest Regressor model to predict the daily energy production of a system based on the daily energy production of the other systems\n",
    "metrics_df = pd.DataFrame(index=systemsName_Target, columns=['MAPE', 'MAPE-MD', 'MAE', 'RMSE'])\n",
    "features_importance_df = pd.DataFrame(index=systemsName_Target, columns=systemsName_Remaining)\n",
    "permutation_importance_df = pd.DataFrame(index=systemsName_Target, columns=systemsName_Remaining)\n",
    "\n",
    "y_pred_List = {}\n",
    "for targetName in tqdm(set(systemsName_Target) - set(rf_regressors), desc='Training regressors'):\n",
    "    rf_regressor = RandomForestRegressor()\n",
    "    # remove the target column from the features\n",
    "    X = systemsData_MeasuredDailyEnergy.drop(columns=targetName)\n",
    "    y = systemsData_MeasuredDailyEnergy[targetName]\n",
    "    # remove the observations where their is no target value\n",
    "    X = X[~systemsData_MeasuredDailyEnergy[targetName].isna()]\n",
    "    y = y[~systemsData_MeasuredDailyEnergy[targetName].isna()]\n",
    "    # split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=random_state)\n",
    "    # train the regressor\n",
    "    rf_regressor.fit(X_train, y_train)\n",
    "    # save the feature importances\n",
    "    features_importance_df.loc[targetName, X.columns] = rf_regressor.feature_importances_\n",
    "    # permutation_importance_df.loc[targetName, X.columns] = permutation_importance(rf_regressor, X_test, y_test, n_repeats=10, random_state=random_state, n_jobs=-1).importances_mean\n",
    "\n",
    "    \n",
    "    # test the regressor\n",
    "    # y_pred = rf_regressor.predict(X_test)\n",
    "    y_mean, y_std = rf_regressor.predict_w_std(X_test)\n",
    "\n",
    "    # y_pred_V_IJ_unbiased = fci.random_forest_error(rf_regressor, X_train, X_test)\n",
    "    # all_tree_predictions = np.array([tree.predict(X_new) for tree in rf_regressor.estimators_])\n",
    "\n",
    "\n",
    "    # create a dataframe with a column with y_pred and a column with y_pred_V_IJ_unbiased\n",
    "    y_pred_df = pd.DataFrame(data={'pred': y_mean, 'err': y_std}, index=X_test.index)\n",
    "    y_pred_List[targetName] = y_pred_df\n",
    "\n",
    "    # compute the metrics\n",
    "    metrics_df.loc[targetName] = metrics(y_test, y_mean)\n",
    "\n",
    "    # save the model\n",
    "    serializer.save_model(rf_regressor, os.path.join(dataCacheDirpath, 'rf_regressors'), targetName)\n",
    "    rf_regressors[targetName] = rf_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot y_mean, y_test and y_std of the system a001159. Data available in the y_pred_List dictionary. Use the standard deviation to plot the error bars.\n",
    "fig = go.Figure()\n",
    "y_pred_df = y_pred_List[\"a001159\"]\n",
    "fig.add_trace(go.Scatter(x=y_pred_df.index, y=y_pred_df['pred'], name='pred', error_y=dict(type='data', array=y_pred_df['err'], visible=True), mode='markers'))\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_test, mode='markers', name='true'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate expected value for each systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheFilename_systemsData_ExpectedDailyEnergy = os.path.join(dataCacheDirpath, 'systemsData_ExpectedDailyEnergy.pkl')\n",
    "\n",
    "if not forceImport and os.path.exists(cacheFilename_systemsData_ExpectedDailyEnergy):\n",
    "    print(f\"Loading cached data in {cacheFilename_systemsData_ExpectedDailyEnergy}\")\n",
    "    systemsData_ExpectedDailyEnergy = pd.read_pickle(cacheFilename_systemsData_ExpectedDailyEnergy)\n",
    "else:\n",
    "    # Create an empty list to store all expected data for each systems\n",
    "    systemsData_ExpectedDailyEnergy_List = []\n",
    "\n",
    "    for systemName in tqdm(systemsName_Remaining, desc='Generating expected daily energy'):\n",
    "        # Comute the expected daily energy for the target system for all the dates\n",
    "        X = systemsData_MeasuredDailyEnergy.drop(columns=systemName) # remove the target column from the features\n",
    "        X = X[~systemsData_MeasuredDailyEnergy[systemName].isna()] # remove the observations where their is no target value\n",
    "        expectedDailyEnergy = pd.Series(rf_regressors[systemName].predict(X), index=X.index)\n",
    "        expectedDailyEnergy.rename(systemName, inplace=True)\n",
    "        expectedDailyEnergy_V_IJ_unbiased = fci.random_forest_error(rf_regressors[systemName], mpg_X_train, X )\n",
    "        systemsData_ExpectedDailyEnergy_List.append(expectedDailyEnergy)\n",
    "\n",
    "    # Concatenate all the columns in the list to create one dataframe\n",
    "    systemsData_ExpectedDailyEnergy = pd.concat(systemsData_ExpectedDailyEnergy_List, axis=1)\n",
    "    systemsData_ExpectedDailyEnergy.sort_index(inplace=True)\n",
    "\n",
    "    # Save the dataframe to a CSV file\n",
    "    systemsData_ExpectedDailyEnergy.to_pickle(cacheFilename_systemsData_ExpectedDailyEnergy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling technics & Outliers removal\n",
    "\n",
    "Compute the:\n",
    "\n",
    "- Global mean\n",
    "- Global median\n",
    "- Global standard deviation\n",
    "- Rolling mean\n",
    "- Rolling median\n",
    "- Rolling standard deviation\n",
    "- Simulate max production without info\n",
    "- SImulated max production with info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetName = \"a001395\"\n",
    "\n",
    "X = systemsData_MeasuredDailyEnergy.drop(columns=targetName)\n",
    "y = systemsData_MeasuredDailyEnergy[targetName]\n",
    "y.index = pd.to_datetime(y.index)\n",
    "# Global mean of the daily energy production of the target system\n",
    "globalMean = y.mean()\n",
    "\n",
    "# Global std of the daily energy production of the target system\n",
    "globalStd = y.std()\n",
    "\n",
    "# Gloabl median of the daily energy production of the target system\n",
    "globalMedian = y.median()\n",
    "\n",
    "roll = y.rolling(window='30D', min_periods=1, center=True)\n",
    "# Rolling mean of the daily energy production of the target system. The window is 1 month\n",
    "rollingMean = roll.mean()\n",
    "\n",
    "# Rolling std of the daily energy production of the target system. The window is 7 days.\n",
    "rollingStd = roll.std()\n",
    "\n",
    "# Rolling Mean Absolute Deviation of the daily energy production of the target system. the function is mad with the arguments how='median' and center='median'\n",
    "rollingMAD = roll.apply(mad)\n",
    "# Rolling median of the daily energy production of the target system. The window is 7 days.\n",
    "rollingMedian = roll.median()\n",
    "\n",
    "# Rolling z score of the daily energy production of the target system. The function is modified_z_score\n",
    "# modifiedZScore = 0.673 * (y - rollingMedian) / rollingMAD\n",
    "\n",
    "# Plot the global and rolling mean, std, and median of the daily energy production of the target system\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=y.index, y=y, mode='markers', name='Daily energy production'))\n",
    "# fig.add_trace(go.Scatter(x=y.index, y=[globalMean]*len(y), mode='lines', name='Global mean'))\n",
    "# fig.add_trace(go.Scatter(x=y.index, y=[globalMean+globalStd]*len(y), mode='lines', name='Global mean + std'))\n",
    "# fig.add_trace(go.Scatter(x=y.index, y=[globalMean-globalStd]*len(y), mode='lines', name='Global mean - std'))\n",
    "# fig.add_trace(go.Scatter(x=y.index, y=[globalMedian]*len(y), mode='lines', name='Global median'))\n",
    "fig.add_trace(go.Scatter(x=rollingMean.index, y=rollingMean, mode='lines', name='Rolling mean'))\n",
    "# fig.add_trace(go.Scatter(x=rollingMean.index, y=rollingMean+rollingStd, mode='lines', name='Rolling mean + std'))\n",
    "# fig.add_trace(go.Scatter(x=rollingMean.index, y=rollingMean-rollingStd, mode='lines', name='Rolling mean - std'))\n",
    "fig.add_trace(go.Scatter(x=rollingMedian.index, y=rollingMedian, mode='lines', name='Rolling median'))\n",
    "fig.add_trace(go.Scatter(x=rollingMAD.index, y=rollingMedian + 4 * rollingMAD, mode='lines', name='Rolling Median + 4*Rolling MAD'))\n",
    "fig.add_trace(go.Scatter(x=rollingMAD.index, y=rollingMAD, mode='lines', name='Rolling MAD'))\n",
    "\n",
    "# fig.add_trace(go.Scatter(x=modifiedZScore.index, y=rollingMedian+modifiedZScore, mode='lines', name='Rolling Median + Rolling Z score'))\n",
    "\n",
    "fig.update_layout(title=f'Global and rolling mean, std, and median of the daily energy production of system {targetName}', yaxis_title='Daily energy production (kWh)')\n",
    "# fig.update_layout(width=1000, height=666)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(s, window, thresh=3, return_all=False):\n",
    "    roll = s.rolling(window=window, min_periods=1, center=True)\n",
    "    avg = roll.mean()\n",
    "    std = roll.std(ddof=0)\n",
    "    z = s.sub(avg).div(std)\n",
    "    m = z.between(-thresh, thresh)\n",
    "\n",
    "    if return_all:\n",
    "        return z, avg, std, m\n",
    "    return s.where(m, avg)\n",
    "\n",
    "\n",
    "z, avg, std, m = zscore(y, window=50, return_all=True)\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "y.plot(label='data')\n",
    "avg.plot(label='mean')\n",
    "y.loc[~m].plot(label='outliers', marker='o', ls='')\n",
    "# avg[~m].plot(label='replacement', marker='o', ls='')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(layout_yaxis_title=\"Daily Energy (kWh)\")\n",
    "\n",
    "# Add initial traces\n",
    "systemName = systemsName_Remaining[0]\n",
    "fig.add_trace(go.Scatter(x=systemsData_SimulatedMaxDailyEnergy[systemName].index, y=systemsData_SimulatedMaxDailyEnergy[systemName], mode='markers', name='Simulated Max Daily Energy'))\n",
    "fig.add_trace(go.Scatter(x=systemsData_MeasuredDailyEnergy[systemName].index, y=systemsData_MeasuredDailyEnergy[systemName], mode='markers', name='Measured Daily Energy'))\n",
    "fig.add_trace(go.Scatter(x=systemsData_ExpectedDailyEnergy[systemName].index, y=systemsData_ExpectedDailyEnergy[systemName], mode='markers', name='Expected Daily Energy'))\n",
    "\n",
    "# Create dropdown menu\n",
    "buttons = []\n",
    "for systemName in systemsName_Remaining:\n",
    "    button = dict(\n",
    "        label=systemName,\n",
    "        method=\"update\",\n",
    "        args=[{\"x\": [systemsData_SimulatedMaxDailyEnergy[systemName].index, systemsData_MeasuredDailyEnergy[systemName].index, systemsData_ExpectedDailyEnergy[systemName].index],\n",
    "               \"y\": [systemsData_SimulatedMaxDailyEnergy[systemName], systemsData_MeasuredDailyEnergy[systemName], systemsData_ExpectedDailyEnergy[systemName]]}]\n",
    "    )\n",
    "    buttons.append(button)\n",
    "\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            buttons=buttons,\n",
    "            direction=\"down\",\n",
    "            showactive=True,\n",
    "            x=0.05,\n",
    "            xanchor=\"left\",\n",
    "            y=1.15,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# add axis labels\n",
    "# fig.update_yaxes(title_text=\"Daily Energy (kWh)\")\n",
    "\n",
    "# Set size\n",
    "# fig.update_layout(\n",
    "#     autosize=False,\n",
    "#     width=1000,\n",
    "#     height=666,\n",
    "# )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(layout_yaxis_title=\"Features Importance\", layout_legend_x=0.7, layout_legend_y=0.9)\n",
    "\n",
    "# Add initial traces\n",
    "systemName = systemsName_Remaining[0]\n",
    "fig.add_trace(go.Bar(x=features_importance_df.columns, y=features_importance_df.loc[systemName], name='Impurity-based Importance'))\n",
    "fig.add_trace(go.Bar(x=permutation_importance_df.columns, y=permutation_importance_df.loc[systemName], name='Permutation Importance'))\n",
    "\n",
    "# Create dropdown menu\n",
    "buttons = []\n",
    "for systemName in systemsName_Remaining:\n",
    "    button = dict(\n",
    "        label=systemName,\n",
    "        method=\"update\",\n",
    "        args=[{\"x\": [features_importance_df.columns, permutation_importance_df.columns],\n",
    "               \"y\": [features_importance_df.loc[systemName], permutation_importance_df.loc[systemName]]}]\n",
    "    )\n",
    "    buttons.append(button)\n",
    "\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            buttons=buttons,\n",
    "            direction=\"down\",\n",
    "            showactive=True,\n",
    "            x=0.05,\n",
    "            xanchor=\"left\",\n",
    "            y=1.15,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# add axis labels\n",
    "# fig.update_yaxes(title_text=\"Daily Energy (kWh)\")\n",
    "\n",
    "# Set size\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=666,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimise regressor parameters\n",
    "\n",
    "Find the appropriate parameters with https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the importance of each feature (PV System) in the regression\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "rf_regressors[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
